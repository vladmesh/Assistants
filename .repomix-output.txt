This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-04-05T21:17:40.376Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
admin_service/
  src/
    config/
      settings.py
    pages/
      assistants/
        assistants.py
      tools/
        tools.py
      users/
        users.py
    utils/
      async_utils.py
    __init__.py
    main.py
    rest_client.py
  tests/
    __init__.py
    test_rest_client.py
  docker-compose.test.yml
  Dockerfile
  Dockerfile.test
  llm_context_admin.md
  pyproject.toml
cron_service/
  src/
    __init__.py
    main.py
    models.py
    redis_client.py
    rest_client.py
    scheduler.py
  tests/
    __init__.py
    conftest.py
    test_scheduler.py
  docker-compose.test.yml
  Dockerfile
  Dockerfile.test
  llm_context_cron.md
  pyproject.toml
  requirements.txt
google_calendar_service/
  src/
    api/
      __init__.py
      routes.py
    config/
      __init__.py
      logger.py
      settings.py
    schemas/
      calendar.py
    services/
      __init__.py
      calendar.py
      redis_service.py
      rest_service.py
    main.py
  tests/
    conftest.py
    test_routes.py
  docker-compose.test.yml
  Dockerfile
  Dockerfile.test
  llm_context_google_calendar.md
  pyproject.toml
  requirements.txt
qdrant_data/
rag_service/
  src/
    api/
      routes.py
    config/
      settings.py
    models/
      rag_models.py
    services/
      vector_db_service.py
    main.py
  tests/
    test_api_routes.py
    test_vector_db_service.py
  docker-compose.test.yml
  Dockerfile
  Dockerfile.test
  llm_context_rag.md
  pyproject.toml
  README.md
rest_service/
  alembic/
    versions/
      20250321_111905_11f7771296eb_initial_migration.py
      20250321_122512_b201a35b7385_add_is_active_field_to_cronjob.py
    env.py
    script.py.mako
  src/
    models/
      __init__.py
      assistant.py
      base.py
      calendar.py
      cron.py
      user_secretary.py
      user.py
    routers/
      assistant_tools.py
      assistants.py
      calendar.py
      cron_jobs.py
      secretaries.py
      threads.py
      tools.py
      users.py
    schemas/
      base.py
    scripts/
      fixtures/
        __init__.py
        assistant_tools.py
        assistants.py
        tools.py
        users.py
      __init__.py
      check_tools.py
      test_data.py
    config.py
    database.py
    main.py
    models.py
  tests/
    test_dummy.py
  alembic.ini
  docker-compose.test.yml
  Dockerfile
  Dockerfile.test
  llm_context_rest.md
  manage.py
  pyproject.toml
  pytest.ini
  requirements.txt
  run_tests.sh
scripts/
  format.py
  lint.py
shared_models/
  src/
    shared_models/
      __init__.py
      queue.py
  tests/
    conftest.py
    test_queue.py
  pyproject.toml
  README.md
telegram_bot_service/
  src/
    client/
      rest.py
      telegram.py
    config/
      settings.py
    handlers/
      start.py
    services/
      response_handler.py
    main.py
  tests/
    test_smoke.py
  docker-compose.test.yml
  Dockerfile
  Dockerfile.test
  llm_context_tg_bot.md
  pyproject.toml
.gitignore
docker_templates.md
docker-compose.yml
llm_context.md
manage.py
naming_conventions.md
package.json
poetry_requirements.md
pyproject.toml
rag_service_implementation_plan.md
README.md
refactoring_plan.md
requirements.txt
run_formatters.sh
run_tests.sh
service_template.md
sub_assistant_implementation_plan.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="admin_service/src/config/settings.py">
"""Configuration settings for the admin panel."""
from pydantic import Field
from pydantic_settings import BaseSettings, SettingsConfigDict
class Settings(BaseSettings):
    """Admin panel settings."""
    # REST API settings
    REST_SERVICE_URL: str = Field(default="http://rest_service:8000")
    # Logging settings
    LOG_LEVEL: str = Field(default="INFO")
    # Application settings
    APP_TITLE: str = "Admin Panel"
    APP_ICON: str = "üîß"
    APP_LAYOUT: str = "wide"
    # Navigation
    NAV_ITEMS: list[str] = ["–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏", "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã", "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã"]
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        extra="ignore",
    )
# Create settings instance
settings = Settings()
</file>

<file path="admin_service/src/pages/assistants/assistants.py">
"""Assistants page of the admin panel"""
import pandas as pd
import streamlit as st
from rest_client import AssistantCreate, AssistantUpdate, RestServiceClient, Tool
from utils.async_utils import run_async
def show_assistants_page(rest_client: RestServiceClient):
    """Display assistants page with CRUD functionality."""
    st.title("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã")
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
    assistants = run_async(rest_client.get_assistants())
    all_tools = run_async(rest_client.get_tools())
    if not assistants:
        st.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤")
    else:
        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
        assistants_data = []
        for assistant in assistants:
            # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
            assistant_tools = run_async(rest_client.get_assistant_tools(assistant.id))
            tools_count = len(assistant_tools)
            assistants_data.append(
                {
                    "ID": str(assistant.id),
                    "–ò–º—è": assistant.name,
                    "–¢–∏–ø": assistant.assistant_type,
                    "–ú–æ–¥–µ–ª—å": assistant.model,
                    "–°–µ–∫—Ä–µ—Ç–∞—Ä—å": "–î–∞" if assistant.is_secretary else "–ù–µ—Ç",
                    "–ê–∫—Ç–∏–≤–µ–Ω": "–î–∞" if assistant.is_active else "–ù–µ—Ç",
                    "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã": f"{tools_count} —à—Ç.",
                }
            )
        assistants_df = pd.DataFrame(assistants_data)
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
        st.subheader("–°–ø–∏—Å–æ–∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤")
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∏ –∫–Ω–æ–ø–∫–∏ –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö
        col1, col2 = st.columns([4, 1])
        with col1:
            st.dataframe(assistants_df, hide_index=True, use_container_width=True)
        with col2:
            for assistant in assistants:
                col_edit, col_delete, col_tools = st.columns(3)
                with col_edit:
                    if st.button(
                        "‚úèÔ∏è", key=f"edit_{assistant.id}", help="–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å"
                    ):
                        st.session_state["editing_assistant"] = assistant
                        st.rerun()
                with col_delete:
                    if st.button("üóëÔ∏è", key=f"delete_{assistant.id}", help="–£–¥–∞–ª–∏—Ç—å"):
                        st.session_state["deleting_assistant"] = assistant
                        st.rerun()
                with col_tools:
                    if st.button(
                        "üõ†Ô∏è",
                        key=f"tools_{assistant.id}",
                        help="–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏",
                    ):
                        st.session_state["managing_tools"] = assistant
                        st.rerun()
    # –°–µ–∫—Ü–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    if "managing_tools" in st.session_state:
        assistant = st.session_state["managing_tools"]
        st.subheader(f"–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏: {assistant.name}")
        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
        assistant_tools = run_async(rest_client.get_assistant_tools(assistant.id))
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
        if assistant_tools:
            st.write("–¢–µ–∫—É—â–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:")
            for tool in assistant_tools:
                col1, col2 = st.columns([4, 1])
                with col1:
                    st.write(f"- {tool.name} ({tool.tool_type})")
                with col2:
                    if st.button(
                        "üóëÔ∏è", key=f"remove_tool_{tool.id}", help="–£–¥–∞–ª–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç"
                    ):
                        with st.spinner("–£–¥–∞–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç..."):
                            run_async(
                                rest_client.remove_tool_from_assistant(
                                    assistant.id, tool.id
                                )
                            )
                            st.success(f"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç {tool.name} —É–¥–∞–ª–µ–Ω")
                            st.rerun()
        else:
            st.info("–£ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ–∫–∞ –Ω–µ—Ç –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤")
        # –§–æ—Ä–º–∞ –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        with st.form("add_tool_form"):
            # –§–∏–ª—å—Ç—Ä—É–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ –Ω–∞–∑–Ω–∞—á–µ–Ω—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
            available_tools = [
                t for t in all_tools if t.id not in [at.id for at in assistant_tools]
            ]
            if available_tools:
                selected_tool = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è",
                    options=available_tools,
                    format_func=lambda x: f"{x.name} ({x.tool_type})",
                )
                submit_button = st.form_submit_button("–î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç")
                if submit_button:
                    with st.spinner("–î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç..."):
                        run_async(
                            rest_client.add_tool_to_assistant(
                                assistant.id, selected_tool.id
                            )
                        )
                        st.success(f"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç {selected_tool.name} –¥–æ–±–∞–≤–ª–µ–Ω")
                        st.rerun()
            else:
                st.info("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è")
        # –ö–Ω–æ–ø–∫–∞ –≤–æ–∑–≤—Ä–∞—Ç–∞
        if st.button("–ù–∞–∑–∞–¥"):
            del st.session_state["managing_tools"]
            st.rerun()
    # –°–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    with st.expander("‚ûï –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞", expanded=False):
        with st.form("create_assistant_form"):
            name = st.text_input("–ò–º—è")
            is_secretary = st.checkbox("–Ø–≤–ª—è–µ—Ç—Å—è —Å–µ–∫—Ä–µ—Ç–∞—Ä–µ–º")
            model = st.text_input("–ú–æ–¥–µ–ª—å")
            instructions = st.text_area("–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏")
            assistant_type = st.selectbox("–¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞", ["llm", "openai_api"])
            openai_assistant_id = st.text_input("ID –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ OpenAI (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)")
            submit_button = st.form_submit_button("–°–æ–∑–¥–∞—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
            if submit_button:
                if not name or not model or not instructions:
                    st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è")
                else:
                    with st.spinner("–°–æ–∑–¥–∞–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞..."):
                        new_assistant = AssistantCreate(
                            name=name,
                            is_secretary=is_secretary,
                            model=model,
                            instructions=instructions,
                            assistant_type=assistant_type,
                            openai_assistant_id=openai_assistant_id
                            if openai_assistant_id
                            else None,
                        )
                        created_assistant = run_async(
                            rest_client.create_assistant(new_assistant)
                        )
                        st.success(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç {created_assistant.name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω")
                        st.rerun()
    # –°–µ–∫—Ü–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    if "editing_assistant" in st.session_state:
        assistant = st.session_state["editing_assistant"]
        st.subheader(f"–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞: {assistant.name}")
        with st.form("edit_assistant_form"):
            new_name = st.text_input("–ò–º—è", value=assistant.name)
            new_is_secretary = st.checkbox(
                "–Ø–≤–ª—è–µ—Ç—Å—è —Å–µ–∫—Ä–µ—Ç–∞—Ä–µ–º", value=assistant.is_secretary
            )
            new_model = st.text_input("–ú–æ–¥–µ–ª—å", value=assistant.model)
            new_instructions = st.text_area("–ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏", value=assistant.instructions)
            new_assistant_type = st.selectbox(
                "–¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞",
                ["llm", "openai_api"],
                index=0 if assistant.assistant_type == "llm" else 1,
            )
            new_openai_assistant_id = st.text_input(
                "ID –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ OpenAI (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)",
                value=assistant.openai_assistant_id or "",
            )
            new_is_active = st.checkbox("–ê–∫—Ç–∏–≤–µ–Ω", value=assistant.is_active)
            col1, col2 = st.columns(2)
            with col1:
                submit_button = st.form_submit_button("–û–±–Ω–æ–≤–∏—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
            with col2:
                cancel_button = st.form_submit_button("–û—Ç–º–µ–Ω–∞")
            if submit_button:
                if not new_name or not new_model or not new_instructions:
                    st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è")
                else:
                    with st.spinner("–û–±–Ω–æ–≤–ª—è–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞..."):
                        updated_assistant = AssistantUpdate(
                            name=new_name,
                            is_secretary=new_is_secretary,
                            model=new_model,
                            instructions=new_instructions,
                            assistant_type=new_assistant_type,
                            openai_assistant_id=new_openai_assistant_id
                            if new_openai_assistant_id
                            else None,
                            is_active=new_is_active,
                        )
                        run_async(
                            rest_client.update_assistant(
                                assistant.id, updated_assistant
                            )
                        )
                        st.success(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç {new_name} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω")
                        del st.session_state["editing_assistant"]
                        st.rerun()
            if cancel_button:
                del st.session_state["editing_assistant"]
                st.rerun()
    # –°–µ–∫—Ü–∏—è —É–¥–∞–ª–µ–Ω–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    if "deleting_assistant" in st.session_state:
        assistant = st.session_state["deleting_assistant"]
        st.subheader(f"–£–¥–∞–ª–∏—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞: {assistant.name}")
        st.warning(
            f"–í—ã —É–≤–µ—Ä–µ–Ω—ã, —á—Ç–æ —Ö–æ—Ç–∏—Ç–µ —É–¥–∞–ª–∏—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ '{assistant.name}'? –≠—Ç–æ –¥–µ–π—Å—Ç–≤–∏–µ –Ω–µ–ª—å–∑—è –æ—Ç–º–µ–Ω–∏—Ç—å."
        )
        col1, col2 = st.columns(2)
        with col1:
            if st.button("–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å —É–¥–∞–ª–µ–Ω–∏–µ"):
                with st.spinner("–£–¥–∞–ª—è–µ–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞..."):
                    run_async(rest_client.delete_assistant(assistant.id))
                    st.success(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç {assistant.name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω")
                    del st.session_state["deleting_assistant"]
                    st.rerun()
        with col2:
            if st.button("–û—Ç–º–µ–Ω–∞"):
                del st.session_state["deleting_assistant"]
                st.rerun()
</file>

<file path="admin_service/src/pages/tools/tools.py">
"""Tools page of the admin panel"""
import pandas as pd
import streamlit as st
from rest_client import RestServiceClient, Tool, ToolCreate, ToolUpdate
from utils.async_utils import run_async
def show_tools_page(rest_client: RestServiceClient):
    """Display tools page with management functionality."""
    st.title("–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã")
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
    tools = run_async(rest_client.get_tools())
    assistants = run_async(rest_client.get_assistants())
    if not tools:
        st.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤")
    else:
        # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        tools_data = []
        for tool in tools:
            tools_data.append(
                {
                    "ID": str(tool.id),
                    "–ò–º—è": tool.name,
                    "–¢–∏–ø": tool.tool_type,
                    "–û–ø–∏—Å–∞–Ω–∏–µ": tool.description,
                    "–ê–∫—Ç–∏–≤–µ–Ω": "–î–∞" if tool.is_active else "–ù–µ—Ç",
                }
            )
        tools_df = pd.DataFrame(tools_data)
        # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
        st.subheader("–°–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤")
        st.dataframe(tools_df, hide_index=True, use_container_width=True)
        # –°–µ–∫—Ü–∏—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
        st.subheader("–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞")
        # –°–æ–∑–¥–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤—ã–ø–∞–¥–∞—é—â–µ–≥–æ —Å–ø–∏—Å–∫–∞
        tool_options = {f"{tool.name} ({tool.tool_type})": tool for tool in tools}
        selected_tool_name = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è",
            options=list(tool_options.keys()),
            index=None,
            placeholder="–í—ã–±–µ—Ä–∏—Ç–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç...",
        )
        if selected_tool_name:
            tool = tool_options[selected_tool_name]
            with st.expander("–§–æ—Ä–º–∞ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", expanded=True):
                with st.form("edit_tool_form"):
                    st.write(f"**–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞:** {tool.name}")
                    new_description = st.text_area("–û–ø–∏—Å–∞–Ω–∏–µ", value=tool.description)
                    new_is_active = st.checkbox("–ê–∫—Ç–∏–≤–µ–Ω", value=tool.is_active)
                    col1, col2, col3 = st.columns([1, 1, 1])
                    with col1:
                        submit_button = st.form_submit_button("üíæ –°–æ—Ö—Ä–∞–Ω–∏—Ç—å")
                    with col2:
                        cancel_button = st.form_submit_button("‚ùå –û—Ç–º–µ–Ω–∞")
                    with col3:
                        delete_button = st.form_submit_button("üóëÔ∏è –£–¥–∞–ª–∏—Ç—å")
                    if submit_button:
                        with st.spinner("–û–±–Ω–æ–≤–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç..."):
                            updated_tool = ToolUpdate(
                                description=new_description,
                                is_active=new_is_active,
                            )
                            run_async(rest_client.update_tool(tool.id, updated_tool))
                            st.success(f"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç {tool.name} —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω")
                            st.rerun()
                    if cancel_button:
                        st.rerun()
                    if delete_button:
                        if st.checkbox("–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ —É–¥–∞–ª–µ–Ω–∏–µ", key="confirm_delete"):
                            with st.spinner("–£–¥–∞–ª—è–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç..."):
                                run_async(rest_client.delete_tool(tool.id))
                                st.success(f"–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç {tool.name} —É—Å–ø–µ—à–Ω–æ —É–¥–∞–ª–µ–Ω")
                                st.rerun()
    # –°–µ–∫—Ü–∏—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    with st.expander("‚ûï –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", expanded=False):
        with st.form("create_tool_form"):
            name = st.text_input("–ò–º—è")
            description = st.text_area("–û–ø–∏—Å–∞–Ω–∏–µ")
            is_active = st.checkbox("–ê–∫—Ç–∏–≤–µ–Ω", value=True)
            # –í—ã–±–æ—Ä –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
            assistant_options = {a.name: a for a in assistants if a.is_active}
            if not assistant_options:
                st.error(
                    "–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞–∫—Ç–∏–≤–Ω—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"
                )
            else:
                selected_assistant = st.selectbox(
                    "–í—ã–±–µ—Ä–∏—Ç–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞",
                    options=list(assistant_options.keys()),
                )
            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª–µ –¥–ª—è input_schema
            input_schema = st.text_area(
                "–°—Ö–µ–º–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (JSON)",
                value='{"type": "object", "properties": {"message": {"type": "string"}}, "required": ["message"]}',
                help="JSON —Å—Ö–µ–º–∞ –¥–ª—è –æ–ø–∏—Å–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞",
            )
            submit_button = st.form_submit_button("–°–æ–∑–¥–∞—Ç—å –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç")
            if submit_button:
                if not name or not description or not input_schema:
                    st.error("–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è")
                elif not assistant_options:
                    st.error(
                        "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –±–µ–∑ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤"
                    )
                else:
                    with st.spinner("–°–æ–∑–¥–∞–µ–º –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç..."):
                        selected_assistant_obj = assistant_options[selected_assistant]
                        new_tool = ToolCreate(
                            name=name,
                            tool_type="sub_assistant",
                            description=description,
                            input_schema=input_schema,
                            assistant_id=selected_assistant_obj.id,
                            is_active=is_active,
                        )
                        created_tool = run_async(rest_client.create_tool(new_tool))
                        st.success(f"–ü–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç {created_tool.name} —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω")
                        st.rerun()
</file>

<file path="admin_service/src/pages/users/users.py">
"""Users page of the admin panel"""
import pandas as pd
import streamlit as st
from rest_client import RestServiceClient
from utils.async_utils import run_async
def show_users_page(rest_client: RestServiceClient):
    """Display users page with secretary assignment functionality."""
    st.title("–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏")
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Å–µ–∫—Ä–µ—Ç–∞—Ä–µ–π
    users_and_secretaries = run_async(rest_client.get_users_and_secretaries())
    users, secretary_assistants = users_and_secretaries
    if not users:
        st.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
        return
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã—Ö —Å–µ–∫—Ä–µ—Ç–∞—Ä—è—Ö –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    user_secretaries = {}
    for user in users:
        secretary = run_async(rest_client.get_user_secretary(user.id))
        user_secretaries[user.id] = secretary
    # –û—Ç–æ–±—Ä–∞–∂–∞–µ–º —Ç–∞–±–ª–∏—Ü—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    users_df = pd.DataFrame(
        [
            {
                "ID": user.id,
                "Telegram ID": user.telegram_id,
                "Username": user.username or "–ù–µ —É–∫–∞–∑–∞–Ω",
                "–°–µ–∫—Ä–µ—Ç–∞—Ä—å": user_secretaries[user.id].name
                if user_secretaries[user.id]
                else "–ù–µ—Ç —Å–µ–∫—Ä–µ—Ç–∞—Ä—è",
            }
            for user in users
        ]
    )
    st.dataframe(users_df)
    # –°–µ–∫—Ü–∏—è –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
    with st.expander("‚ûï –ù–∞–∑–Ω–∞—á–∏—Ç—å —Å–µ–∫—Ä–µ—Ç–∞—Ä—è", expanded=False):
        # –í—ã–±–æ—Ä –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        selected_user = st.selectbox(
            "–í—ã–±–µ—Ä–∏—Ç–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è",
            options=users,
            format_func=lambda x: f"{x.username or '–ë–µ–∑ –∏–º–µ–Ω–∏'} (ID: {x.telegram_id})",
        )
        if selected_user:
            # –í—ã–±–æ—Ä —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
            secretary_options = [None] + secretary_assistants
            selected_secretary = st.selectbox(
                "–í—ã–±–µ—Ä–∏—Ç–µ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è",
                options=secretary_options,
                format_func=lambda x: "–ë–µ–∑ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è"
                if x is None
                else f"{x.name} ({x.model})",
            )
            # –ö–Ω–æ–ø–∫–∞ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è
            if st.button("–ù–∞–∑–Ω–∞—á–∏—Ç—å —Å–µ–∫—Ä–µ—Ç–∞—Ä—è"):
                with st.spinner("–ù–∞–∑–Ω–∞—á–∞–µ–º —Å–µ–∫—Ä–µ—Ç–∞—Ä—è..."):
                    if selected_secretary:
                        run_async(
                            rest_client.set_user_secretary(
                                selected_user.id, selected_secretary.id
                            )
                        )
                        st.success(
                            f"–°–µ–∫—Ä–µ—Ç–∞—Ä—å {selected_secretary.name} –Ω–∞–∑–Ω–∞—á–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é "
                            f"{selected_user.username or selected_user.telegram_id}"
                        )
                    else:
                        # –ï—Å–ª–∏ –≤—ã–±—Ä–∞–Ω None, —Ç–æ —É–¥–∞–ª—è–µ–º —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
                        run_async(
                            rest_client.set_user_secretary(selected_user.id, None)
                        )
                        st.success(
                            f"–°–µ–∫—Ä–µ—Ç–∞—Ä—å —É–¥–∞–ª–µ–Ω —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è "
                            f"{selected_user.username or selected_user.telegram_id}"
                        )
</file>

<file path="admin_service/src/utils/async_utils.py">
"""Utilities for working with async code in Streamlit"""
import asyncio
import streamlit as st
def get_event_loop():
    """Get or create event loop for current Streamlit session."""
    if "loop" not in st.session_state:
        st.session_state.loop = asyncio.new_event_loop()
    return st.session_state.loop
def run_async(coro):
    """Safely run async coroutines in Streamlit."""
    loop = get_event_loop()
    return loop.run_until_complete(coro)
</file>

<file path="admin_service/src/__init__.py">
"""
Admin Panel Service for Smart Assistant
"""
__version__ = "0.1.0"
</file>

<file path="admin_service/src/main.py">
"""Main entry point for the admin panel"""
import logging
import streamlit as st
import structlog
from config.settings import settings
from pages.assistants.assistants import show_assistants_page
from pages.tools.tools import show_tools_page
from pages.users.users import show_users_page
from rest_client import RestServiceClient
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=settings.LOG_LEVEL)
logger = structlog.get_logger()
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–ª–∏–µ–Ω—Ç–∞ REST API
rest_client = RestServiceClient()
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title=settings.APP_TITLE,
    page_icon=settings.APP_ICON,
    layout=settings.APP_LAYOUT,
)
# –°–∞–π–¥–±–∞—Ä —Å –Ω–∞–≤–∏–≥–∞—Ü–∏–µ–π
page = st.sidebar.radio("–ù–∞–≤–∏–≥–∞—Ü–∏—è", settings.NAV_ITEMS)
# –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü
if page == settings.NAV_ITEMS[0]:  # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
    show_users_page(rest_client)
elif page == settings.NAV_ITEMS[1]:  # –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã
    show_assistants_page(rest_client)
elif page == settings.NAV_ITEMS[2]:  # –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
    show_tools_page(rest_client)
</file>

<file path="admin_service/src/rest_client.py">
"""REST client for the admin panel"""
from typing import Any, Dict, List, Optional
from uuid import UUID
import httpx
from config.settings import settings
from pydantic import BaseModel
class User(BaseModel):
    """User model."""
    id: int
    telegram_id: int
    username: Optional[str] = None
class Assistant(BaseModel):
    """Assistant model."""
    id: UUID
    name: str
    is_secretary: bool
    model: str
    instructions: str
    assistant_type: str
    openai_assistant_id: Optional[str] = None
    is_active: bool
class AssistantCreate(BaseModel):
    """Model for creating a new assistant"""
    name: str
    is_secretary: bool = False
    model: str
    instructions: str
    assistant_type: str = "llm"
    openai_assistant_id: Optional[str] = None
class AssistantUpdate(BaseModel):
    """Model for updating an assistant"""
    name: Optional[str] = None
    is_secretary: Optional[bool] = None
    model: Optional[str] = None
    instructions: Optional[str] = None
    assistant_type: Optional[str] = None
    openai_assistant_id: Optional[str] = None
    is_active: Optional[bool] = None
class Tool(BaseModel):
    """Tool model."""
    id: UUID
    name: str
    tool_type: str
    description: str
    input_schema: Optional[str] = None
    assistant_id: Optional[UUID] = None
    is_active: bool = True
class ToolCreate(BaseModel):
    """Model for creating a new tool"""
    name: str
    tool_type: str
    description: str
    input_schema: Optional[str] = None
    assistant_id: Optional[UUID] = None
    is_active: bool = True
class ToolUpdate(BaseModel):
    """Model for updating a tool"""
    description: Optional[str] = None
    is_active: Optional[bool] = None
class RestServiceClient:
    """Client for interacting with the REST service."""
    def __init__(self, base_url: Optional[str] = None):
        """Initialize the client.
        Args:
            base_url: Optional base URL for the REST service.
                     If not provided, uses settings.REST_SERVICE_URL
        """
        self.base_url = (base_url or settings.REST_SERVICE_URL).rstrip("/")
        self._client = httpx.AsyncClient()
        self._cache: Dict[str, Any] = {}
    async def close(self):
        """Close the HTTP client."""
        await self._client.aclose()
    async def get_users(self) -> List[User]:
        """Get list of all users."""
        response = await self._client.get(f"{self.base_url}/api/users/all/")
        response.raise_for_status()
        return [User(**user) for user in response.json()]
    async def get_assistants(self) -> List[Assistant]:
        """Get list of all assistants."""
        response = await self._client.get(f"{self.base_url}/api/assistants/")
        response.raise_for_status()
        return [Assistant(**assistant) for assistant in response.json()]
    async def get_assistant(self, assistant_id: UUID) -> Assistant:
        """Get assistant by ID
        Args:
            assistant_id: ID of the assistant to get
        Returns:
            Assistant object
        """
        response = await self._client.get(
            f"{self.base_url}/api/assistants/{assistant_id}"
        )
        response.raise_for_status()
        return Assistant(**response.json())
    async def create_assistant(self, assistant: AssistantCreate) -> Assistant:
        """Create a new assistant
        Args:
            assistant: Assistant data to create
        Returns:
            Created Assistant object
        """
        response = await self._client.post(
            f"{self.base_url}/api/assistants/",
            json=assistant.model_dump(exclude_none=True),
        )
        response.raise_for_status()
        return Assistant(**response.json())
    async def update_assistant(
        self, assistant_id: UUID, assistant: AssistantUpdate
    ) -> Assistant:
        """Update an existing assistant
        Args:
            assistant_id: ID of the assistant to update
            assistant: Updated assistant data
        Returns:
            Updated Assistant object
        """
        response = await self._client.put(
            f"{self.base_url}/api/assistants/{assistant_id}",
            json=assistant.model_dump(exclude_none=True),
        )
        response.raise_for_status()
        return Assistant(**response.json())
    async def delete_assistant(self, assistant_id: UUID) -> None:
        """Delete an assistant
        Args:
            assistant_id: ID of the assistant to delete
        """
        response = await self._client.delete(
            f"{self.base_url}/api/assistants/{assistant_id}"
        )
        response.raise_for_status()
    async def set_user_secretary(
        self, user_id: int, secretary_id: Optional[UUID]
    ) -> None:
        """Set secretary for a user.
        Args:
            user_id: User ID
            secretary_id: Secretary assistant ID or None to remove secretary
        """
        if secretary_id:
            response = await self._client.post(
                f"{self.base_url}/api/users/{user_id}/secretary/{secretary_id}",
            )
        else:
            # –ï—Å–ª–∏ secretary_id is None, —É–¥–∞–ª—è–µ–º —Å–≤—è–∑—å
            response = await self._client.delete(
                f"{self.base_url}/api/users/{user_id}/secretary",
            )
        response.raise_for_status()
    async def get_users_and_secretaries(self):
        """Get list of users and secretary assistants."""
        users = await self.get_users()
        assistants = await self.get_assistants()
        secretary_assistants = [a for a in assistants if a.is_secretary and a.is_active]
        return users, secretary_assistants
    async def get_user_secretary(self, user_id: int) -> Optional[Assistant]:
        """Get secretary assistant for user.
        Args:
            user_id: User ID
        Returns:
            Assistant object if found, None otherwise
        """
        try:
            response = await self._client.get(
                f"{self.base_url}/api/users/{user_id}/secretary"
            )
            response.raise_for_status()
            return Assistant(**response.json())
        except httpx.HTTPStatusError as e:
            if e.response.status_code == 404:
                return None
            raise
    async def get_tools(self) -> List[Tool]:
        """Get list of all tools."""
        response = await self._client.get(f"{self.base_url}/api/tools/")
        response.raise_for_status()
        return [Tool(**tool) for tool in response.json()]
    async def get_assistant_tools(self, assistant_id: UUID) -> List[Tool]:
        """Get list of tools for an assistant."""
        response = await self._client.get(
            f"{self.base_url}/api/assistants/{assistant_id}/tools"
        )
        response.raise_for_status()
        return [Tool(**tool) for tool in response.json()]
    async def add_tool_to_assistant(self, assistant_id: UUID, tool_id: UUID) -> None:
        """Add tool to assistant."""
        response = await self._client.post(
            f"{self.base_url}/api/assistants/{assistant_id}/tools/{tool_id}"
        )
        response.raise_for_status()
    async def remove_tool_from_assistant(
        self, assistant_id: UUID, tool_id: UUID
    ) -> None:
        """Remove tool from assistant."""
        response = await self._client.delete(
            f"{self.base_url}/api/assistants/{assistant_id}/tools/{tool_id}"
        )
        response.raise_for_status()
    async def create_tool(self, tool: ToolCreate) -> Tool:
        """Create a new tool
        Args:
            tool: Tool data to create
        Returns:
            Created Tool object
        """
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º UUID –≤ —Å—Ç—Ä–æ–∫—É –¥–ª—è JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
        tool_data = tool.model_dump(exclude_none=True)
        if tool_data.get("assistant_id"):
            tool_data["assistant_id"] = str(tool_data["assistant_id"])
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
        print(f"Sending tool data: {tool_data}")
        response = await self._client.post(
            f"{self.base_url}/api/tools/",
            json=tool_data,
        )
        response.raise_for_status()
        return Tool(**response.json())
    async def update_tool(self, tool_id: UUID, tool: ToolUpdate) -> Tool:
        """Update an existing tool
        Args:
            tool_id: ID of the tool to update
            tool: Updated tool data
        Returns:
            Updated Tool object
        """
        response = await self._client.put(
            f"{self.base_url}/api/tools/{tool_id}",
            json=tool.model_dump(exclude_none=True),
        )
        response.raise_for_status()
        return Tool(**response.json())
</file>

<file path="admin_service/tests/__init__.py">
"""
Tests for Admin Panel Service
"""
</file>

<file path="admin_service/tests/test_rest_client.py">
import json
from unittest.mock import AsyncMock, MagicMock, patch
import httpx
import pytest
from pydantic import BaseModel
from src.rest_client import RestServiceClient, User
class MockResponse:
    def __init__(self, status_code, json_data):
        self.status_code = status_code
        self._json_data = json_data
    def json(self):
        return self._json_data
    def raise_for_status(self):
        if self.status_code >= 400:
            raise httpx.HTTPStatusError(
                "HTTP Error", request=MagicMock(), response=MagicMock()
            )
@pytest.fixture
def mock_users():
    return [
        {"id": 1, "telegram_id": 123456789, "username": "user1"},
        {"id": 2, "telegram_id": 987654321, "username": "user2"},
    ]
@pytest.fixture
def rest_client():
    return RestServiceClient(base_url="http://test-rest:8000")
@pytest.mark.asyncio
async def test_get_users_success(rest_client, mock_users):
    """–¢–µ—Å—Ç —É—Å–ø–µ—à–Ω–æ–≥–æ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."""
    with patch("httpx.AsyncClient.get") as mock_get:
        mock_get.return_value = MockResponse(200, mock_users)
        users = await rest_client.get_users()
        assert len(users) == 2
        assert isinstance(users[0], User)
        assert users[0].id == 1
        assert users[0].telegram_id == 123456789
        assert users[0].username == "user1"
        assert users[1].id == 2
        assert users[1].telegram_id == 987654321
        assert users[1].username == "user2"
        mock_get.assert_called_once_with("http://test-rest:8000/api/users/all/")
@pytest.mark.asyncio
async def test_get_users_error(rest_client):
    """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ—à–∏–±–∫–∏ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."""
    with patch("httpx.AsyncClient.get") as mock_get:
        mock_get.side_effect = httpx.HTTPStatusError(
            "HTTP Error", request=MagicMock(), response=MagicMock()
        )
        with pytest.raises(httpx.HTTPStatusError):
            await rest_client.get_users()
        mock_get.assert_called_once_with("http://test-rest:8000/api/users/all/")
@pytest.mark.asyncio
async def test_close(rest_client):
    """–¢–µ—Å—Ç –∑–∞–∫—Ä—ã—Ç–∏—è –∫–ª–∏–µ–Ω—Ç–∞."""
    with patch("httpx.AsyncClient.aclose") as mock_aclose:
        await rest_client.close()
        mock_aclose.assert_called_once()
</file>

<file path="admin_service/docker-compose.test.yml">
version: '3.8'
services:
  test:
    build:
      context: ..
      dockerfile: admin_service/Dockerfile.test
    environment:
      - TESTING=1
      - PYTHONPATH=/src
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - REST_SERVICE_URL=http://mock-rest:8000
    env_file:
      - ./tests/.env.test
volumes:
  test_db_data:
</file>

<file path="admin_service/Dockerfile">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY admin_service/pyproject.toml admin_service/poetry.lock* ./admin_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd admin_service && \
    poetry install --only main --no-interaction --no-ansi --no-root

# Production stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code
COPY admin_service/src/ /src/

CMD ["streamlit", "run", "src/main.py", "--server.port=8501", "--server.address=0.0.0.0"]
</file>

<file path="admin_service/Dockerfile.test">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY admin_service/pyproject.toml admin_service/poetry.lock* ./admin_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd admin_service && \
    poetry install --only main,test --no-interaction --no-ansi --no-root

# Test stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    TESTING=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code and tests
COPY admin_service/src/ /src/
COPY admin_service/tests/ /tests/

CMD ["pytest", "-v", "--cov=src", "--cov-report=term-missing", "/tests/"]
</file>

<file path="admin_service/llm_context_admin.md">
# –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å Smart Assistant

## –û–±—â–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
–ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –≤–µ–±-–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –Ω–∞ Streamlit –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤—Å–µ–º–∏ –∞—Å–ø–µ–∫—Ç–∞–º–∏ Smart Assistant. –ü–∞–Ω–µ–ª—å –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç —É–¥–æ–±–Ω—ã–π –¥–æ—Å—Ç—É–ø –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏, –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º–∏, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ —Å–∏—Å—Ç–µ–º—ã.

## –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã

### 1. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏
- –ü—Ä–æ—Å–º–æ—Ç—Ä —Å–ø–∏—Å–∫–∞ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –ø–æ–∏—Å–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ:
  - –û—Å–Ω–æ–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (ID, Telegram ID, username)
  - –ò—Å—Ç–æ—Ä–∏—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏–π
  - –ù–∞–∑–Ω–∞—á–µ–Ω–Ω—ã–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—ã
  - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏/—Ä–∞–∑–±–ª–æ–∫–∏—Ä–æ–≤–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–∞–≤–∞–º–∏ –¥–æ—Å—Ç—É–ø–∞

### 2. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º–∏
- –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
- –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤:
  - –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
  - –ú–æ–¥–µ–ª—å OpenAI
  - –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
  - –¢–∏–ø –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
  - –°—Ç–∞—Ç—É—Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
- –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º
- –ü—Ä–æ—Å–º–æ—Ç—Ä —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏

### 3. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
- –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
- –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤:
  - –ù–∞–∑–≤–∞–Ω–∏–µ –∏ –æ–ø–∏—Å–∞–Ω–∏–µ
  - –°—Ö–µ–º–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
  - –¢–∏–ø –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
  - –°—Ç–∞—Ç—É—Å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
- –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

### 4. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º—ã
- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–∫—Ç–∏–≤–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
  - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π
  - –í—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞
  - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–æ–≤
- –õ–æ–≥–∏ —Å–∏—Å—Ç–µ–º—ã
- –°—Ç–∞—Ç—É—Å –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
- –û—à–∏–±–∫–∏ –∏ –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è

### 5. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è OpenAI:
  - API –∫–ª—é—á–∏
  - –ú–æ–¥–µ–ª–∏
  - –õ–∏–º–∏—Ç—ã
- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ Redis
- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
- –û–±—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–∏—Å—Ç–µ–º—ã

### 6. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á–∞–º–∏
- –ü—Ä–æ—Å–º–æ—Ç—Ä –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á
- –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á
- –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–∞–¥–∞—á
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á
- –ò—Å—Ç–æ—Ä–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

## –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è

### –ë–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å
- –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤
- –†–∞–∑–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –ø—Ä–∞–≤ –¥–æ—Å—Ç—É–ø–∞
- –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ–π—Å—Ç–≤–∏–π –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤
- –ó–∞—â–∏—Ç–∞ –æ—Ç CSRF –∏ XSS –∞—Ç–∞–∫

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- –ü–∞–≥–∏–Ω–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö —Å–ø–∏—Å–∫–æ–≤
- –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö

### –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å
- –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –¥–∏–∑–∞–π–Ω
- –¢–µ–º–Ω–∞—è/—Å–≤–µ—Ç–ª–∞—è —Ç–µ–º–∞
- –ò–Ω—Ç—É–∏—Ç–∏–≤–Ω–æ –ø–æ–Ω—è—Ç–Ω–∞—è –Ω–∞–≤–∏–≥–∞—Ü–∏—è
- –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö
- –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –≤–∞–∂–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
- REST API –¥–ª—è –≤—Å–µ—Ö –æ–ø–µ—Ä–∞—Ü–∏–π
- WebSocket –¥–ª—è real-time –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
- –≠–∫—Å–ø–æ—Ä—Ç –¥–∞–Ω–Ω—ã—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
- –†–µ–∑–µ—Ä–≤–Ω–æ–µ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
- –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
- –≠–∫—Å–ø–æ—Ä—Ç/–∏–º–ø–æ—Ä—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
- –ú–∞—Å—Å–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏
- –°–∏—Å—Ç–µ–º–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π –¥–ª—è –∞–¥–º–∏–Ω–∏—Å—Ç—Ä–∞—Ç–æ—Ä–æ–≤

```bash
# Start the service
docker compose up -d admin_service

# View logs
docker compose logs -f admin_service
```

## Development

### Project Structure

```
admin_service/
‚îú‚îÄ‚îÄ src/                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ admin_service/      # Service package
‚îÇ   ‚îú‚îÄ‚îÄ config.py           # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ main.py             # Entry point
‚îÇ   ‚îî‚îÄ‚îÄ rest_client.py      # REST API client
‚îú‚îÄ‚îÄ tests/                  # Tests
‚îú‚îÄ‚îÄ Dockerfile              # Production Dockerfile
‚îú‚îÄ‚îÄ Dockerfile.test         # Test Dockerfile
‚îú‚îÄ‚îÄ docker-compose.test.yml # Test configuration
‚îî‚îÄ‚îÄ pyproject.toml          # Poetry dependencies and settings
```

### Running Tests

```bash
# Run tests
docker compose -f admin_service/docker-compose.test.yml up --build
```

## Interaction with Other Services

### REST Service

Admin Service interacts with the REST Service to obtain data about users, assistants, tools, and tasks.

## Configuration

The service uses the following environment variables:

- `REST_SERVICE_URL`: REST API URL (default: http://rest_service:8000)
- `LOG_LEVEL`: Logging level (default: INFO)
</file>

<file path="admin_service/pyproject.toml">
[tool.poetry]
name = "admin-service"
version = "0.1.0"
description = "Admin panel service for Smart Assistant"
authors = ["Your Name <your.email@example.com>"]
packages = [{ include = "admin_service/src" }]

[tool.poetry.dependencies]
python = "^3.11"
streamlit = "^1.32.0"
httpx = "^0.26.0"
pydantic = "^2.6.1"
python-dotenv = "^1.0.1"
structlog = ">=24.1.0"
shared-models = {path = "../shared_models"}

[tool.poetry.group.dev.dependencies]
black = "^23.10.1"
isort = "^5.12.0"
flake8 = "^6.1.0"
flake8-pyproject = "^1.2.3"
mypy = "^1.6.1"
pylint = "^3.0.2"
autoflake = "^2.3.1"

[tool.poetry.group.test.dependencies]
pytest = "^8.0.0"
pytest-asyncio = "^0.23.5"
pytest-cov = "^4.1.0"
pytest-env = "^1.1.3"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --cov=src --cov-report=term-missing"
asyncio_mode = "auto"
</file>

<file path="cron_service/src/__init__.py">
"""
Cron service application package
"""
</file>

<file path="cron_service/src/main.py">
import logging
from scheduler import start_scheduler
logger = logging.getLogger(__name__)
def main():
    logger.info("–ó–∞–ø—É—Å–∫–∞–µ–º Cron Service...")
    start_scheduler()
if __name__ == "__main__":
    main()
</file>

<file path="cron_service/src/models.py">
from datetime import datetime
from typing import Dict, Optional
from pydantic import BaseModel, Field
from shared_models.queue import (
    QueueMessage,
    QueueMessageContent,
    QueueMessageSource,
    QueueMessageType,
)
class MessageContent(BaseModel):
    """Content of the message with metadata."""
    message: str
    metadata: Dict = Field(default_factory=dict)
class QueueMessage(BaseModel):
    """Standardized message format for queue communication."""
    type: str = "tool_message"
    user_id: int
    source: str = "cron"
    content: MessageContent
    timestamp: str = Field(default_factory=lambda: datetime.utcnow().isoformat())
class CronMessage(QueueMessage):
    """Message from cron service."""
    type: QueueMessageType = QueueMessageType.TOOL
    source: QueueMessageSource = QueueMessageSource.CRON
    tool_name: str = "cron"
</file>

<file path="cron_service/src/redis_client.py">
import json
import logging
import os
import redis
from models import CronMessage
logger = logging.getLogger(__name__)
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", "6379"))
REDIS_DB = int(os.getenv("REDIS_DB", "0"))
OUTPUT_QUEUE = os.getenv("REDIS_QUEUE_TO_SECRETARY", "queue:to_secretary")
redis_client = redis.Redis(
    host=REDIS_HOST, port=REDIS_PORT, db=REDIS_DB, decode_responses=True
)
def send_notification(user_id: int, message: str, metadata: dict = None) -> None:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Redis –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ.
    Args:
        user_id: ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
        message: –¢–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
        metadata: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –æ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–∏
    """
    try:
        queue_message = CronMessage(
            user_id=user_id,
            content={"message": message, "metadata": metadata or {}},
        )
        redis_client.rpush(OUTPUT_QUEUE, queue_message.model_dump_json())
        logger.info("–£–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ Redis")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ Redis: {str(e)}")
        raise
</file>

<file path="cron_service/src/rest_client.py">
import logging
import os
import requests
logger = logging.getLogger(__name__)
REST_SERVICE_URL = os.getenv("REST_SERVICE_URL", "http://rest_service:8000")
def fetch_scheduled_jobs():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ—Ç REST-—Å–µ—Ä–≤–∏—Å–∞."""
    url = f"{REST_SERVICE_URL}/api/cronjobs/"
    try:
        response = requests.get(url)
        response.raise_for_status()
        jobs = response.json()
        logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ –∑–∞–¥–∞—á: {len(jobs)}")
        return jobs
    except requests.RequestException as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –∑–∞–¥–∞—á: {e}")
        return []
</file>

<file path="cron_service/src/scheduler.py">
import logging
import os
import time
from apscheduler.schedulers.background import BackgroundScheduler
from pytz import utc
from redis_client import send_notification
from rest_client import fetch_scheduled_jobs
# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)
MAX_RETRIES = 3
RETRY_DELAY = 5  # —Å–µ–∫—É–Ω–¥—ã
# –°–æ–∑–¥–∞–µ–º –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º UTC
scheduler = BackgroundScheduler(timezone=utc)
def start_scheduler():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á."""
    try:
        # –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –∑–∞–¥–∞—á–∏ –∏–∑ REST-—Å–µ—Ä–≤–∏—Å–∞
        scheduler.add_job(
            update_jobs_from_rest,
            "interval",
            minutes=1,
            id="update_jobs_from_rest",
            name="update_jobs_from_rest",
        )
        scheduler.start()
        logger.info("–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω")
        # –î–µ—Ä–∂–∏–º –ø—Ä–æ—Ü–µ—Å—Å –∞–∫—Ç–∏–≤–Ω—ã–º
        try:
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            scheduler.shutdown()
            logger.info("–ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞: {e}")
        scheduler.shutdown()
        raise
def update_jobs_from_rest():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç –∑–∞–¥–∞—á–∏ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–µ, –∑–∞–ø—Ä–∞—à–∏–≤–∞—è –∏—Ö —É REST-—Å–µ—Ä–≤–∏—Å–∞."""
    retries = 0
    while retries < MAX_RETRIES:
        try:
            logger.info("–ù–∞—á–∏–Ω–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞–¥–∞—á –∏–∑ REST-—Å–µ—Ä–≤–∏—Å–∞...")
            logger.info(
                "REST_SERVICE_URL: "
                f"{os.getenv('REST_SERVICE_URL', 'http://rest_service:8000')}"
            )
            jobs = fetch_scheduled_jobs()
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(jobs)} –∑–∞–¥–∞—á –∏–∑ REST-—Å–µ—Ä–≤–∏—Å–∞")
            # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â–∏–µ –∑–∞–¥–∞—á–∏ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–µ
            current_scheduler_jobs = scheduler.get_jobs()
            logger.info(
                f"–¢–µ–∫—É—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–¥–∞—á –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–µ: "
                f"{len(current_scheduler_jobs)}"
            )
            # –£–¥–∞–ª—è–µ–º –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã—Ö –±–æ–ª—å—à–µ –Ω–µ—Ç –≤ REST-—Å–µ—Ä–≤–∏—Å–µ
            current_jobs = {f"job_{job['id']}" for job in jobs}
            logger.info(f"ID –∑–∞–¥–∞—á –∏–∑ REST-—Å–µ—Ä–≤–∏—Å–∞: {current_jobs}")
            for job in current_scheduler_jobs:
                logger.info(f"–ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–¥–∞—á—É –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞: {job.id} ({job.name})")
                if (
                    job.id not in current_jobs and job.id != "update_jobs_from_rest"
                ):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ ID
                    scheduler.remove_job(job.id)
                    logger.info(f"–ó–∞–¥–∞—á–∞ {job.name} —É–¥–∞–ª–µ–Ω–∞ –∏–∑ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞")
            # –î–æ–±–∞–≤–ª—è–µ–º –∏–ª–∏ –æ–±–Ω–æ–≤–ª—è–µ–º –∑–∞–¥–∞—á–∏
            for job in jobs:
                job_id = f"job_{job['id']}"
                logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∑–∞–¥–∞—á—É –∏–∑ REST: {job_id} ({job['name']})")
                if not scheduler.get_job(job_id):
                    try:
                        cron_args = parse_cron_expression(job["cron_expression"])
                        logger.info(f"CRON –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è {job['name']}: {cron_args}")
                        scheduler.add_job(
                            execute_job,
                            trigger="cron",
                            id=job_id,
                            name=job["name"],
                            args=[job],
                            **cron_args,
                        )
                        logger.info(
                            f"–ó–∞–¥–∞—á–∞ {job['name']} —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫"
                        )
                    except Exception as e:
                        logger.error(
                            f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –¥–æ–±–∞–≤–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ {job['name']}: {str(e)}"
                        )
                else:
                    logger.info(f"–ó–∞–¥–∞—á–∞ {job['name']} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –≤ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–µ")
            logger.info("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –∑–∞–¥–∞—á —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
            break
        except Exception as e:
            retries += 1
            logger.error(
                f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∑–∞–¥–∞—á (–ø–æ–ø—ã—Ç–∫–∞ {retries}/{MAX_RETRIES}): "
                f"{str(e)}"
            )
            if retries < MAX_RETRIES:
                time.sleep(RETRY_DELAY)
            else:
                logger.error(
                    "–ü—Ä–µ–≤—ã—à–µ–Ω–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∑–∞–¥–∞—á"
                )
                raise  # –ü—Ä–æ–±—Ä–∞—Å—ã–≤–∞–µ–º –∏—Å–∫–ª—é—á–µ–Ω–∏–µ –¥–∞–ª—å—à–µ
def parse_cron_expression(cron_expression: str) -> dict:
    """
    –ü–∞—Ä—Å–∏—Ç —Å—Ç—Ä–æ–∫—É CRON-–≤—ã—Ä–∞–∂–µ–Ω–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ—ë –≤ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –¥–ª—è APScheduler.
    –í—Å–µ –≤—Ä–µ–º–µ–Ω–∞ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É—é—Ç—Å—è –∫–∞–∫ UTC.
    """
    parts = cron_expression.split()
    if len(parts) != 5:
        raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç CRON-–≤—ã—Ä–∞–∂–µ–Ω–∏—è: {cron_expression}")
    return {
        "minute": parts[0],
        "hour": parts[1],
        "day": parts[2],
        "month": parts[3],
        "day_of_week": parts[4],
        "timezone": utc,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º UTC –¥–ª—è –∫–∞–∂–¥–æ–π –∑–∞–¥–∞—á–∏
    }
def execute_job(job):
    """–í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–¥–∞—á—É."""
    try:
        logger.info(f"–í—ã–ø–æ–ª–Ω—è–µ–º –∑–∞–¥–∞—á—É: {job['name']}")
        metadata = {
            "job_id": job["id"],
            "job_name": job["name"],
            "cron_expression": job["cron_expression"],
        }
        send_notification(
            user_id=job["user_id"],
            message=f"–ó–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–¥–∞—á–∞: {job['name']}",
            metadata=metadata,
        )
        logger.info(f"–ó–∞–¥–∞—á–∞ {job['name']} —É—Å–ø–µ—à–Ω–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ –∑–∞–¥–∞—á–∏ {job['name']}: {e}")
        raise
</file>

<file path="cron_service/tests/__init__.py">
"""
Test package for cron service
"""
</file>

<file path="cron_service/tests/conftest.py">
import os
import sys
# Add the app directory to Python path
sys.path.insert(0, os.path.abspath(os.path.dirname(os.path.dirname(__file__))))
</file>

<file path="cron_service/tests/test_scheduler.py">
from unittest.mock import patch
import pytest
import requests
from pytz import utc
from redis_client import OUTPUT_QUEUE, send_notification
from rest_client import fetch_scheduled_jobs
from scheduler import execute_job, parse_cron_expression
def test_parse_cron_expression_valid():
    """Test parsing valid cron expressions"""
    # Test all wildcards
    assert parse_cron_expression("* * * * *") == {
        "minute": "*",
        "hour": "*",
        "day": "*",
        "month": "*",
        "day_of_week": "*",
        "timezone": utc,
    }
    # Test specific values
    assert parse_cron_expression("0 12 * * 1") == {
        "minute": "0",
        "hour": "12",
        "day": "*",
        "month": "*",
        "day_of_week": "1",
        "timezone": utc,
    }
    # Test complex expression
    assert parse_cron_expression("*/15 2,12 1-15 * 1-5") == {
        "minute": "*/15",
        "hour": "2,12",
        "day": "1-15",
        "month": "*",
        "day_of_week": "1-5",
        "timezone": utc,
    }
def test_parse_cron_expression_invalid():
    """Test parsing invalid cron expressions"""
    # Test incomplete expression
    with pytest.raises(ValueError):
        parse_cron_expression("* * *")
    # Test empty expression
    with pytest.raises(ValueError):
        parse_cron_expression("")
    # Test invalid format
    with pytest.raises(ValueError):
        parse_cron_expression("* * * * * *")
@patch("redis_client.redis_client")
def test_send_notification_success(mock_redis):
    """Test successful notification sending"""
    # Setup mock
    mock_redis.rpush.return_value = 1
    # Call function
    send_notification(123, "test message", {"priority": "high"})
    # Verify the request was made correctly
    mock_redis.rpush.assert_called_once()
    args, kwargs = mock_redis.rpush.call_args
    assert args[0] == OUTPUT_QUEUE
    # Parse the sent message to verify its structure
    sent_message = args[1]
    assert '"type":"tool_message"' in sent_message
    assert '"user_id":123' in sent_message
    assert '"source":"cron"' in sent_message
    assert '"message":"test message"' in sent_message
    assert '"priority":"high"' in sent_message
@patch("redis_client.redis_client")
def test_send_notification_failure(mock_redis):
    """Test notification sending failure"""
    # Setup mock to simulate error
    mock_redis.rpush.side_effect = Exception("Redis error")
    # Call function and verify it raises the exception
    with pytest.raises(Exception) as exc_info:
        send_notification(123, "test message")
    assert str(exc_info.value) == "Redis error"
    mock_redis.rpush.assert_called_once()
def test_execute_job_success():
    """Test successful job execution"""
    # Setup test data
    test_job = {
        "id": 1,
        "name": "test_job",
        "cron_expression": "* * * * *",
        "user_id": 123,
    }
    # Execute job
    execute_job(test_job)
    # Verify message was sent to Redis
    # Note: We can't verify the exact message content as it's in Redis
    # but we can verify that no exceptions were raised
def test_execute_job_failure():
    """Test job execution failure"""
    # Setup test data with invalid user_id to trigger error
    test_job = {
        "id": 1,
        "name": "test_job",
        "cron_expression": "* * * * *",
        "user_id": None,  # This will cause validation error
    }
    # Execute job and verify it raises the exception
    with pytest.raises(Exception):
        execute_job(test_job)
@patch("requests.get")
def test_fetch_scheduled_jobs_success(mock_get):
    """Test successful jobs fetching"""
    # Setup mock
    expected_jobs = [
        {"id": 1, "name": "job1", "cron_expression": "* * * * *", "user_id": 123},
        {"id": 2, "name": "job2", "cron_expression": "0 12 * * *", "user_id": 456},
    ]
    mock_get.return_value.json.return_value = expected_jobs
    mock_get.return_value.status_code = 200
    # Call function
    jobs = fetch_scheduled_jobs()
    # Verify results
    assert jobs == expected_jobs
    mock_get.assert_called_once()
@patch("requests.get")
def test_fetch_scheduled_jobs_failure(mock_get):
    """Test jobs fetching failure"""
    # Setup mock to simulate network error
    mock_get.side_effect = requests.RequestException("Network error")
    # Call function
    jobs = fetch_scheduled_jobs()
    # Verify empty list is returned on error
    assert jobs == []
    mock_get.assert_called_once()
</file>

<file path="cron_service/docker-compose.test.yml">
services:
  test:
    build:
      context: ..
      dockerfile: cron_service/Dockerfile.test
    environment:
      - TESTING=1
      - PYTHONPATH=/src
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - REST_SERVICE_URL=${REST_SERVICE_URL}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    env_file:
      - ./tests/.env.test
    depends_on:
      redis:
        condition: service_healthy
  redis:
    image: redis:7.2-alpine
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3
</file>

<file path="cron_service/Dockerfile">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY cron_service/pyproject.toml cron_service/poetry.lock* ./cron_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd cron_service && \
    poetry install --only main --no-interaction --no-ansi --no-root

# Production stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code
COPY cron_service/src/ /src/

CMD ["python", "src/main.py"]
</file>

<file path="cron_service/Dockerfile.test">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY cron_service/pyproject.toml cron_service/poetry.lock* ./cron_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd cron_service && \
    poetry install --only main,test --no-interaction --no-ansi --no-root

# Test stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    TESTING=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code and tests
COPY cron_service/src/ /src/
COPY cron_service/tests/ /tests/

CMD ["pytest", "-v", "--cov=src", "--cov-report=term-missing", "/tests/"]
</file>

<file path="cron_service/llm_context_cron.md">
# Cron Service Detailed Overview

## 1. Overview
- **Purpose:** Manages and executes scheduled tasks.
- **Functions:** 
  - Periodically fetches job configurations from REST API
  - Parses and validates CRON expressions
  - Executes scheduled tasks
  - Sends notifications via Redis
- **Tech Stack:** 
  - Python with APScheduler
  - Redis for message queuing
  - REST API integration
  - Dockerized microservice architecture

## 2. Directory Structure
```
cron_service/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # Service entry point
‚îÇ   ‚îú‚îÄ‚îÄ scheduler.py         # Core scheduling logic
‚îÇ   ‚îú‚îÄ‚îÄ redis_client.py      # Redis integration
‚îÇ   ‚îî‚îÄ‚îÄ rest_client.py       # REST API integration
‚îî‚îÄ‚îÄ tests/                  # Test suite
    ‚îú‚îÄ‚îÄ conftest.py         # Test fixtures
    ‚îú‚îÄ‚îÄ test_scheduler.py   # Scheduler tests
    ‚îî‚îÄ‚îÄ .env.test           # Test environment config
```

## 3. Key Components

### 3.1 Scheduler
```python
scheduler = BackgroundScheduler(timezone=utc)

def start_scheduler():
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –∑–∞–¥–∞—á."""
    try:
        scheduler.add_job(
            update_jobs_from_rest,
            "interval",
            minutes=1,
            id="update_jobs_from_rest"
        )
        scheduler.start()
```

- **Features:**
  - Background scheduling with APScheduler
  - UTC timezone support
  - Automatic job updates
  - Error handling and logging
  - Graceful shutdown

### 3.2 Redis Client
```python
def send_notification(chat_id: int, message: str, priority: str = "normal") -> None:
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Redis."""
    data = {
        "chat_id": chat_id,
        "response": message,
        "status": "success"
    }
    redis_client.rpush(OUTPUT_QUEUE, json.dumps(data))
```

- **Features:**
  - Message queuing
  - Priority levels
  - Error handling
  - Connection pooling

### 3.3 REST Client
```python
def fetch_scheduled_jobs():
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –æ—Ç REST-—Å–µ—Ä–≤–∏—Å–∞."""
    url = f"{REST_SERVICE_URL}/api/cronjobs/"
    response = requests.get(url)
    return response.json()
```

- **Features:**
  - Job configuration retrieval
  - Error handling
  - Automatic retries
  - Logging

## 4. Configuration

### 4.1 Environment Variables
```python
# Redis settings
REDIS_HOST = os.getenv("REDIS_HOST", "redis")
REDIS_PORT = int(os.getenv("REDIS_PORT", "6379"))
REDIS_DB = int(os.getenv("REDIS_DB", "0"))
OUTPUT_QUEUE = os.getenv("REDIS_QUEUE_TO_TELEGRAM", "queue:to_telegram")

# REST service settings
REST_SERVICE_URL = os.getenv("REST_SERVICE_URL", "http://rest_service:8000")

# Scheduler settings
CHAT_ID = int(os.getenv("TELEGRAM_ID", "0"))
MAX_RETRIES = 3
RETRY_DELAY = 5  # seconds
```

## 5. Job Management

### 5.1 CRON Expression Parsing
```python
def parse_cron_expression(expression: str) -> dict:
    """–ü–∞—Ä—Å–∏—Ç CRON –≤—ã—Ä–∞–∂–µ–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
    parts = expression.split()
    if len(parts) != 5:
        raise ValueError("Invalid CRON expression")
    
    return {
        "minute": parts[0],
        "hour": parts[1],
        "day": parts[2],
        "month": parts[3],
        "day_of_week": parts[4],
        "timezone": utc
    }
```

### 5.2 Job Execution
- Automatic job updates every minute
- CRON expression validation
- Error handling and retries
- Status tracking

## 6. Testing

### 6.1 Test Structure
- **Unit Tests:**
  - CRON expression parsing
  - Job scheduling
  - Notification sending
  - REST API integration

### 6.2 Test Environment
```python
# conftest.py
@pytest.fixture
def mock_redis():
    with patch('redis_client.redis_client') as mock:
        yield mock

@pytest.fixture
def mock_rest():
    with patch('rest_client.requests') as mock:
        yield mock
```

## 7. Integration

### 7.1 REST API Integration
- Fetches job configurations
- Updates job status
- Handles errors and retries

### 7.2 Redis Integration
- Sends notifications
- Queues messages
- Handles connection issues

## 8. Best Practices

### 8.1 Development Guidelines
- Use type hints
- Follow naming conventions
- Document changes
- Maintain test coverage

### 8.2 Error Handling
- Structured logging
- Retry mechanisms
- Graceful degradation
- Error reporting

### 8.3 Performance
- Connection pooling
- Efficient scheduling
- Resource management
- Monitoring
</file>

<file path="cron_service/pyproject.toml">
[tool.poetry]
name = "cron_service"
version = "0.1.0"
description = "Cron Service for Smart Assistant Project"
authors = ["Your Name <your.email@example.com>"]
packages = [{include = "src"}]

[tool.poetry.dependencies]
python = "^3.11"
apscheduler = "^3.10.1"
requests = "^2.31.0"
sqlmodel = "^0.0.16"
psycopg2-binary = "^2.9.9"
python-dotenv = "^1.0.1"
redis = "^5.0.1"
pytz = "^2024.1"
shared_models = {path = "../shared_models"}

[tool.poetry.group.test.dependencies]
shared_models = {path = "../shared_models"}
pytest = "^8.0.2"
pytest-mock = "^3.12.0"
pytest-asyncio = "^0.23.5"
requests-mock = "^1.11.0"
pytest-cov = "^4.1.0"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --cov=src --cov-report=term-missing"
asyncio_mode = "auto"

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_optional = true
mypy_path = "src"
</file>

<file path="cron_service/requirements.txt">
apscheduler==3.10.1
requests==2.31.0
sqlmodel==0.0.16
psycopg2-binary==2.9.9
python-dotenv==1.0.1
pytest==8.0.2
pytest-mock==3.12.0
pytest-asyncio==0.23.5
requests-mock==1.11.0
redis==5.0.1
pytz==2024.1
</file>

<file path="google_calendar_service/src/api/__init__.py">
"""
API package for Google Calendar Service
"""
</file>

<file path="google_calendar_service/src/api/routes.py">
from datetime import datetime
from typing import Any, Dict, List, Optional
import structlog
from config.settings import Settings
from fastapi import APIRouter, Body, Depends, HTTPException, Request, Response
from pydantic import BaseModel, Field
from schemas.calendar import CreateEventRequest
from services.calendar import GoogleCalendarService
from services.redis_service import RedisService
from services.rest_service import RestService
logger = structlog.get_logger()
router = APIRouter()
# Create settings instance once
settings = Settings()
class EventBase(BaseModel):
    """Base model for event fields"""
    summary: str = Field(..., description="Event title")
    description: Optional[str] = Field(None, description="Event description")
    location: Optional[str] = Field(None, description="Event location")
class EventCreate(EventBase):
    """Model for event creation"""
    start: Dict[str, str] = Field(
        ..., description="Event start time with dateTime and timeZone"
    )
    end: Dict[str, str] = Field(
        ..., description="Event end time with dateTime and timeZone"
    )
    def to_google_format(self) -> Dict[str, Any]:
        """Convert event data to Google Calendar API format"""
        event = {"summary": self.summary, "start": self.start, "end": self.end}
        if self.description:
            event["description"] = self.description
        if self.location:
            event["location"] = self.location
        return event
class EventResponse(EventBase):
    """Model for event response"""
    id: str
    start: Dict[str, str]
    end: Dict[str, str]
    htmlLink: Optional[str] = None
    status: str
async def get_rest_service(request: Request) -> RestService:
    """Get REST service from app state"""
    return request.app.state.rest_service
async def get_calendar_service(request: Request) -> GoogleCalendarService:
    """Get Google Calendar service from app state"""
    return request.app.state.calendar_service
async def get_redis_service(request: Request) -> RedisService:
    """Get Redis service from app state"""
    return request.app.state.redis_service
@router.get("/auth/url/{user_id}")
async def get_auth_url(
    user_id: str,
    request: Request,
    rest_service: RestService = Depends(get_rest_service),
    calendar_service: GoogleCalendarService = Depends(get_calendar_service),
) -> Dict[str, str]:
    """Get Google OAuth URL for user authorization"""
    try:
        # Check if user exists
        user = await rest_service.get_user(int(user_id))
        if not user:
            raise HTTPException(status_code=404, detail="User not found")
        # Check if user already has credentials
        credentials = await rest_service.get_calendar_token(user_id)
        if credentials:
            raise HTTPException(status_code=400, detail="User already authorized")
        # Get auth URL with state
        auth_url = calendar_service.get_auth_url(user_id)
        return {"auth_url": auth_url}
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    except HTTPException:
        raise
    except Exception as e:
        logger.error("Failed to get auth URL", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))
@router.get("/auth/callback")
async def handle_callback(
    code: str,
    state: str,
    request: Request,
    rest_service: RestService = Depends(get_rest_service),
    calendar_service: GoogleCalendarService = Depends(get_calendar_service),
    redis_service: RedisService = Depends(get_redis_service),
) -> Response:
    """Handle OAuth callback"""
    try:
        # Parse user_id from state
        user_id = int(state)
        # Handle callback
        credentials = await calendar_service.handle_callback(code)
        # Save credentials to REST service
        success = await rest_service.update_calendar_token(
            user_id=user_id,
            access_token=credentials.token,
            refresh_token=credentials.refresh_token,
            token_expiry=credentials.expiry,
        )
        if not success:
            raise HTTPException(status_code=500, detail="Failed to save credentials")
        # Send message to assistant
        await redis_service.send_to_assistant(
            user_id=user_id,
            message="‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –≤ Google Calendar —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!",
        )
        # Redirect to Telegram
        telegram_url = settings.TELEGRAM_DEEP_LINK_URL.format(
            TELEGRAM_BOT_USERNAME=settings.TELEGRAM_BOT_USERNAME
        )
        return Response(status_code=302, headers={"Location": telegram_url})
    except Exception as e:
        logger.error("Failed to handle callback", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))
@router.get("/events/{user_id}")
async def get_events(
    user_id: str,
    time_min: Optional[datetime] = None,
    time_max: Optional[datetime] = None,
    rest_service: RestService = Depends(get_rest_service),
    calendar_service: GoogleCalendarService = Depends(get_calendar_service),
) -> List[Dict[str, Any]]:
    """Get user's calendar events"""
    try:
        # Get credentials from REST service
        credentials = await rest_service.get_calendar_token(int(user_id))
        if not credentials:
            raise HTTPException(status_code=401, detail="User not authorized")
        # Get events
        events = await calendar_service.get_events(credentials, time_min, time_max)
        return events
    except HTTPException:
        raise
    except ValueError as e:
        raise HTTPException(status_code=401, detail=str(e))
    except Exception as e:
        logger.error("Failed to get events", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))
@router.post("/events/{user_id}", response_model=Dict[str, Any])
async def create_event(
    user_id: str,
    event: CreateEventRequest = Body(..., description="Event details"),
    rest_service: RestService = Depends(get_rest_service),
    calendar_service: GoogleCalendarService = Depends(get_calendar_service),
) -> Dict[str, Any]:
    """Create new calendar event using simplified data model"""
    try:
        # Get credentials from REST service
        credentials = await rest_service.get_calendar_token(int(user_id))
        if not credentials:
            raise HTTPException(status_code=401, detail="User not authorized")
        # Log event data
        logger.info("Received event data", user_id=user_id, event_data=event.dict())
        # Create event
        created_event = await calendar_service.create_event(credentials, event)
        logger.info(
            "Event created successfully",
            user_id=user_id,
            event_id=created_event.get("id"),
        )
        return created_event
    except ValueError as e:
        logger.error("Authorization error", error=str(e))
        raise HTTPException(status_code=401, detail=str(e))
    except Exception as e:
        logger.error("Failed to create event", error=str(e))
        raise HTTPException(status_code=500, detail=str(e))
</file>

<file path="google_calendar_service/src/config/__init__.py">
"""
Configuration package for Google Calendar Service
"""
</file>

<file path="google_calendar_service/src/config/logger.py">
import logging
import sys
import structlog
def configure_logger(environment: str = "development") -> None:
    """
    Configure structured logger based on environment
    Args:
        environment: Current environment (development/production)
    """
    # Set up standard logging
    logging.basicConfig(format="%(message)s", stream=sys.stdout, level=logging.INFO)
    # Configure structlog
    structlog.configure(
        processors=[
            # Add timestamp in a more readable format
            structlog.processors.TimeStamper(fmt="%Y-%m-%d %H:%M:%S.%f%z"),
            # Add log level
            structlog.processors.add_log_level,
            # Add logger name
            structlog.contextvars.merge_contextvars,
            # Add file and line number
            structlog.processors.CallsiteParameterAdder(),
            structlog.processors.StackInfoRenderer(),
            structlog.processors.format_exc_info,
            structlog.processors.UnicodeDecoder(),
            # Use better console output in development
            (
                structlog.dev.ConsoleRenderer(colors=True, sort_keys=True)
                if environment == "development"
                else structlog.processors.JSONRenderer()
            ),
        ],
        logger_factory=structlog.PrintLoggerFactory(),
        wrapper_class=structlog.BoundLogger,
        cache_logger_on_first_use=True,
    )
def get_logger(name: str) -> structlog.BoundLogger:
    """
    Get a configured logger instance
    Args:
        name: Logger name (usually __name__)
    Returns:
        Configured logger instance
    """
    return structlog.get_logger(name)
# Configure logger on import
configure_logger()
# Default logger instance
logger = get_logger("calendar_service")
</file>

<file path="google_calendar_service/src/config/settings.py">
from pydantic_settings import BaseSettings, SettingsConfigDict
class Settings(BaseSettings):
    """Application settings"""
    # Basic settings
    ENVIRONMENT: str = "development"
    LOG_LEVEL: str = "DEBUG"
    # Google OAuth settings
    GOOGLE_CLIENT_ID: str
    GOOGLE_CLIENT_SECRET: str
    GOOGLE_REDIRECT_URI: str
    GOOGLE_TOKEN_URI: str = "https://oauth2.googleapis.com/token"
    GOOGLE_AUTH_PROVIDER_CERT_URL: str = "https://www.googleapis.com/oauth2/v1/certs"
    # Telegram settings
    TELEGRAM_BOT_USERNAME: str
    TELEGRAM_DEEP_LINK_URL: str = "https://t.me/{TELEGRAM_BOT_USERNAME}"
    # REST service settings
    REST_SERVICE_URL: str = "http://rest_service:8000"
    # Redis settings
    REDIS_HOST: str = "redis"
    REDIS_PORT: int = 6379
    REDIS_DB: int = 0
    # Queue names
    REDIS_QUEUE_TO_SECRETARY: str
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")
</file>

<file path="google_calendar_service/src/schemas/calendar.py">
from datetime import datetime
from typing import Optional
from pydantic import BaseModel, Field
class EventTime(BaseModel):
    """Model for event time with timezone"""
    date_time: datetime = Field(..., description="Event date and time")
    time_zone: str = Field(default="UTC", description="Timezone for the event")
class CreateEventRequest(BaseModel):
    """Simplified model for creating calendar event"""
    title: str = Field(..., description="Event title")
    start_time: EventTime = Field(..., description="Event start time")
    end_time: EventTime = Field(..., description="Event end time")
    description: Optional[str] = Field(None, description="Event description")
    location: Optional[str] = Field(None, description="Event location")
</file>

<file path="google_calendar_service/src/services/__init__.py">
"""
Services package for Google Calendar Service
"""
from services.calendar import GoogleCalendarService
from services.redis_service import RedisService
from services.rest_service import RestService
__all__ = ["GoogleCalendarService", "RedisService", "RestService"]
</file>

<file path="google_calendar_service/src/services/calendar.py">
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional
from config.logger import get_logger
from config.settings import Settings
from google.auth.transport.requests import Request
from google.oauth2.credentials import Credentials
from google_auth_oauthlib.flow import Flow
from googleapiclient.discovery import build
from schemas.calendar import CreateEventRequest
logger = get_logger(__name__)
class GoogleCalendarService:
    """Service for working with Google Calendar API"""
    SCOPES = [
        "https://www.googleapis.com/auth/calendar",
        "https://www.googleapis.com/auth/calendar.readonly",
        "https://www.googleapis.com/auth/calendar.events",
    ]
    def __init__(self, settings: Settings):
        self.settings = settings
        # Create client configuration
        client_config = {
            "web": {
                "client_id": settings.GOOGLE_CLIENT_ID,
                "project_id": "smart-assistant",
                "auth_uri": "https://accounts.google.com/o/oauth2/auth",
                "token_uri": settings.GOOGLE_TOKEN_URI,
                "auth_provider_x509_cert_url": settings.GOOGLE_AUTH_PROVIDER_CERT_URL,
                "client_secret": settings.GOOGLE_CLIENT_SECRET,
                "redirect_uris": [settings.GOOGLE_REDIRECT_URI],
            }
        }
        # Create flow with redirect URI
        self._flow = Flow.from_client_config(
            client_config=client_config,
            scopes=self.SCOPES,
            redirect_uri=settings.GOOGLE_REDIRECT_URI,
        )
    def get_auth_url(self, state: str) -> str:
        """Get Google OAuth URL for user authorization"""
        auth_url, _ = self._flow.authorization_url(
            access_type="offline",
            include_granted_scopes="true",
            prompt="consent",
            state=state,
        )
        return auth_url
    async def handle_callback(self, code: str) -> Credentials:
        """Handle OAuth callback and return credentials"""
        try:
            # Exchange code for tokens
            self._flow.fetch_token(code=code)
            return self._flow.credentials
        except Exception as e:
            logger.error("Failed to handle callback", error=str(e))
            raise
    def _refresh_credentials_if_needed(self, credentials: Credentials) -> Credentials:
        """Refresh credentials if expired"""
        try:
            if credentials.expired:
                credentials.refresh(Request())
            return credentials
        except Exception as e:
            logger.error("Failed to refresh credentials", error=str(e))
            raise
    async def get_events(
        self,
        credentials_data: Dict[str, Any],
        time_min: Optional[datetime] = None,
        time_max: Optional[datetime] = None,
    ) -> List[Dict[str, Any]]:
        """Get user's calendar events"""
        try:
            # Create credentials object
            credentials = Credentials(
                token=credentials_data["access_token"],
                refresh_token=credentials_data["refresh_token"],
                token_uri="https://oauth2.googleapis.com/token",
                client_id=self.settings.GOOGLE_CLIENT_ID,
                client_secret=self.settings.GOOGLE_CLIENT_SECRET,
                scopes=self.SCOPES,
                expiry=datetime.fromisoformat(credentials_data["token_expiry"]),
            )
            # Refresh if needed
            credentials = self._refresh_credentials_if_needed(credentials)
            service = build("calendar", "v3", credentials=credentials)
            # Set default time range if not provided
            if not time_min:
                time_min = datetime.utcnow()
            elif isinstance(time_min, str):
                time_min = datetime.fromisoformat(time_min)
            if not time_max:
                time_max = time_min + timedelta(days=7)
            elif isinstance(time_max, str):
                time_max = datetime.fromisoformat(time_max)
            # Ensure timezone info is present
            if time_min.tzinfo is None:
                time_min = time_min.replace(tzinfo=datetime.now().astimezone().tzinfo)
            if time_max.tzinfo is None:
                time_max = time_max.replace(tzinfo=datetime.now().astimezone().tzinfo)
            events_result = (
                service.events()
                .list(
                    calendarId="primary",
                    timeMin=time_min.isoformat(),  # RFC3339 with timezone
                    timeMax=time_max.isoformat(),  # RFC3339 with timezone
                    singleEvents=True,
                    orderBy="startTime",
                )
                .execute()
            )
            return events_result.get("items", [])
        except Exception as e:
            logger.error("Failed to get events", error=str(e))
            raise
    async def create_event(
        self, credentials_data: Dict[str, Any], event_data: CreateEventRequest
    ) -> Dict[str, Any]:
        """Create new calendar event using simplified data model"""
        try:
            # Create credentials object
            credentials = Credentials(
                token=credentials_data["access_token"],
                refresh_token=credentials_data["refresh_token"],
                token_uri="https://oauth2.googleapis.com/token",
                client_id=self.settings.GOOGLE_CLIENT_ID,
                client_secret=self.settings.GOOGLE_CLIENT_SECRET,
                scopes=self.SCOPES,
                expiry=datetime.fromisoformat(credentials_data["token_expiry"]),
            )
            # Refresh if needed
            credentials = self._refresh_credentials_if_needed(credentials)
            service = build("calendar", "v3", credentials=credentials)
            # Log incoming event data
            logger.info("Creating event with data", event_data=event_data.dict())
            # Format event data according to Google Calendar API requirements
            formatted_event = {
                "summary": event_data.title,
                "start": {
                    "dateTime": event_data.start_time.date_time.isoformat(),
                    "timeZone": event_data.start_time.time_zone,
                },
                "end": {
                    "dateTime": event_data.end_time.date_time.isoformat(),
                    "timeZone": event_data.end_time.time_zone,
                },
            }
            # Add optional fields if present
            if event_data.description:
                formatted_event["description"] = event_data.description
            if event_data.location:
                formatted_event["location"] = event_data.location
            # Log formatted event data
            logger.info("Formatted event data", formatted_event=formatted_event)
            event = (
                service.events()
                .insert(calendarId="primary", body=formatted_event)
                .execute()
            )
            logger.info("Event created successfully", event_id=event.get("id"))
            return event
        except Exception as e:
            logger.error("Failed to create event", error=str(e))
            raise
</file>

<file path="google_calendar_service/src/services/redis_service.py">
import json
from typing import Awaitable
import redis.asyncio as redis
import structlog
from config.settings import Settings
from shared_models import HumanQueueMessage, QueueMessageSource
logger = structlog.get_logger()
class RedisService:
    """Service for working with Redis"""
    def __init__(self, settings: Settings):
        self.settings = settings
        self.redis: redis.Redis = redis.Redis(
            host=settings.REDIS_HOST,
            port=settings.REDIS_PORT,
            db=settings.REDIS_DB,
            decode_responses=True,
        )
    async def send_to_assistant(self, user_id: int, message: str) -> bool:
        """Send message to assistant input queue"""
        try:
            # Create a HumanQueueMessage using shared_models
            queue_message = HumanQueueMessage(
                user_id=user_id,
                source=QueueMessageSource.CALENDAR,
                content={"message": message},
            )
            logger.info(
                "Sending message to Redis",
                user_id=user_id,
                message=message,
                queue=self.settings.REDIS_QUEUE_TO_SECRETARY,
            )
            result: Awaitable[int] = self.redis.lpush(  # type: ignore[assignment]
                self.settings.REDIS_QUEUE_TO_SECRETARY, queue_message.to_dict()
            )
            await result
            logger.info("Message sent successfully")
            return True
        except Exception as e:
            logger.error("Failed to send message to assistant", error=str(e))
            return False
    async def close(self) -> None:
        """Close Redis connection"""
        result: Awaitable[int] = self.redis.close()
        await result
</file>

<file path="google_calendar_service/src/services/rest_service.py">
from datetime import datetime
from typing import Optional
import httpx
import structlog
from config.settings import Settings
logger = structlog.get_logger()
class RestService:
    def __init__(self, settings: Settings):
        self.base_url = settings.REST_SERVICE_URL
        self.client = httpx.AsyncClient()
    async def get_user(self, user_id: int) -> Optional[dict]:
        """Get user from REST service by user_id"""
        try:
            response = await self.client.get(f"{self.base_url}/api/users/{user_id}")
            if response.status_code == 404:
                return None
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            logger.error("Failed to get user", error=str(e), user_id=user_id)
            return None
    async def get_calendar_token(self, user_id: int) -> Optional[dict]:
        """Get calendar token for user from REST service"""
        try:
            response = await self.client.get(
                f"{self.base_url}/api/calendar/user/{user_id}/token"
            )
            if response.status_code == 404:
                return None
            response.raise_for_status()
            return response.json()
        except httpx.HTTPError as e:
            logger.error("Failed to get calendar token", error=str(e), user_id=user_id)
            return None
    async def update_calendar_token(
        self,
        user_id: int,
        access_token: str,
        refresh_token: str,
        token_expiry: datetime,
    ) -> bool:
        """Update calendar token for user in REST service"""
        try:
            response = await self.client.put(
                f"{self.base_url}/api/calendar/user/{user_id}/token",
                params={
                    "access_token": access_token,
                    "refresh_token": refresh_token,
                    "token_expiry": token_expiry.isoformat(),
                },
            )
            response.raise_for_status()
            return True
        except httpx.HTTPError as e:
            logger.error(
                "Failed to update calendar token", error=str(e), user_id=user_id
            )
            return False
    async def close(self):
        """Close HTTP client"""
        await self.client.aclose()
</file>

<file path="google_calendar_service/src/main.py">
from api.routes import router
from config.logger import get_logger
from config.settings import Settings
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from services.calendar import GoogleCalendarService
from services.redis_service import RedisService
from services.rest_service import RestService
# Get configured logger
logger = get_logger(__name__)
# Create settings instance
settings = Settings()
# Create FastAPI app
app = FastAPI(
    title="Google Calendar Service",
    description="Service for working with Google Calendar",
    version="1.0.0",
)
# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
# Create services
rest_service = RestService(settings)
calendar_service = GoogleCalendarService(settings)
redis_service = RedisService(settings)
# Add services to app state
app.state.rest_service = rest_service
app.state.calendar_service = calendar_service
app.state.redis_service = redis_service
# Include API routes
app.include_router(router)
@app.on_event("startup")
async def startup_event():
    """Startup event handler"""
    logger.info("Starting Google Calendar service")
@app.on_event("shutdown")
async def shutdown_event():
    """Close service connections"""
    await rest_service.close()
    await redis_service.close()
    logger.info("Shutting down Google Calendar service")
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy"}
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="google_calendar_service/tests/conftest.py">
from unittest.mock import AsyncMock, MagicMock
import pytest
from main import app
from services.calendar import GoogleCalendarService
from services.rest_service import RestService
@pytest.fixture
def mock_rest_service():
    """Create mock for REST service."""
    mock = AsyncMock(spec=RestService)
    mock.get_user.side_effect = lambda user_id: (
        {"id": 123, "name": "Test User"} if user_id == 123 else None
    )
    mock.get_calendar_token.return_value = None
    return mock
@pytest.fixture
def mock_calendar_service():
    """Create mock for Calendar service."""
    mock = MagicMock(spec=GoogleCalendarService)
    mock.get_auth_url.return_value = "https://test.auth.url"
    return mock
@pytest.fixture
def mock_services(mock_rest_service, mock_calendar_service):
    """Setup and teardown mocked services."""
    # Store original services
    original_rest_service = app.state.rest_service
    original_calendar_service = app.state.calendar_service
    # Set up mocks
    app.state.rest_service = mock_rest_service
    app.state.calendar_service = mock_calendar_service
    yield {"rest_service": mock_rest_service, "calendar_service": mock_calendar_service}
    # Restore original services
    app.state.rest_service = original_rest_service
    app.state.calendar_service = original_calendar_service
</file>

<file path="google_calendar_service/tests/test_routes.py">
import pytest
from httpx import AsyncClient
from main import app
@pytest.mark.asyncio
async def test_get_auth_url(mock_services):
    """Test getting auth URL."""
    async with AsyncClient(app=app, base_url="http://test") as client:
        # Test successful case
        response = await client.get("/auth/url/123")
        assert response.status_code == 200
        assert "auth_url" in response.json()
        assert response.json()["auth_url"] == "https://test.auth.url"
        # Verify service calls
        mock_services["rest_service"].get_user.assert_called_with(123)
        mock_services["rest_service"].get_calendar_token.assert_called_with("123")
        mock_services["calendar_service"].get_auth_url.assert_called_with("123")
        # Test user not found
        response = await client.get("/auth/url/999")
        assert response.status_code == 404
        assert response.json()["detail"] == "User not found"
</file>

<file path="google_calendar_service/docker-compose.test.yml">
services:
  test:
    build:
      context: ..
      dockerfile: google_calendar_service/Dockerfile.test
    environment:
      - TESTING=1
      - PYTHONPATH=/src
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - GOOGLE_CALENDAR_CREDENTIALS=${GOOGLE_CALENDAR_CREDENTIALS}
      - GOOGLE_CALENDAR_TOKEN=${GOOGLE_CALENDAR_TOKEN}
    env_file:
      - ./tests/.env.test
    depends_on:
      redis:
        condition: service_healthy
  redis:
    image: redis:7.2-alpine
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3
</file>

<file path="google_calendar_service/Dockerfile">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY google_calendar_service/pyproject.toml google_calendar_service/poetry.lock* ./google_calendar_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd google_calendar_service && \
    poetry install --only main --no-interaction --no-ansi --no-root

# Production stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    gcc \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code
COPY google_calendar_service/src/ /src/

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
</file>

<file path="google_calendar_service/Dockerfile.test">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY google_calendar_service/pyproject.toml google_calendar_service/poetry.lock* ./google_calendar_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd google_calendar_service && \
    poetry install --only main,test --no-interaction --no-ansi --no-root

# Test stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    TESTING=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    gcc \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code and tests
COPY google_calendar_service/src/ /src/
COPY google_calendar_service/tests/ /tests/

CMD ["pytest", "-v", "--cov=src", "--cov-report=term-missing", "/tests/"]
</file>

<file path="google_calendar_service/llm_context_google_calendar.md">
# Google Calendar Service Detailed Overview

## 1. Overview
- **Purpose:** Integrates with Google Calendar API for event management.
- **Functions:** 
  - OAuth 2.0 authorization and token management
  - Event retrieval and creation
  - Integration with REST API for user data
  - Redis-based message queuing
- **Tech Stack:** 
  - FastAPI for asynchronous processing
  - Google Calendar API integration
  - Redis for message queuing
  - Dockerized microservice architecture

## 2. Directory Structure
```
google_calendar_service/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI application entry point
‚îÇ   ‚îú‚îÄ‚îÄ api/                 # API endpoints
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py        # API routes and handlers
‚îÇ   ‚îú‚îÄ‚îÄ services/            # Business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calendar.py      # Google Calendar service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ redis_service.py # Redis integration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rest_service.py  # REST API integration
‚îÇ   ‚îú‚îÄ‚îÄ config/             # Configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py     # Application settings
‚îÇ   ‚îî‚îÄ‚îÄ schemas/            # Pydantic models
‚îÇ       ‚îî‚îÄ‚îÄ calendar.py     # Calendar data models
‚îî‚îÄ‚îÄ tests/                  # Test suite
    ‚îú‚îÄ‚îÄ conftest.py         # Test fixtures
    ‚îú‚îÄ‚îÄ test_routes.py      # API endpoint tests
    ‚îî‚îÄ‚îÄ .env.test           # Test environment config
```

## 3. Key Components

### 3.1 FastAPI Application
- Initializes middleware and CORS
- Manages service lifecycle
- Handles startup/shutdown events
- Provides health check endpoint

### 3.2 GoogleCalendarService
- **OAuth Management:**
  - `get_auth_url(state)`: Generates OAuth URL
  - `handle_callback(code)`: Processes OAuth callback
  - `_refresh_credentials_if_needed()`: Token refresh logic

- **Calendar Operations:**
  - `get_events()`: Retrieves calendar events
  - `create_event()`: Creates new events
  - `_refresh_credentials_if_needed()`: Token management

### 3.3 RedisService
- Manages message queues
- Handles notifications
- Implements retry mechanisms
- Provides connection pooling

### 3.4 RestService
- User data management
- Token storage
- Configuration retrieval
- Error handling and retries

## 4. API Endpoints

### 4.1 Authentication
- **GET `/auth/url/{user_id}`**
  - Returns OAuth URL for user authorization
  - Validates user existence
  - Checks for existing credentials

- **POST `/auth/callback`**
  - Processes OAuth callback
  - Exchanges code for tokens
  - Stores credentials in REST service

### 4.2 Events
- **GET `/events/{user_id}`**
  - Retrieves user's calendar events
  - Supports time range filtering
  - Handles token refresh

- **POST `/events/{user_id}`**
  - Creates new calendar events
  - Validates event data
  - Handles timezone conversion

## 5. Data Models

### 5.1 Event Models
```python
class EventBase(BaseModel):
    summary: str
    description: Optional[str]
    location: Optional[str]

class EventCreate(EventBase):
    start: Dict[str, str]
    end: Dict[str, str]

class EventResponse(EventBase):
    id: str
    start: Dict[str, str]
    end: Dict[str, str]
    htmlLink: Optional[str]
    status: str
```

## 6. Configuration

### 6.1 Settings
```python
class Settings(BaseSettings):
    # Google OAuth settings
    GOOGLE_CLIENT_ID: str
    GOOGLE_CLIENT_SECRET: str
    GOOGLE_REDIRECT_URI: str
    GOOGLE_TOKEN_URI: str = "https://oauth2.googleapis.com/token"
    
    # Redis settings
    REDIS_HOST: str = "redis"
    REDIS_PORT: int = 6379
    REDIS_DB: int = 0
    
    # REST service settings
    REST_SERVICE_URL: str = "http://rest_service:8000"
```

## 7. Testing

### 7.1 Test Structure
- **Unit Tests:**
  - Service functionality
  - Model validation
  - Error handling

- **Integration Tests:**
  - API endpoints
  - Service interactions
  - Redis operations

### 7.2 Test Environment
- Isolated test database
- Mock services
- Environment variables
- Test fixtures

## 8. Integration & Security

### 8.1 Inter-Service Communication
- **REST API:**
  - User data management
  - Token storage
  - Configuration retrieval

- **Redis:**
  - Message queuing
  - Notifications
  - Event updates

### 8.2 Security Measures
- OAuth 2.0 implementation
- Token validation
- Credential refresh
- Input sanitization

## 9. Best Practices

### 9.1 Development Guidelines
- Use type hints
- Follow naming conventions
- Document changes
- Maintain test coverage

### 9.2 Error Handling
- Structured logging
- Retry mechanisms
- Graceful degradation
- User-friendly messages

### 9.3 Performance
- Async/await for I/O
- Connection pooling
- Token caching
- Rate limiting
</file>

<file path="google_calendar_service/pyproject.toml">
[tool.poetry]
name = "google-calendar-service"
version = "0.1.0"
description = "Google Calendar Service for Smart Assistant"
authors = ["Your Name <your.email@example.com>"]
packages = [{ include = "src" }]

[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.109.2"
uvicorn = "^0.27.1"
pydantic = "^2.6.1"
pydantic-settings = "^2.1.0"
python-dotenv = "^1.0.1"
structlog = "^24.1.0"
httpx = "^0.26.0"
redis = "^5.0.1"
google-auth = "^2.27.0"
google-auth-oauthlib = "^1.2.0"
google-auth-httplib2 = "^0.2.0"
google-api-python-client = "^2.116.0"
shared-models = {path = "../shared_models"}

[tool.poetry.group.dev.dependencies]
flake8 = "^6.1.0"
flake8-pyproject = "^1.2.3"
mypy = "^1.6.1"
pylint = "^3.0.2"
autoflake = "^2.3.1"

[tool.poetry.group.test.dependencies]
pytest = "^7.4.3"
pytest-asyncio = "^0.21.1"
pytest-cov = "^4.1.0"
pytest-mock = "^3.12.0"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --cov=src --cov-report=term-missing"
asyncio_mode = "auto"

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_optional = true
mypy_path = "src"

[[tool.mypy.overrides]]
module = ["google_auth_oauthlib.*", "googleapiclient.*"]
ignore_missing_imports = true
</file>

<file path="google_calendar_service/requirements.txt">
fastapi==0.109.2
uvicorn==0.27.1
pydantic==2.6.1
pydantic-settings==2.1.0
python-dotenv==1.0.1
structlog==24.1.0
httpx==0.26.0
redis==5.0.1
google-auth==2.27.0
google-auth-oauthlib==1.2.0
google-auth-httplib2==0.2.0
google-api-python-client==2.116.0
</file>

<file path="rag_service/src/api/routes.py">
from typing import List
from fastapi import APIRouter, Depends, HTTPException
from src.services.vector_db_service import VectorDBService
from src.models.rag_models import RagData, SearchQuery, SearchResult
router = APIRouter()
async def get_vector_db_service() -> VectorDBService:
    """–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è VectorDBService."""
    return VectorDBService()
@router.post("/data/add", response_model=RagData)
async def add_data_endpoint(rag_data: RagData, vector_db_service: VectorDBService = Depends(get_vector_db_service)):
    """–≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
    try:
        await vector_db_service.add_data(rag_data)
        return rag_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
@router.post("/data/search", response_model=List[SearchResult])
async def search_data_endpoint(search_query: SearchQuery, vector_db_service: VectorDBService = Depends(get_vector_db_service)):
    """–≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
    try:
        results = await vector_db_service.search_data(
            query_embedding=search_query.query_embedding,
            data_type=search_query.data_type,
            user_id=search_query.user_id,
            assistant_id=search_query.assistant_id,
            top_k=search_query.top_k
        )
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
</file>

<file path="rag_service/src/config/settings.py">
from pydantic_settings import BaseSettings, SettingsConfigDict
class Settings(BaseSettings):
    """Rag Service Settings."""
    ENVIRONMENT: str = "development"
    LOG_LEVEL: str = "INFO"
    API_PORT: int = 8002 # –í—ã–±—Ä–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ—Ä—Ç
    VECTOR_DB_PATH: str = "chroma_db" # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ChromaDB
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")
settings = Settings()
</file>

<file path="rag_service/src/models/rag_models.py">
from datetime import datetime
from typing import List, Optional, Dict, Any
from uuid import UUID, uuid4
from pydantic import BaseModel, Field
class RagData(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –¥–∞–Ω–Ω—ã—Ö, —Ö—Ä–∞–Ω—è—â–∏—Ö—Å—è –≤ RAG —Å–µ—Ä–≤–∏—Å–µ."""
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    text: str = Field(..., description="–¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ")
    embedding: List[float] = Field(..., description="–í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞")
    data_type: str = Field(..., description="–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'shared_rule', 'user_history', 'assistant_note')")
    user_id: Optional[int] = Field(None, description="ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    assistant_id: Optional[UUID] = Field(None, description="ID –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
    timestamp: datetime = Field(default_factory=datetime.utcnow)
class SearchQuery(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ RAG —Å–µ—Ä–≤–∏—Å—É."""
    query_embedding: List[float] = Field(..., description="–í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞")
    data_type: str = Field(..., description="–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞")
    user_id: Optional[int] = Field(None, description="–§–∏–ª—å—Ç—Ä –ø–æ ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    assistant_id: Optional[UUID] = Field(None, description="–§–∏–ª—å—Ç—Ä –ø–æ ID –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
    top_k: int = Field(default=5, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞")
class SearchResult(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –∏–∑ RAG —Å–µ—Ä–≤–∏—Å–∞."""
    id: UUID
    text: str
    distance: float
    metadata: Dict[str, Any]
</file>

<file path="rag_service/src/services/vector_db_service.py">
from typing import List, Optional, UUID
import chromadb
import structlog
from src.config.settings import settings
from src.models.rag_models import RagData, SearchResult
logger = structlog.get_logger()
class VectorDBService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö."""
    def __init__(self):
        self.client = chromadb.PersistentClient(path=settings.VECTOR_DB_PATH)
        self.collection = self.client.get_or_create_collection(name="rag_data") # –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º
    async def add_data(self, rag_data: RagData) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
        try:
            self.collection.add(
                embeddings=[rag_data.embedding],
                documents=[rag_data.text],
                ids=[str(rag_data.id)], # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å UUID –∫–∞–∫ ID
                metadatas=[rag_data.model_dump(exclude={"embedding", "text", "id"})], # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –∏—Å–∫–ª—é—á–∏—Ç—å embedding –∏ text —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            )
            logger.info(f"Data added to vector DB with id: {rag_data.id}")
        except Exception as e:
            logger.error(f"Error adding data to vector DB: {e}")
            raise
    async def search_data(self, query_embedding: List[float], data_type: str, user_id: Optional[int], assistant_id: Optional[UUID], top_k: int = 5) -> List[SearchResult]:
        """–ò—â–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
        try:
            query_results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where={"data_type": data_type, "user_id": str(user_id) if user_id else None, "assistant_id": str(assistant_id) if assistant_id else None}, # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                where_document={}, # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            )
            results = []
            for i in range(len(query_results['ids'][0])): # –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
                results.append(SearchResult(
                    id=UUID(query_results['ids'][0][i]),
                    text=query_results['documents'][0][i],
                    distance=query_results['distances'][0][i],
                    metadata=query_results['metadatas'][0][i]
                ))
            logger.info(f"Search completed, found {len(results)} results")
            return results
        except Exception as e:
            logger.error(f"Error searching vector DB: {e}")
            raise
    def get_client(self): # –î–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        return self.client
</file>

<file path="rag_service/src/main.py">
from fastapi import FastAPI
from src.api.routes import router
from src.config.settings import settings
import structlog
logger = structlog.get_logger()
app = FastAPI(
    title="RAG Service",
    description="Service for Retrieval-Augmented Generation",
    version="0.1.0",
)
app.include_router(router, prefix="/api", tags=["RAG Data"])
@app.on_event("startup")
async def startup_event():
    logger.info("Starting RAG service")
@app.get("/health")
async def health_check():
    """–≠–Ω–¥–ø–æ–∏–Ω—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏."""
    return {"status": "healthy"}
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=settings.API_PORT)
</file>

<file path="rag_service/tests/test_api_routes.py">
import pytest
from fastapi.testclient import TestClient
from unittest.mock import patch, MagicMock
from uuid import uuid4
from src.main import app
from src.models.rag_models import RagData, SearchQuery, SearchResult
client = TestClient(app)
@pytest.fixture
def sample_rag_data():
    return RagData(
        id=uuid4(),
        text="–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç",
        embedding=[0.1, 0.2, 0.3],
        data_type="test_data",
        user_id=123,
        assistant_id=uuid4()
    )
@pytest.fixture
def sample_search_query():
    return SearchQuery(
        query_embedding=[0.1, 0.2, 0.3],
        data_type="test_data",
        user_id=123,
        assistant_id=uuid4(),
        top_k=5
    )
@pytest.fixture
def sample_search_results():
    return [
        SearchResult(
            id=uuid4(),
            text="–†–µ–∑—É–ª—å—Ç–∞—Ç 1",
            distance=0.1,
            metadata={"data_type": "test_data", "user_id": "123", "assistant_id": str(uuid4())}
        ),
        SearchResult(
            id=uuid4(),
            text="–†–µ–∑—É–ª—å—Ç–∞—Ç 2",
            distance=0.2,
            metadata={"data_type": "test_data", "user_id": "123", "assistant_id": str(uuid4())}
        )
    ]
def test_add_data_endpoint(sample_rag_data):
    with patch("src.api.routes.VectorDBService") as mock_service:
        mock_instance = MagicMock()
        mock_service.return_value = mock_instance
        response = client.post("/api/data/add", json=sample_rag_data.model_dump())
        assert response.status_code == 200
        assert response.json()["text"] == sample_rag_data.text
        assert response.json()["data_type"] == sample_rag_data.data_type
        assert response.json()["user_id"] == sample_rag_data.user_id
        assert response.json()["assistant_id"] == str(sample_rag_data.assistant_id)
        mock_instance.add_data.assert_called_once()
def test_search_data_endpoint(sample_search_query, sample_search_results):
    with patch("src.api.routes.VectorDBService") as mock_service:
        mock_instance = MagicMock()
        mock_service.return_value = mock_instance
        mock_instance.search_data.return_value = sample_search_results
        response = client.post("/api/data/search", json=sample_search_query.model_dump())
        assert response.status_code == 200
        results = response.json()
        assert len(results) == 2
        assert results[0]["text"] == "–†–µ–∑—É–ª—å—Ç–∞—Ç 1"
        assert results[0]["distance"] == 0.1
        assert results[1]["text"] == "–†–µ–∑—É–ª—å—Ç–∞—Ç 2"
        assert results[1]["distance"] == 0.2
        mock_instance.search_data.assert_called_once_with(
            query_embedding=sample_search_query.query_embedding,
            data_type=sample_search_query.data_type,
            user_id=sample_search_query.user_id,
            assistant_id=sample_search_query.assistant_id,
            top_k=sample_search_query.top_k
        )
def test_health_check():
    response = client.get("/health")
    assert response.status_code == 200
    assert response.json() == {"status": "healthy"}
</file>

<file path="rag_service/tests/test_vector_db_service.py">
import pytest
from unittest.mock import MagicMock, patch
from uuid import UUID, uuid4
from src.services.vector_db_service import VectorDBService
from src.models.rag_models import RagData, SearchResult
@pytest.fixture
def mock_chroma_client():
    with patch("chromadb.PersistentClient") as mock_client:
        mock_collection = MagicMock()
        mock_client.return_value.get_or_create_collection.return_value = mock_collection
        yield mock_client, mock_collection
@pytest.fixture
def vector_db_service(mock_chroma_client):
    return VectorDBService()
@pytest.fixture
def sample_rag_data():
    return RagData(
        id=uuid4(),
        text="–¢–µ—Å—Ç–æ–≤—ã–π —Ç–µ–∫—Å—Ç",
        embedding=[0.1, 0.2, 0.3],
        data_type="test_data",
        user_id=123,
        assistant_id=uuid4()
    )
@pytest.mark.asyncio
async def test_add_data(vector_db_service, mock_chroma_client, sample_rag_data):
    _, mock_collection = mock_chroma_client
    await vector_db_service.add_data(sample_rag_data)
    mock_collection.add.assert_called_once()
    call_args = mock_collection.add.call_args[1]
    assert call_args["embeddings"] == [sample_rag_data.embedding]
    assert call_args["documents"] == [sample_rag_data.text]
    assert call_args["ids"] == [str(sample_rag_data.id)]
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç embedding, text –∏ id
    metadata = call_args["metadatas"][0]
    assert "embedding" not in metadata
    assert "text" not in metadata
    assert "id" not in metadata
    assert metadata["data_type"] == sample_rag_data.data_type
    assert metadata["user_id"] == str(sample_rag_data.user_id)
    assert metadata["assistant_id"] == str(sample_rag_data.assistant_id)
@pytest.mark.asyncio
async def test_search_data(vector_db_service, mock_chroma_client):
    _, mock_collection = mock_chroma_client
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞
    query_embedding = [0.1, 0.2, 0.3]
    data_type = "test_data"
    user_id = 123
    assistant_id = uuid4()
    top_k = 2
    # –ú–æ–∫–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–ø—Ä–æ—Å–∞
    mock_collection.query.return_value = {
        "ids": [["id1", "id2"]],
        "documents": [["doc1", "doc2"]],
        "distances": [[0.1, 0.2]],
        "metadatas": [[{"data_type": "test_data", "user_id": "123", "assistant_id": str(assistant_id)}, 
                       {"data_type": "test_data", "user_id": "123", "assistant_id": str(assistant_id)}]]
    }
    results = await vector_db_service.search_data(
        query_embedding=query_embedding,
        data_type=data_type,
        user_id=user_id,
        assistant_id=assistant_id,
        top_k=top_k
    )
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–µ—Ç–æ–¥ query –±—ã–ª –≤—ã–∑–≤–∞–Ω —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    mock_collection.query.assert_called_once()
    call_args = mock_collection.query.call_args[1]
    assert call_args["query_embeddings"] == [query_embedding]
    assert call_args["n_results"] == top_k
    assert call_args["where"]["data_type"] == data_type
    assert call_args["where"]["user_id"] == str(user_id)
    assert call_args["where"]["assistant_id"] == str(assistant_id)
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    assert len(results) == 2
    assert isinstance(results[0], SearchResult)
    assert results[0].id == UUID("id1")
    assert results[0].text == "doc1"
    assert results[0].distance == 0.1
    assert results[0].metadata["data_type"] == "test_data"
    assert results[0].metadata["user_id"] == "123"
    assert results[0].metadata["assistant_id"] == str(assistant_id)
</file>

<file path="rag_service/docker-compose.test.yml">
version: '3.8'
services:
  rag_service_test:
    build:
      context: .
      dockerfile: Dockerfile.test
    volumes:
      - .:/app
      - ./chroma_db:/app/chroma_db
    environment:
      - ENVIRONMENT=test
      - LOG_LEVEL=DEBUG
      - API_PORT=8002
      - VECTOR_DB_PATH=/app/chroma_db
    networks:
      - test_network
networks:
  test_network:
    driver: bridge
</file>

<file path="rag_service/Dockerfile">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY rag_service/pyproject.toml rag_service/poetry.lock* ./rag_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd rag_service && \
    poetry install --only main --no-interaction --no-ansi --no-root

# Production stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Install system dependencies (if needed)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code
COPY rag_service/src/ /src/

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
RUN mkdir -p /app/chroma_db

CMD ["python", "src/main.py"]
</file>

<file path="rag_service/Dockerfile.test">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY rag_service/pyproject.toml rag_service/poetry.lock* ./rag_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd rag_service && \
    poetry install --only main,test --no-interaction --no-ansi --no-root

# Test stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    TESTING=1

# Install system dependencies (if needed)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code and tests
COPY rag_service/src/ /src/
COPY rag_service/tests/ /tests/

# –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
RUN mkdir -p /app/chroma_db

CMD ["pytest", "-v", "--cov=src", "--cov-report=term-missing", "/tests/"]
</file>

<file path="rag_service/llm_context_rag.md">
# RAG Service

## –û–±–∑–æ—Ä

RAG (Retrieval-Augmented Generation) —Å–µ—Ä–≤–∏—Å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –°–µ—Ä–≤–∏—Å –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö ChromaDB –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º.

## –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π

```
rag_service/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes.py
‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag_models.py
‚îÇ   ‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vector_db_service.py
‚îÇ   ‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ Dockerfile.test
‚îú‚îÄ‚îÄ docker-compose.test.yml
‚îú‚îÄ‚îÄ pyproject.toml
‚îî‚îÄ‚îÄ llm_context_rag.md
```

## API –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

```
POST /api/data/add
```

–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.

**–¢–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞:**
```json
{
  "text": "–¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ",
  "embedding": [0.1, 0.2, 0.3, ...],
  "data_type": "shared_rule",
  "user_id": 123,
  "assistant_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

**–û—Ç–≤–µ—Ç:**
```json
{
  "id": "550e8400-e29b-41d4-a716-446655440000",
  "text": "–¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ",
  "embedding": [0.1, 0.2, 0.3, ...],
  "data_type": "shared_rule",
  "user_id": 123,
  "assistant_id": "550e8400-e29b-41d4-a716-446655440000",
  "timestamp": "2023-01-01T00:00:00Z"
}
```

### –ü–æ–∏—Å–∫ –¥–∞–Ω–Ω—ã—Ö

```
POST /api/data/search
```

–ò—â–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥—É –∑–∞–ø—Ä–æ—Å–∞.

**–¢–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞:**
```json
{
  "query_embedding": [0.1, 0.2, 0.3, ...],
  "data_type": "shared_rule",
  "user_id": 123,
  "assistant_id": "550e8400-e29b-41d4-a716-446655440000",
  "top_k": 5
}
```

**–û—Ç–≤–µ—Ç:**
```json
[
  {
    "id": "550e8400-e29b-41d4-a716-446655440000",
    "text": "–¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ",
    "distance": 0.123,
    "metadata": {
      "data_type": "shared_rule",
      "user_id": 123,
      "assistant_id": "550e8400-e29b-41d4-a716-446655440000",
      "timestamp": "2023-01-01T00:00:00Z"
    }
  }
]
```

### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏

```
GET /health
```

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞.

**–û—Ç–≤–µ—Ç:**
```json
{
  "status": "healthy"
}
```

## –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Assistant Service

RAG —Å–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å Assistant Service —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç `RAGTool`, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å RAG –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.

## –ó–∞–ø—É—Å–∫ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞

```bash
cd rag_service
poetry install
poetry run python -m src.main
```

### –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤

```bash
cd rag_service
poetry install
poetry run pytest
```

### –ó–∞–ø—É—Å–∫ –≤ Docker

```bash
cd rag_service
docker build -t rag-service .
docker run -p 8002:8002 rag-service
```

### –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –≤ Docker

```bash
cd rag_service
docker compose -f docker-compose.test.yml up --build
```
</file>

<file path="rag_service/pyproject.toml">
[tool.poetry]
name = "rag-service"
version = "0.1.0"
description = "RAG Service for Smart Assistant"
authors = ["Your Name <your.email@example.com>"]
packages = [{ include = "src" }]

[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.109.2"
uvicorn = "^0.27.1"
pydantic = "^2.6.1"
pydantic-settings = "^2.1.0"
python-dotenv = "^1.0.1"
structlog = "^24.1.0"
chromadb = "^0.4.24" # –ü—Ä–∏–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î, –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å
httpx = "^0.26.0"
shared-models = {path = "../shared_models"} # –î–æ–±–∞–≤–∏—Ç—å –æ–±—â–∏–µ –º–æ–¥–µ–ª–∏


[tool.poetry.group.dev.dependencies]
flake8 = "^6.1.0"
flake8-pyproject = "^1.2.3"
mypy = "^1.6.1"
pylint = "^3.0.2"
autoflake = "^2.3.1"

[tool.poetry.group.test.dependencies]
pytest = "^7.4.3"
pytest-asyncio = "^0.21.1"
pytest-cov = "^4.1.0"
pytest-mock = "^3.12.0"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --cov=src --cov-report=term-missing"
asyncio_mode = "auto"

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_optional = true
mypy_path = "src"
</file>

<file path="rag_service/README.md">
# RAG Service

–°–µ—Ä–≤–∏—Å –¥–ª—è Retrieval-Augmented Generation (RAG), –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—â–∏–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.

## –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- –•—Ä–∞–Ω–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ü–æ–∏—Å–∫ –ø–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∞–º —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º ChromaDB
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –¥–∞–Ω–Ω—ã—Ö (–æ–±—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è, –∑–∞–º–µ—Ç–∫–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤)
- –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ —Ç–∏–ø—É –¥–∞–Ω–Ω—ã—Ö, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é –∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É
- REST API –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –∏ –ø–æ–∏—Å–∫–∞ –¥–∞–Ω–Ω—ã—Ö

## –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.11+
- Poetry –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
- Docker –∏ Docker Compose (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –õ–æ–∫–∞–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

1. –ö–ª–æ–Ω–∏—Ä—É–π—Ç–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π:
   ```bash
   git clone <repository-url>
   cd rag_service
   ```

2. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é Poetry:
   ```bash
   poetry install
   ```

3. –°–æ–∑–¥–∞–π—Ç–µ —Ñ–∞–π–ª `.env` –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–º–µ—Ä–∞:
   ```bash
   cp .env.example .env
   ```

4. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–∏—Å:
   ```bash
   poetry run python -m src.main
   ```

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Docker

1. –°–æ–±–µ—Ä–∏—Ç–µ –æ–±—Ä–∞–∑:
   ```bash
   docker build -t rag-service .
   ```

2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä:
   ```bash
   docker run -p 8002:8002 rag-service
   ```

## –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### API –≠–Ω–¥–ø–æ–∏–Ω—Ç—ã

#### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö

```
POST /api/data/add
```

–î–æ–±–∞–≤–ª—è–µ—Ç –Ω–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö.

**–¢–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞:**
```json
{
  "text": "–¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ",
  "embedding": [0.1, 0.2, 0.3, ...],
  "data_type": "shared_rule",
  "user_id": 123,
  "assistant_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

#### –ü–æ–∏—Å–∫ –¥–∞–Ω–Ω—ã—Ö

```
POST /api/data/search
```

–ò—â–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ —ç–º–±–µ–¥–¥–∏–Ω–≥—É –∑–∞–ø—Ä–æ—Å–∞.

**–¢–µ–ª–æ –∑–∞–ø—Ä–æ—Å–∞:**
```json
{
  "query_embedding": [0.1, 0.2, 0.3, ...],
  "data_type": "shared_rule",
  "user_id": 123,
  "assistant_id": "550e8400-e29b-41d4-a716-446655440000",
  "top_k": 5
}
```

#### –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏

```
GET /health
```

–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å —Å–µ—Ä–≤–∏—Å–∞.

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Assistant Service

RAG —Å–µ—Ä–≤–∏—Å –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å Assistant Service —á–µ—Ä–µ–∑ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç `RAGTool`, –∫–æ—Ç–æ—Ä—ã–π –ø–æ–∑–≤–æ–ª—è–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å RAG –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–æ–≤ —Å —É—á–µ—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.

## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –õ–æ–∫–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
poetry run pytest
```

### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Docker

```bash
docker compose -f docker-compose.test.yml up --build
```

## –õ–∏—Ü–µ–Ω–∑–∏—è

MIT
</file>

<file path="rest_service/alembic/versions/20250321_111905_11f7771296eb_initial_migration.py">
"""Initial migration
Revision ID: 11f7771296eb
Revises:
Create Date: 2025-03-21 11:19:05.269325+00:00
"""
from typing import Sequence, Union
import sqlalchemy as sa
from alembic import op
# revision identifiers, used by Alembic.
revision: str = "11f7771296eb"
down_revision: Union[str, None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None
def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "assistant",
        sa.Column("created_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("name", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column("is_secretary", sa.Boolean(), nullable=False),
        sa.Column("model", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column("instructions", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column("assistant_type", sa.String(), nullable=True),
        sa.Column(
            "openai_assistant_id", sqlmodel.sql.sqltypes.AutoString(), nullable=True
        ),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_assistant_is_active"), "assistant", ["is_active"], unique=False
    )
    op.create_index(
        op.f("ix_assistant_is_secretary"), "assistant", ["is_secretary"], unique=False
    )
    op.create_index(op.f("ix_assistant_name"), "assistant", ["name"], unique=False)
    op.create_index(
        op.f("ix_assistant_openai_assistant_id"),
        "assistant",
        ["openai_assistant_id"],
        unique=False,
    )
    op.create_table(
        "telegramuser",
        sa.Column("created_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("telegram_id", sa.BigInteger(), nullable=False),
        sa.Column("username", sqlmodel.sql.sqltypes.AutoString(), nullable=True),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint("telegram_id"),
    )
    op.create_table(
        "calendarcredentials",
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("access_token", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column("refresh_token", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column("token_expiry", sa.DateTime(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["telegramuser.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "cronjob",
        sa.Column("created_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("name", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column(
            "type",
            sa.Enum("NOTIFICATION", "SCHEDULE", name="cronjobtype"),
            nullable=False,
        ),
        sa.Column(
            "cron_expression", sqlmodel.sql.sqltypes.AutoString(), nullable=False
        ),
        sa.Column("user_id", sa.Integer(), nullable=True),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["telegramuser.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "tool",
        sa.Column("created_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("name", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column("tool_type", sa.String(), nullable=True),
        sa.Column("description", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column("input_schema", sqlmodel.sql.sqltypes.AutoString(), nullable=True),
        sa.Column("assistant_id", sa.Uuid(), nullable=True),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.ForeignKeyConstraint(
            ["assistant_id"],
            ["assistant.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_tool_assistant_id"), "tool", ["assistant_id"], unique=False
    )
    op.create_index(op.f("ix_tool_is_active"), "tool", ["is_active"], unique=False)
    op.create_index(op.f("ix_tool_name"), "tool", ["name"], unique=False)
    op.create_table(
        "userassistantthread",
        sa.Column("created_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column("assistant_id", sa.Uuid(), nullable=False),
        sa.Column("thread_id", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.Column("last_used", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["assistant_id"],
            ["assistant.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_userassistantthread_assistant_id"),
        "userassistantthread",
        ["assistant_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_userassistantthread_last_used"),
        "userassistantthread",
        ["last_used"],
        unique=False,
    )
    op.create_index(
        op.f("ix_userassistantthread_user_id"),
        "userassistantthread",
        ["user_id"],
        unique=False,
    )
    op.create_table(
        "usersecretarylink",
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("user_id", sa.Integer(), nullable=False),
        sa.Column("secretary_id", sa.Uuid(), nullable=False),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.Column("created_at", sa.DateTime(), nullable=False),
        sa.Column("updated_at", sa.DateTime(), nullable=False),
        sa.ForeignKeyConstraint(
            ["secretary_id"],
            ["assistant.id"],
        ),
        sa.ForeignKeyConstraint(
            ["user_id"],
            ["telegramuser.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "assistanttoollink",
        sa.Column("created_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("id", sa.Uuid(), nullable=False),
        sa.Column("assistant_id", sa.Uuid(), nullable=False),
        sa.Column("tool_id", sa.Uuid(), nullable=False),
        sa.Column("sub_assistant_id", sa.Uuid(), nullable=True),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.ForeignKeyConstraint(
            ["assistant_id"],
            ["assistant.id"],
        ),
        sa.ForeignKeyConstraint(
            ["sub_assistant_id"],
            ["assistant.id"],
        ),
        sa.ForeignKeyConstraint(
            ["tool_id"],
            ["tool.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_assistanttoollink_assistant_id"),
        "assistanttoollink",
        ["assistant_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_assistanttoollink_is_active"),
        "assistanttoollink",
        ["is_active"],
        unique=False,
    )
    op.create_index(
        op.f("ix_assistanttoollink_sub_assistant_id"),
        "assistanttoollink",
        ["sub_assistant_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_assistanttoollink_tool_id"),
        "assistanttoollink",
        ["tool_id"],
        unique=False,
    )
    op.create_table(
        "cronjobnotification",
        sa.Column("created_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("cron_job_id", sa.Integer(), nullable=True),
        sa.Column("message", sqlmodel.sql.sqltypes.AutoString(), nullable=False),
        sa.ForeignKeyConstraint(
            ["cron_job_id"],
            ["cronjob.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "cronjobrecord",
        sa.Column("created_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("updated_at", sa.TIMESTAMP(), nullable=False),
        sa.Column("id", sa.Integer(), nullable=False),
        sa.Column("cron_job_id", sa.Integer(), nullable=True),
        sa.Column("started_at", sa.DateTime(), nullable=False),
        sa.Column("finished_at", sa.DateTime(), nullable=True),
        sa.Column(
            "status",
            sa.Enum("CREATED", "RUNNING", "DONE", "FAILED", name="cronjobstatus"),
            nullable=False,
        ),
        sa.ForeignKeyConstraint(
            ["cron_job_id"],
            ["cronjob.id"],
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    # ### end Alembic commands ###
def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_table("cronjobrecord")
    op.drop_table("cronjobnotification")
    op.drop_index(op.f("ix_assistanttoollink_tool_id"), table_name="assistanttoollink")
    op.drop_index(
        op.f("ix_assistanttoollink_sub_assistant_id"), table_name="assistanttoollink"
    )
    op.drop_index(
        op.f("ix_assistanttoollink_is_active"), table_name="assistanttoollink"
    )
    op.drop_index(
        op.f("ix_assistanttoollink_assistant_id"), table_name="assistanttoollink"
    )
    op.drop_table("assistanttoollink")
    op.drop_table("usersecretarylink")
    op.drop_index(
        op.f("ix_userassistantthread_user_id"), table_name="userassistantthread"
    )
    op.drop_index(
        op.f("ix_userassistantthread_last_used"), table_name="userassistantthread"
    )
    op.drop_index(
        op.f("ix_userassistantthread_assistant_id"), table_name="userassistantthread"
    )
    op.drop_table("userassistantthread")
    op.drop_index(op.f("ix_tool_name"), table_name="tool")
    op.drop_index(op.f("ix_tool_is_active"), table_name="tool")
    op.drop_index(op.f("ix_tool_assistant_id"), table_name="tool")
    op.drop_table("tool")
    op.drop_table("cronjob")
    op.drop_table("calendarcredentials")
    op.drop_table("telegramuser")
    op.drop_index(op.f("ix_assistant_openai_assistant_id"), table_name="assistant")
    op.drop_index(op.f("ix_assistant_name"), table_name="assistant")
    op.drop_index(op.f("ix_assistant_is_secretary"), table_name="assistant")
    op.drop_index(op.f("ix_assistant_is_active"), table_name="assistant")
    op.drop_table("assistant")
    # ### end Alembic commands ###
</file>

<file path="rest_service/alembic/versions/20250321_122512_b201a35b7385_add_is_active_field_to_cronjob.py">
"""Add is_active field to CronJob
Revision ID: b201a35b7385
Revises: 11f7771296eb
Create Date: 2025-03-21 12:25:12.220885+00:00
"""
from typing import Sequence, Union
import sqlalchemy as sa
from alembic import op
# revision identifiers, used by Alembic.
revision: str = "b201a35b7385"
down_revision: Union[str, None] = "11f7771296eb"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None
def upgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column("cronjob", sa.Column("is_active", sa.Boolean(), nullable=False))
    # ### end Alembic commands ###
def downgrade() -> None:
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_column("cronjob", "is_active")
    # ### end Alembic commands ###
</file>

<file path="rest_service/alembic/env.py">
import logging
import os
import sys
from logging.config import fileConfig
from alembic import context
from sqlalchemy import engine_from_config, pool
from sqlmodel import SQLModel
# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–Ω–µ–≤—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config
# Interpret the config file for Python logging.
# This line sets up loggers basically.
if config.config_file_name is not None:
    fileConfig(config.config_file_name)
# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –º–æ–¥–µ–ª–∏ –∏–∑ –ø–∞–∫–µ—Ç–∞ models
# add your model's MetaData object here
# for 'autogenerate' support
target_metadata = SQLModel.metadata
# –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ —Ç–∞–±–ª–∏—Ü—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤–∏–¥–∏—Ç SQLAlchemy
logger.info("Available tables:")
for table in target_metadata.tables:
    logger.info(f"- {table}")
# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.
def get_url():
    """Get database URL from environment variable."""
    url = os.getenv("ASYNC_DATABASE_URL")
    if not url:
        raise ValueError("ASYNC_DATABASE_URL environment variable is not set")
    # –ó–∞–º–µ–Ω—è–µ–º asyncpg –Ω–∞ psycopg2 –¥–ª—è —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã—Ö –º–∏–≥—Ä–∞—Ü–∏–π
    sync_url = url.replace("+asyncpg", "")
    logger.info(f"Using database URL: {sync_url}")
    return sync_url
def run_migrations_offline() -> None:
    """Run migrations in 'offline' mode.
    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.
    Calls to context.execute() here emit the given string to the
    script output.
    """
    url = get_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )
    with context.begin_transaction():
        context.run_migrations()
def run_migrations_online() -> None:
    """Run migrations in 'online' mode.
    In this scenario we need to create an Engine
    and associate a connection with the context.
    """
    configuration = config.get_section(config.config_ini_section)
    configuration["sqlalchemy.url"] = get_url()
    connectable = engine_from_config(
        configuration,
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )
    with connectable.connect() as connection:
        context.configure(connection=connection, target_metadata=target_metadata)
        with context.begin_transaction():
            context.run_migrations()
if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="rest_service/alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    ${downgrades if downgrades else "pass"}
</file>

<file path="rest_service/src/models/__init__.py">
from .assistant import (
    Assistant,
    AssistantToolLink,
    AssistantType,
    Tool,
    ToolType,
    UserAssistantThread,
)
from .base import BaseModel
from .calendar import CalendarCredentials
from .cron import (
    CronJob,
    CronJobNotification,
    CronJobRecord,
    CronJobStatus,
    CronJobType,
)
from .user import TelegramUser
from .user_secretary import UserSecretaryLink
__all__ = [
    "BaseModel",
    "TelegramUser",
    "CalendarCredentials",
    "Assistant",
    "AssistantType",
    "Tool",
    "ToolType",
    "AssistantToolLink",
    "UserAssistantThread",
    "CronJob",
    "CronJobType",
    "CronJobStatus",
    "CronJobNotification",
    "CronJobRecord",
    "UserSecretaryLink",
]
</file>

<file path="rest_service/src/models/assistant.py">
import enum
from datetime import UTC, datetime
from typing import Annotated, List, Optional
from uuid import UUID, uuid4
from sqlalchemy import Column, String
from sqlmodel import Field, Relationship
from .base import BaseModel
class AssistantType(str, enum.Enum):
    LLM = "llm"  # –ü—Ä—è–º–∞—è —Ä–∞–±–æ—Ç–∞ —Å LLM
    OPENAI_API = "openai_api"  # –†–∞–±–æ—Ç–∞ —á–µ—Ä–µ–∑ OpenAI Assistants API
class ToolType(str, enum.Enum):
    CALENDAR = "calendar"
    REMINDER = "reminder"
    TIME = "time"
    SUB_ASSISTANT = "sub_assistant"
    WEATHER = "weather"
    WEB_SEARCH = "web_search"
class AssistantToolLink(BaseModel, table=True):
    """–°–≤—è–∑—å –º–µ–∂–¥—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º"""
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    assistant_id: UUID = Field(foreign_key="assistant.id", index=True)
    tool_id: UUID = Field(foreign_key="tool.id", index=True)
    sub_assistant_id: Optional[UUID] = Field(
        default=None, foreign_key="assistant.id", index=True
    )
    is_active: bool = Field(default=True, index=True)
    class Config:
        table_name = "assistant_tool_link"
class Assistant(BaseModel, table=True):
    """–ú–æ–¥–µ–ª—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    name: str = Field(index=True)
    is_secretary: bool = Field(default=False, index=True)
    model: str  # gpt-4, gpt-3.5-turbo –∏ —Ç.–¥.
    instructions: str  # –ü—Ä–æ–º–ø—Ç/–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    assistant_type: Annotated[str, AssistantType] = Field(
        sa_column=Column(String), default=AssistantType.LLM.value
    )
    openai_assistant_id: Optional[str] = Field(default=None, index=True)
    is_active: bool = Field(default=True, index=True)
    # Relationships
    tools: List["Tool"] = Relationship(
        back_populates="assistants",
        link_model=AssistantToolLink,
        sa_relationship_kwargs={
            "foreign_keys": [AssistantToolLink.assistant_id],
            "primaryjoin": "Assistant.id == AssistantToolLink.assistant_id",
            "secondaryjoin": (
                "and_(Tool.id == foreign(AssistantToolLink.tool_id), "
                "AssistantToolLink.is_active == True)"
            ),
        },
    )
    user_links: List["UserSecretaryLink"] = Relationship(  # noqa: F821
        back_populates="secretary", sa_relationship_kwargs={"lazy": "selectin"}
    )
    def validate_type(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ç–∏–ø–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
        if self.assistant_type not in [t.value for t in AssistantType]:
            raise ValueError("Invalid assistant type")
class Tool(BaseModel, table=True):
    """–ú–æ–¥–µ–ª—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    name: str = Field(index=True)  # –ù–∞–∑–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    tool_type: Annotated[str, ToolType] = Field(
        sa_column=Column(String), default=ToolType.TIME.value
    )
    description: str
    input_schema: Optional[str] = Field(
        default=None
    )  # JSON —Å—Ö–µ–º–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ –≤–∏–¥–µ —Å—Ç—Ä–æ–∫–∏
    assistant_id: Optional[UUID] = Field(
        default=None, foreign_key="assistant.id", index=True
    )  # –î–ª—è sub_assistant, —Å—Å—ã–ª–∫–∞ –Ω–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, –∫–æ—Ç–æ—Ä–æ–≥–æ –≤—ã–∑—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
    is_active: bool = Field(default=True, index=True)
    # Relationships
    assistants: List[Assistant] = Relationship(
        back_populates="tools",
        link_model=AssistantToolLink,
        sa_relationship_kwargs={
            "foreign_keys": [AssistantToolLink.tool_id],
            "primaryjoin": "Tool.id == AssistantToolLink.tool_id",
            "secondaryjoin": (
                "and_(Assistant.id == foreign(AssistantToolLink.assistant_id), "
                "AssistantToolLink.is_active == True)"
            ),
        },
    )
    def validate_type(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ç–∏–ø–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞"""
        if self.tool_type not in [t.value for t in ToolType]:
            raise ValueError("Invalid tool type")
    def validate_schema(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å JSON —Å—Ö–µ–º—ã"""
        if self.input_schema is not None:
            try:
                import json
                json.loads(self.input_schema)
            except json.JSONDecodeError:
                raise ValueError("Invalid JSON schema")
class UserAssistantThread(BaseModel, table=True):
    """–•—Ä–∞–Ω–µ–Ω–∏–µ thread_id –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    user_id: str = Field(index=True)  # ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏–∑ TelegramUser
    assistant_id: UUID = Field(foreign_key="assistant.id", index=True)
    thread_id: str  # Thread ID –æ—Ç OpenAI
    last_used: datetime = Field(default_factory=lambda: datetime.now(UTC), index=True)
    class Config:
        table_name = "user_assistant_threads"
        unique_together = [
            ("user_id", "assistant_id")
        ]  # –û–¥–∏–Ω —Ç—Ä–µ–¥ –Ω–∞ –ø–∞—Ä—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç
</file>

<file path="rest_service/src/models/base.py">
from datetime import UTC, datetime
from sqlalchemy import TIMESTAMP, event
from sqlmodel import Field, SQLModel
def get_utc_now() -> datetime:
    """Get current UTC time without timezone info"""
    return datetime.now(UTC).replace(tzinfo=None)
class BaseModel(SQLModel):
    created_at: datetime = Field(
        default_factory=get_utc_now, nullable=False, sa_type=TIMESTAMP(timezone=False)
    )
    updated_at: datetime = Field(
        default_factory=get_utc_now, nullable=False, sa_type=TIMESTAMP(timezone=False)
    )
# –°–æ–±—ã—Ç–∏—è –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è
@event.listens_for(BaseModel, "before_update", propagate=True)
def before_update(mapper, connection, target):
    target.updated_at = get_utc_now()
</file>

<file path="rest_service/src/models/calendar.py">
from datetime import datetime
from typing import Optional
from sqlmodel import Field, Relationship, SQLModel
class CalendarCredentials(SQLModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    user_id: int = Field(foreign_key="telegramuser.id")
    access_token: str
    refresh_token: str
    token_expiry: datetime
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
    # Relationships
    user: Optional["TelegramUser"] = Relationship(  # noqa: F821
        back_populates="calendar_credentials"
    )
</file>

<file path="rest_service/src/models/cron.py">
import enum
from datetime import UTC, datetime
from typing import Optional
from sqlmodel import Field, Relationship
from .base import BaseModel
class CronJobType(str, enum.Enum):
    NOTIFICATION = "notification"
    SCHEDULE = "schedule"
class CronJobStatus(str, enum.Enum):
    CREATED = "created"
    RUNNING = "running"
    DONE = "done"
    FAILED = "failed"
class CronJob(BaseModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str
    type: CronJobType = Field(default=CronJobType.NOTIFICATION)
    cron_expression: str
    user_id: Optional[int] = Field(default=None, foreign_key="telegramuser.id")
    is_active: bool = Field(default=True)
    # Relationships
    user: Optional["TelegramUser"] = Relationship(  # noqa: F821
        back_populates="cronjobs"
    )
class CronJobNotification(BaseModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    cron_job_id: Optional[int] = Field(default=None, foreign_key="cronjob.id")
    message: str
class CronJobRecord(BaseModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    cron_job_id: Optional[int] = Field(default=None, foreign_key="cronjob.id")
    started_at: datetime = Field(
        default_factory=lambda: datetime.now(UTC), nullable=False
    )
    finished_at: Optional[datetime] = None
    status: CronJobStatus = Field(default=CronJobStatus.CREATED)
</file>

<file path="rest_service/src/models/user_secretary.py">
"""User-Secretary relationship model"""
from datetime import datetime
from uuid import UUID, uuid4
from sqlmodel import Field, Relationship
from .assistant import Assistant
from .base import BaseModel
from .user import TelegramUser
class UserSecretaryLink(BaseModel, table=True):
    """Model for linking users with their chosen secretary assistants"""
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    user_id: int = Field(foreign_key="telegramuser.id")
    secretary_id: UUID = Field(foreign_key="assistant.id")
    is_active: bool = Field(default=True)
    created_at: datetime = Field(default_factory=datetime.now)
    updated_at: datetime = Field(default_factory=datetime.now)
    # Relationships
    user: "TelegramUser" = Relationship(
        back_populates="secretary_links", sa_relationship_kwargs={"lazy": "selectin"}
    )
    secretary: "Assistant" = Relationship(
        back_populates="user_links", sa_relationship_kwargs={"lazy": "selectin"}
    )
</file>

<file path="rest_service/src/models/user.py">
from typing import List, Optional
from sqlalchemy import BigInteger, Column
from sqlmodel import Field, Relationship
from .base import BaseModel
from .calendar import CalendarCredentials
from .cron import CronJob
class TelegramUser(BaseModel, table=True):
    id: Optional[int] = Field(default=None, primary_key=True)
    telegram_id: int = Field(sa_column=Column(BigInteger, unique=True, nullable=False))
    username: Optional[str]
    # Relationships
    cronjobs: List[CronJob] = Relationship(back_populates="user")
    calendar_credentials: Optional[CalendarCredentials] = Relationship(
        back_populates="user"
    )
    secretary_links: List["UserSecretaryLink"] = Relationship(  # noqa: F821
        back_populates="user", sa_relationship_kwargs={"lazy": "selectin"}
    )
</file>

<file path="rest_service/src/routers/assistant_tools.py">
from uuid import UUID
from database import get_session
from fastapi import APIRouter, Depends, HTTPException
from models.assistant import Assistant, AssistantToolLink, Tool
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
router = APIRouter()
@router.post("/assistants/{assistant_id}/tools/{tool_id}")
async def add_tool_to_assistant(
    assistant_id: UUID, tool_id: UUID, session: AsyncSession = Depends(get_session)
):
    """–î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –∫ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    assistant = await session.get(Assistant, assistant_id)
    if not assistant:
        raise HTTPException(status_code=404, detail="Assistant not found")
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
    tool = await session.get(Tool, tool_id)
    if not tool:
        raise HTTPException(status_code=404, detail="Tool not found")
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ç–∞–∫–∞—è —Å–≤—è–∑—å
    query = select(AssistantToolLink).where(
        AssistantToolLink.assistant_id == assistant_id,
        AssistantToolLink.tool_id == tool_id,
        AssistantToolLink.is_active.is_(True),
    )
    result = await session.execute(query)
    existing_link = result.scalar_one_or_none()
    if existing_link:
        raise HTTPException(
            status_code=400, detail="Tool is already linked to this assistant"
        )
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–≤—è–∑—å
    link = AssistantToolLink(assistant_id=assistant_id, tool_id=tool_id)
    session.add(link)
    await session.commit()
    await session.refresh(link)
    return {"message": "Tool successfully linked to assistant"}
@router.delete("/assistants/{assistant_id}/tools/{tool_id}")
async def remove_tool_from_assistant(
    assistant_id: UUID, tool_id: UUID, session: AsyncSession = Depends(get_session)
):
    """–£–¥–∞–ª–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å–≤—è–∑–∏
    query = select(AssistantToolLink).where(
        AssistantToolLink.assistant_id == assistant_id,
        AssistantToolLink.tool_id == tool_id,
        AssistantToolLink.is_active.is_(True),
    )
    result = await session.execute(query)
    link = result.scalar_one_or_none()
    if not link:
        raise HTTPException(
            status_code=404, detail="Tool is not linked to this assistant"
        )
    # –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º —Å–≤—è–∑—å (soft delete)
    link.is_active = False
    await session.commit()
    return {"message": "Tool successfully unlinked from assistant"}
@router.get("/assistants/{assistant_id}/tools")
async def get_assistant_tools(
    assistant_id: UUID, session: AsyncSession = Depends(get_session)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    assistant = await session.get(Assistant, assistant_id)
    if not assistant:
        raise HTTPException(status_code=404, detail="Assistant not found")
    # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ —Å–≤—è–∑–∏
    query = select(AssistantToolLink).where(
        AssistantToolLink.assistant_id == assistant_id,
        AssistantToolLink.is_active.is_(True),
    )
    result = await session.execute(query)
    links = result.scalars().all()
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
    tools = []
    for link in links:
        tool = await session.get(Tool, link.tool_id)
        if tool:
            tools.append(tool)
    return tools
</file>

<file path="rest_service/src/routers/assistants.py">
from uuid import UUID
from database import get_session
from fastapi import APIRouter, Depends, HTTPException
from models.assistant import Assistant, AssistantType
from pydantic import BaseModel
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
router = APIRouter()
class AssistantCreate(BaseModel):
    name: str
    is_secretary: bool = False
    model: str
    instructions: str
    assistant_type: str = AssistantType.LLM.value  # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Å—Ç—Ä–æ–∫—É
    openai_assistant_id: str = None
class AssistantUpdate(BaseModel):
    name: str = None
    is_secretary: bool = None
    model: str = None
    instructions: str = None
    assistant_type: str = None  # –ü—Ä–∏–Ω–∏–º–∞–µ–º —Å—Ç—Ä–æ–∫—É
    openai_assistant_id: str = None
    is_active: bool = None
@router.get("/assistants/")
async def list_assistants(
    session: AsyncSession = Depends(get_session), skip: int = 0, limit: int = 100
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤"""
    query = select(Assistant).offset(skip).limit(limit)
    result = await session.execute(query)
    assistants = result.scalars().all()
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤—è–∑–∏ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
    for assistant in assistants:
        await session.refresh(assistant, ["tools"])
    return assistants
@router.get("/assistants/{assistant_id}")
async def get_assistant(
    assistant_id: UUID, session: AsyncSession = Depends(get_session)
):
    """–ü–æ–ª—É—á–∏—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø–æ ID"""
    assistant = await session.get(Assistant, assistant_id)
    if not assistant:
        raise HTTPException(status_code=404, detail="Assistant not found")
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–≤—è–∑–∏ —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
    await session.refresh(assistant, ["tools"])
    return assistant
@router.post("/assistants/")
async def create_assistant(
    assistant: AssistantCreate, session: AsyncSession = Depends(get_session)
):
    """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ enum
    assistant_data = assistant.model_dump()
    if assistant_data["assistant_type"] not in [t.value for t in AssistantType]:
        raise HTTPException(status_code=400, detail="Invalid assistant type")
    assistant_data["assistant_type"] = AssistantType(assistant_data["assistant_type"])
    db_assistant = Assistant(**assistant_data)
    session.add(db_assistant)
    await session.commit()
    await session.refresh(db_assistant)
    return db_assistant
@router.put("/assistants/{assistant_id}")
async def update_assistant(
    assistant_id: UUID,
    assistant_update: AssistantUpdate,
    session: AsyncSession = Depends(get_session),
):
    """–û–±–Ω–æ–≤–∏—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    db_assistant = await session.get(Assistant, assistant_id)
    if not db_assistant:
        raise HTTPException(status_code=404, detail="Assistant not found")
    assistant_data = assistant_update.model_dump(exclude_unset=True)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ enum, –µ—Å–ª–∏ —Ç–∏–ø —É–∫–∞–∑–∞–Ω
    if "assistant_type" in assistant_data:
        if assistant_data["assistant_type"] not in [t.value for t in AssistantType]:
            raise HTTPException(status_code=400, detail="Invalid assistant type")
        assistant_data["assistant_type"] = AssistantType(
            assistant_data["assistant_type"]
        )
    for key, value in assistant_data.items():
        setattr(db_assistant, key, value)
    await session.commit()
    await session.refresh(db_assistant)
    return db_assistant
@router.delete("/assistants/{assistant_id}")
async def delete_assistant(
    assistant_id: UUID, session: AsyncSession = Depends(get_session)
):
    """–£–¥–∞–ª–∏—Ç—å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    assistant = await session.get(Assistant, assistant_id)
    if not assistant:
        raise HTTPException(status_code=404, detail="Assistant not found")
    await session.delete(assistant)
    await session.commit()
    return {"message": "Assistant deleted successfully"}
</file>

<file path="rest_service/src/routers/calendar.py">
from datetime import datetime
from typing import Any, Dict, Optional
import structlog
from database import get_session
from fastapi import APIRouter, Depends, HTTPException
from models import CalendarCredentials, TelegramUser
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
logger = structlog.get_logger()
router = APIRouter(prefix="/calendar", tags=["calendar"])
@router.put("/user/{user_id}/token")
async def update_calendar_token(
    user_id: int,
    access_token: str,
    refresh_token: str,
    token_expiry: datetime,
    db: AsyncSession = Depends(get_session),
):
    """Update user's Google Calendar token"""
    result = await db.execute(select(TelegramUser).where(TelegramUser.id == user_id))
    user = result.scalar_one_or_none()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    try:
        # Get or create credentials
        result = await db.execute(
            select(CalendarCredentials).where(CalendarCredentials.user_id == user_id)
        )
        credentials = result.scalar_one_or_none()
        if not credentials:
            credentials = CalendarCredentials(
                user_id=user_id,
                access_token=access_token,
                refresh_token=refresh_token,
                token_expiry=token_expiry,
            )
            db.add(credentials)
        else:
            credentials.access_token = access_token
            credentials.refresh_token = refresh_token
            credentials.token_expiry = token_expiry
            credentials.updated_at = datetime.utcnow()
        await db.commit()
        return {"message": "Token updated successfully"}
    except Exception as e:
        logger.error("Failed to update token", error=str(e))
        raise HTTPException(status_code=500, detail="Failed to update token")
@router.get("/user/{user_id}/token")
async def get_calendar_token(
    user_id: int, db: AsyncSession = Depends(get_session)
) -> Optional[Dict[str, Any]]:
    """Get user's Google Calendar token"""
    try:
        # Get credentials
        result = await db.execute(
            select(CalendarCredentials).where(CalendarCredentials.user_id == user_id)
        )
        credentials = result.scalar_one_or_none()
        if not credentials:
            return None
        return {
            "access_token": credentials.access_token,
            "refresh_token": credentials.refresh_token,
            "token_expiry": credentials.token_expiry.isoformat(),
        }
    except Exception as e:
        logger.error("Failed to get token", error=str(e))
        raise HTTPException(status_code=500, detail="Failed to get token")
</file>

<file path="rest_service/src/routers/cron_jobs.py">
from typing import Optional
from database import get_session
from fastapi import APIRouter, Depends, HTTPException
from models import CronJob, CronJobType, TelegramUser
from pydantic import BaseModel
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
router = APIRouter()
class CronJobCreate(BaseModel):
    name: str
    type: CronJobType
    cron_expression: str
    user_id: int
class CronJobUpdateRequest(BaseModel):
    name: Optional[str]
    type: Optional[CronJobType]
    cron_expression: Optional[str]
    def dict_for_update(self):
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Ç–æ–ª—å–∫–æ —Å —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º–∏ –ø–æ–ª—è–º–∏."""
        return {
            key: value
            for key, value in self.model_dump(exclude_unset=True).items()
            if value is not None
        }
@router.get("/cronjobs/")
async def list_cronjobs(session: AsyncSession = Depends(get_session)):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö CronJob."""
    query = select(CronJob)
    result = await session.execute(query)
    return result.scalars().all()
@router.get("/cronjobs/{cronjob_id}")
async def get_cronjob(cronjob_id: int, session: AsyncSession = Depends(get_session)):
    """–ü–æ–ª—É—á–∏—Ç—å CronJob –ø–æ ID."""
    cronjob = await session.get(CronJob, cronjob_id)
    if not cronjob:
        raise HTTPException(status_code=404, detail="CronJob not found")
    return cronjob
@router.post("/cronjobs/")
async def create_cronjob(
    cronjob_data: CronJobCreate, session: AsyncSession = Depends(get_session)
):
    """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π CronJob."""
    user = await session.get(TelegramUser, cronjob_data.user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    cronjob = CronJob(
        name=cronjob_data.name,
        type=cronjob_data.type,
        cron_expression=cronjob_data.cron_expression,
        user_id=cronjob_data.user_id,
    )
    session.add(cronjob)
    await session.commit()
    await session.refresh(cronjob)
    return cronjob
@router.patch("/cronjobs/{cronjob_id}")
async def update_cronjob(
    cronjob_id: int,
    update_data: CronJobUpdateRequest,
    session: AsyncSession = Depends(get_session),
):
    """–û–±–Ω–æ–≤–∏—Ç—å CronJob –ø–æ ID."""
    cronjob = await session.get(CronJob, cronjob_id)
    if not cronjob:
        raise HTTPException(status_code=404, detail="CronJob not found")
    updates = update_data.dict_for_update()
    for key, value in updates.items():
        setattr(cronjob, key, value)
    session.add(cronjob)
    await session.commit()
    await session.refresh(cronjob)
    return cronjob
@router.delete("/cronjobs/{cronjob_id}")
async def delete_cronjob(cronjob_id: int, session: AsyncSession = Depends(get_session)):
    """–£–¥–∞–ª–∏—Ç—å CronJob –ø–æ ID."""
    cronjob = await session.get(CronJob, cronjob_id)
    if not cronjob:
        raise HTTPException(status_code=404, detail="CronJob not found")
    await session.delete(cronjob)
    await session.commit()
    return {"message": f"CronJob with ID {cronjob_id} has been deleted"}
</file>

<file path="rest_service/src/routers/secretaries.py">
from uuid import UUID
from database import get_session
from fastapi import APIRouter, Depends, HTTPException
from models.assistant import Assistant
from models.user_secretary import UserSecretaryLink
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
router = APIRouter()
@router.get("/secretaries/")
async def list_secretaries(session: AsyncSession = Depends(get_session)):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–µ–∫—Ä–µ—Ç–∞—Ä–µ–π"""
    query = select(Assistant).where(Assistant.is_secretary.is_(True))
    result = await session.execute(query)
    secretaries = result.scalars().all()
    return secretaries
@router.get("/users/{user_id}/secretary")
async def get_user_secretary(
    user_id: int, session: AsyncSession = Depends(get_session)
):
    """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–µ–≥–æ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    query = select(UserSecretaryLink).where(
        UserSecretaryLink.user_id == user_id, UserSecretaryLink.is_active.is_(True)
    )
    result = await session.execute(query)
    link = result.scalar_one_or_none()
    if not link:
        raise HTTPException(
            status_code=404, detail="No active secretary found for user"
        )
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
    await session.refresh(link, ["secretary"])
    return link.secretary
@router.post("/users/{user_id}/secretary/{secretary_id}")
async def set_user_secretary(
    user_id: int, secretary_id: UUID, session: AsyncSession = Depends(get_session)
):
    """–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Å–µ–∫—Ä–µ—Ç–∞—Ä—è –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Å–µ–∫—Ä–µ—Ç–∞—Ä—è
    secretary = await session.get(Assistant, secretary_id)
    if not secretary or not secretary.is_secretary:
        raise HTTPException(status_code=404, detail="Secretary not found")
    # –î–µ–∞–∫—Ç–∏–≤–∏—Ä—É–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ —Å–≤—è–∑–∏
    query = select(UserSecretaryLink).where(
        UserSecretaryLink.user_id == user_id, UserSecretaryLink.is_active.is_(True)
    )
    result = await session.execute(query)
    old_links = result.scalars().all()
    for link in old_links:
        link.is_active = False
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é —Å–≤—è–∑—å
    new_link = UserSecretaryLink(
        user_id=user_id, secretary_id=secretary_id, is_active=True
    )
    session.add(new_link)
    await session.commit()
    await session.refresh(new_link)
    return new_link
</file>

<file path="rest_service/src/routers/threads.py">
from datetime import UTC, datetime
from uuid import UUID
from database import get_session
from fastapi import APIRouter, Depends, HTTPException
from models.assistant import UserAssistantThread
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
router = APIRouter()
@router.get("/users/{user_id}/assistants/{assistant_id}/thread")
async def get_user_assistant_thread(
    user_id: str, assistant_id: UUID, session: AsyncSession = Depends(get_session)
):
    """–ü–æ–ª—É—á–∏—Ç—å thread_id –¥–ª—è –ø–∞—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"""
    query = select(UserAssistantThread).where(
        UserAssistantThread.user_id == user_id,
        UserAssistantThread.assistant_id == assistant_id,
    )
    result = await session.execute(query)
    thread = result.scalar_one_or_none()
    if not thread:
        raise HTTPException(status_code=404, detail="Thread not found")
    return thread
@router.post("/users/{user_id}/assistants/{assistant_id}/thread")
async def create_user_assistant_thread(
    user_id: str,
    assistant_id: UUID,
    thread_id: str,
    session: AsyncSession = Depends(get_session),
):
    """–°–æ–∑–¥–∞—Ç—å –∏–ª–∏ –æ–±–Ω–æ–≤–∏—Ç—å thread_id –¥–ª—è –ø–∞—Ä—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"""
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ç—Ä–µ–¥–∞
    query = select(UserAssistantThread).where(
        UserAssistantThread.user_id == user_id,
        UserAssistantThread.assistant_id == assistant_id,
    )
    result = await session.execute(query)
    existing_thread = result.scalar_one_or_none()
    if existing_thread:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π —Ç—Ä–µ–¥
        existing_thread.thread_id = thread_id
        existing_thread.last_used = datetime.now(UTC)
        await session.commit()
        await session.refresh(existing_thread)
        return existing_thread
    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π —Ç—Ä–µ–¥
    new_thread = UserAssistantThread(
        user_id=user_id, assistant_id=assistant_id, thread_id=thread_id
    )
    session.add(new_thread)
    await session.commit()
    await session.refresh(new_thread)
    return new_thread
</file>

<file path="rest_service/src/routers/tools.py">
import re
from uuid import UUID
from database import get_session
from fastapi import APIRouter, Depends, HTTPException
from models.assistant import Tool, ToolType
from pydantic import BaseModel, validator
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
router = APIRouter()
class ToolCreate(BaseModel):
    name: str
    tool_type: str  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å type –Ω–∞ tool_type
    description: str
    input_schema: str
    assistant_id: UUID = None  # –î–ª—è sub_assistant, —Å—Å—ã–ª–∫–∞ –Ω–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞,
    # –∫–æ—Ç–æ—Ä–æ–≥–æ –≤—ã–∑—ã–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
    is_active: bool = True
    @validator("name")
    def validate_name(cls, v):
        if not re.match(r"^[a-zA-Z0-9_-]+$", v):
            raise ValueError(
                "Tool name must contain only Latin letters, numbers, underscores and hyphens"
            )
        return v
class ToolUpdate(BaseModel):
    name: str = None
    tool_type: str = None  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å type –Ω–∞ tool_type
    description: str = None
    input_schema: str = None
    is_active: bool = None
    @validator("name")
    def validate_name(cls, v):
        if v is not None and not re.match(r"^[a-zA-Z0-9_-]+$", v):
            raise ValueError(
                "Tool name must contain only Latin letters, numbers, underscores and hyphens"
            )
        return v
@router.get("/tools/")
async def list_tools(
    session: AsyncSession = Depends(get_session), skip: int = 0, limit: int = 100
):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤"""
    query = select(Tool).offset(skip).limit(limit)
    result = await session.execute(query)
    return result.scalars().all()
@router.get("/tools/{tool_id}")
async def get_tool(tool_id: UUID, session: AsyncSession = Depends(get_session)):
    """–ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –ø–æ ID"""
    tool = await session.get(Tool, tool_id)
    if not tool:
        raise HTTPException(status_code=404, detail="Tool not found")
    return tool
@router.post("/tools/")
async def create_tool(tool: ToolCreate, session: AsyncSession = Depends(get_session)):
    """–°–æ–∑–¥–∞—Ç—å –Ω–æ–≤—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç"""
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ enum
    tool_data = tool.model_dump()
    print(f"Received tool data: {tool_data}")  # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
    if tool_data["tool_type"] not in [t.value for t in ToolType]:
        raise HTTPException(status_code=400, detail="Invalid tool type")
    tool_data["tool_type"] = ToolType(tool_data["tool_type"])
    db_tool = Tool(**tool_data)
    session.add(db_tool)
    await session.commit()
    await session.refresh(db_tool)
    return db_tool
@router.put("/tools/{tool_id}")
async def update_tool(
    tool_id: UUID, tool_update: ToolUpdate, session: AsyncSession = Depends(get_session)
):
    """–û–±–Ω–æ–≤–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç"""
    db_tool = await session.get(Tool, tool_id)
    if not db_tool:
        raise HTTPException(status_code=404, detail="Tool not found")
    tool_data = tool_update.model_dump(exclude_unset=True)
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å—Ç—Ä–æ–∫—É –≤ enum, –µ—Å–ª–∏ —Ç–∏–ø —É–∫–∞–∑–∞–Ω
    if "tool_type" in tool_data:  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å type –Ω–∞ tool_type
        if tool_data["tool_type"] not in [
            t.value for t in ToolType
        ]:  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å type –Ω–∞ tool_type
            raise HTTPException(status_code=400, detail="Invalid tool type")
        tool_data["tool_type"] = ToolType(
            tool_data["tool_type"]
        )  # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ —Å type –Ω–∞ tool_type
    for key, value in tool_data.items():
        setattr(db_tool, key, value)
    await session.commit()
    await session.refresh(db_tool)
    return db_tool
@router.delete("/tools/{tool_id}")
async def delete_tool(tool_id: UUID, session: AsyncSession = Depends(get_session)):
    """–£–¥–∞–ª–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç"""
    tool = await session.get(Tool, tool_id)
    if not tool:
        raise HTTPException(status_code=404, detail="Tool not found")
    await session.delete(tool)
    await session.commit()
    return {"message": "Tool deleted successfully"}
</file>

<file path="rest_service/src/routers/users.py">
from database import get_session
from fastapi import APIRouter, Depends, HTTPException
from models import TelegramUser
from pydantic import BaseModel
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
router = APIRouter()
class UserCreate(BaseModel):
    telegram_id: int
    username: str = None
@router.post("/users/")
async def create_user(user: UserCreate, session: AsyncSession = Depends(get_session)):
    db_user = TelegramUser(telegram_id=user.telegram_id, username=user.username)
    session.add(db_user)
    await session.commit()
    await session.refresh(db_user)
    return db_user
@router.get("/users/")
async def get_user(telegram_id: int, session: AsyncSession = Depends(get_session)):
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ telegram_id."""
    query = select(TelegramUser).where(TelegramUser.telegram_id == telegram_id)
    result = await session.execute(query)
    user = result.scalar_one_or_none()
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return user
@router.get("/users/{user_id}")
async def get_user_by_id(user_id: int, session: AsyncSession = Depends(get_session)):
    """–ü–æ–ª—É—á–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –ø–æ ID."""
    user = await session.get(TelegramUser, user_id)
    if not user:
        raise HTTPException(status_code=404, detail="User not found")
    return user
@router.get("/users/all/")
async def list_users(session: AsyncSession = Depends(get_session)):
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."""
    query = select(TelegramUser)
    result = await session.execute(query)
    return result.scalars().all()
</file>

<file path="rest_service/src/schemas/base.py">
from datetime import datetime
from pydantic import ConfigDict
from sqlmodel import SQLModel
class BaseSchema(SQLModel):
    model_config = ConfigDict(from_attributes=True)
class TimestampSchema(BaseSchema):
    created_at: datetime
    updated_at: datetime
</file>

<file path="rest_service/src/scripts/fixtures/__init__.py">
"""Database fixtures"""
</file>

<file path="rest_service/src/scripts/fixtures/assistant_tools.py">
"""Assistant-Tool relationship fixtures for database initialization"""
from models import Assistant, AssistantToolLink, Tool
from sqlmodel import select
from sqlmodel.ext.asyncio.session import AsyncSession
async def create_secretary_tools(db: AsyncSession) -> list[AssistantToolLink]:
    """Create Secretary-Tool relationship fixtures"""
    # Get secretary assistant from DB
    query = select(Assistant).where(Assistant.is_secretary.is_(True))
    result = await db.execute(query)
    secretary = result.scalar_one_or_none()
    if not secretary:
        raise ValueError("Secretary assistant not found in database")
    # Get all tools from DB
    query = select(Tool)
    result = await db.execute(query)
    tools = result.scalars().all()
    if not tools:
        raise ValueError("No tools found in database")
    return [
        AssistantToolLink(assistant_id=secretary.id, tool_id=tool.id) for tool in tools
    ]
async def get_all_assistant_tools(db: AsyncSession) -> list[AssistantToolLink]:
    """Get all assistant-tool relationship fixtures"""
    return await create_secretary_tools(db)
</file>

<file path="rest_service/src/scripts/fixtures/assistants.py">
"""Assistant fixtures for database initialization"""
import os
from models import Assistant, AssistantType
def create_secretary_assistant() -> Assistant:
    """Create Secretary assistant fixture"""
    return Assistant(
        name="secretary",
        assistant_type=AssistantType.OPENAI_API,
        is_secretary=True,
        model="gpt-4-turbo-preview",
        description="–í—ã - –ø–æ–ª–µ–∑–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-—Å–µ–∫—Ä–µ—Ç–∞—Ä—å",
        instructions="""–í—ã - —É–º–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç-—Å–µ–∫—Ä–µ—Ç–∞—Ä—å. –í—ã –º–æ–∂–µ—Ç–µ:
1. –û—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
2. –ü–æ–º–æ–≥–∞—Ç—å —Å –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ–º
3. –°–æ–∑–¥–∞–≤–∞—Ç—å –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è
4. –î–µ–ª–µ–≥–∏—Ä–æ–≤–∞—Ç—å —Ç–≤–æ—Ä—á–µ—Å–∫–∏–µ –∑–∞–¥–∞—á–∏ –ø–∏—Å–∞—Ç–µ–ª—é
–í—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–º.""",
        openai_assistant_id=os.getenv("OPEN_API_SECRETAR_ID"),
        is_active=True,
    )
def create_writer_assistant() -> Assistant:
    """Create Writer assistant fixture"""
    return Assistant(
        name="writer",
        assistant_type=AssistantType.LLM,
        is_secretary=False,
        model="gpt-4-turbo-preview",
        description=(
            "–í—ã - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –Ω–∞–ø–∏—Å–∞–Ω–∏—è " "—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤"
        ),
        instructions=(
            "–í—ã - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è –Ω–∞–ø–∏—Å–∞–Ω–∏—è "
            "—Ö—É–¥–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤.\n"
            "–í—ã –¥–æ–ª–∂–Ω—ã:\n"
            "1. –ü–∏—Å–∞—Ç—å –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –∏ —É–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç\n"
            "2. –°–ª–µ–¥–æ–≤–∞—Ç—å —Å—Ç–∏–ª—é –∏ —Ç–æ–Ω—É, —É–∫–∞–∑–∞–Ω–Ω–æ–º—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º\n"
            "3. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ\n"
            "4. –û–∫–∞–∑—ã–≤–∞—Ç—å –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é –ø–æ–º–æ—â—å –≤ –Ω–∞–ø–∏—Å–∞–Ω–∏–∏"
        ),
        is_active=True,
    )
def get_all_assistants() -> list[Assistant]:
    """Get all assistant fixtures"""
    return [create_secretary_assistant(), create_writer_assistant()]
</file>

<file path="rest_service/src/scripts/fixtures/tools.py">
"""Tool fixtures for database initialization"""
from uuid import UUID
from models import Tool, ToolType
def create_time_tool() -> Tool:
    """Create TimeTool fixture"""
    return Tool(
        name="time",
        tool_type=ToolType.TIME.value,
        description="–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è –≤ —É–∫–∞–∑–∞–Ω–Ω–æ–º —á–∞—Å–æ–≤–æ–º –ø–æ—è—Å–µ",
        input_schema=(
            '{"type": "object", "properties": {"timezone": {"type": "string", '
            '"description": "–ß–∞—Å–æ–≤–æ–π –ø–æ—è—Å (–Ω–∞–ø—Ä–∏–º–µ—Ä, Europe/Moscow)"}}, '
            '"required": ["timezone"]}'
        ),
        is_active=True,
    )
def create_calendar_create_tool() -> Tool:
    """Create CalendarCreateTool fixture"""
    return Tool(
        name="calendar_create",
        tool_type=ToolType.CALENDAR.value,
        description=(
            "–°–æ–∑–¥–∞–µ—Ç –Ω–æ–≤–æ–µ —Å–æ–±—ã—Ç–∏–µ –≤ Google Calendar. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: "
            "title (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ), start_time (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ), "
            "end_time (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ), description (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ), "
            "location (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)"
        ),
        input_schema=(
            '{"type": "object", "properties": {"title": {"type": "string"}, '
            '"start_time": {"type": "object", "properties": {"date_time": '
            '{"type": "string", "format": "date-time"}, "time_zone": {"type": '
            '"string", "default": "UTC"}}, "required": ["date_time"]}, '
            '"end_time": {"type": "object", "properties": {"date_time": '
            '{"type": "string", "format": "date-time"}, "time_zone": {"type": '
            '"string", "default": "UTC"}}, "required": ["date_time"]}, '
            '"description": {"type": "string"}, "location": {"type": "string"}}, '
            '"required": ["title", "start_time", "end_time"]}'
        ),
        is_active=True,
    )
def create_calendar_list_tool() -> Tool:
    """Create CalendarListTool fixture"""
    return Tool(
        name="calendar_list",
        tool_type=ToolType.CALENDAR.value,
        description=(
            "–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ —Å–æ–±—ã—Ç–∏–π –∏–∑ Google Calendar. –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: "
            "time_min (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ), time_max (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ). "
            "–ï—Å–ª–∏ –≤—Ä–µ–º—è –Ω–µ —É–∫–∞–∑–∞–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–±—ã—Ç–∏—è –Ω–∞ –±–ª–∏–∂–∞–π—à—É—é –Ω–µ–¥–µ–ª—é."
        ),
        input_schema=(
            '{"type": "object", "properties": {"time_min": {"type": "string", '
            '"format": "date-time"}, "time_max": {"type": "string", '
            '"format": "date-time"}}}'
        ),
        is_active=True,
    )
def create_reminder_tool() -> Tool:
    """Create ReminderTool fixture"""
    return Tool(
        name="reminder",
        tool_type=ToolType.REMINDER.value,
        description=(
            "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–π. –ï—Å—Ç—å –¥–≤–∞ —Å–ø–æ—Å–æ–±–∞ —É–∫–∞–∑–∞—Ç—å "
            "–≤—Ä–µ–º—è –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è: 1. delay_seconds - —á–µ—Ä–µ–∑ —Å–∫–æ–ª—å–∫–æ —Å–µ–∫—É–Ω–¥ "
            "–æ—Ç–ø—Ä–∞–≤–∏—Ç—å –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ 2. datetime_str + timezone - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ "
            "–¥–∞—Ç–∞/–≤—Ä–µ–º—è –∏ —á–∞—Å–æ–≤–æ–π –ø–æ—è—Å"
        ),
        input_schema=(
            '{"type": "object", "properties": {"message": {"type": "string"}, '
            '"delay_seconds": {"type": "integer", "minimum": 1}, '
            '"datetime_str": {"type": "string", "format": "date-time"}, '
            '"timezone": {"type": "string"}}, "required": ["message"], '
            '"oneOf": [{"required": ["delay_seconds"]}, '
            '{"required": ["datetime_str", "timezone"]}]}'
        ),
        is_active=True,
    )
def create_sub_assistant_tool(writer_id: str) -> Tool:
    """Create SubAssistantTool fixture"""
    return Tool(
        name="sub_assistant",
        tool_type=ToolType.SUB_ASSISTANT.value,
        description=(
            "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –¥–µ–ª–µ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–¥–∞—á —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É. "
            "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–æ–∂–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ "
            "—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞, —Ä–µ—à–µ–Ω–∏—è —É–∑–∫–æ—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á."
        ),
        input_schema=(
            '{"type": "object", "properties": {"message": {"type": "string"}}, '
            '"required": ["message"]}'
        ),
        assistant_id=UUID(writer_id),  # Convert string ID to UUID
        is_active=True,
    )
def get_all_tools(writer_id: str) -> list[Tool]:
    """Get all tool fixtures"""
    return [
        create_time_tool(),
        create_calendar_create_tool(),
        create_calendar_list_tool(),
        create_reminder_tool(),
        create_sub_assistant_tool(writer_id),
    ]
</file>

<file path="rest_service/src/scripts/fixtures/users.py">
"""User fixtures"""
from models import TelegramUser
def create_test_users() -> list[TelegramUser]:
    """Create test users fixtures"""
    return [
        TelegramUser(telegram_id=625038902, username="vladmesh"),
        TelegramUser(telegram_id=7192299, username="vladislav_mesh88k"),
    ]
</file>

<file path="rest_service/src/scripts/__init__.py">
"""Scripts package"""
</file>

<file path="rest_service/src/scripts/check_tools.py">
import asyncio
from database import AsyncSessionLocal
from models import Tool
from sqlmodel import select
async def main():
    session = AsyncSessionLocal()
    print("Tools in database:")
    tools = await session.exec(select(Tool))
    for tool in tools:
        print(f"- {tool.name} ({tool.tool_type})")
    await session.close()
if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="rest_service/src/scripts/test_data.py">
"""Test data initialization script"""
from models import Assistant, TelegramUser
from sqlalchemy import select
from sqlmodel.ext.asyncio.session import AsyncSession
from scripts.fixtures.assistant_tools import get_all_assistant_tools
from scripts.fixtures.assistants import get_all_assistants
from scripts.fixtures.tools import get_all_tools
async def create_test_data(db: AsyncSession) -> None:
    """Create test data in the database"""
    # Create assistants first to get their IDs
    assistants = get_all_assistants()
    for assistant in assistants:
        db.add(assistant)
    await db.commit()
    # Get writer assistant ID
    query = select(Assistant).where(Assistant.name == "writer")
    result = await db.execute(query)
    writer = result.scalar_one_or_none()
    if not writer:
        raise ValueError("Writer assistant not found in database")
    # Create tools with writer_id
    tools = get_all_tools(str(writer.id))
    for tool in tools:
        db.add(tool)
    await db.commit()
    # Create assistant-tool relationships
    assistant_tools = await get_all_assistant_tools(db)
    for assistant_tool in assistant_tools:
        db.add(assistant_tool)
    await db.commit()
    # Create test users
    test_users = [
        TelegramUser(telegram_id=625038902, username="vladmesh", is_active=True),
        TelegramUser(telegram_id=7192299, username="vladislav_mesh88k", is_active=True),
    ]
    for user_obj in test_users:
        db.add(user_obj)
    await db.commit()
</file>

<file path="rest_service/src/config.py">
"""Application configuration"""
import os
from pydantic_settings import BaseSettings
class Settings(BaseSettings):
    """Application settings"""
    ASYNC_DATABASE_URL: str = os.getenv(
        "ASYNC_DATABASE_URL", "postgresql+asyncpg://postgres:postgres@db:5432/postgres"
    )
    DB_ECHO: bool = False
    DB_POOL_SIZE: int = 5
    DB_MAX_OVERFLOW: int = 10
settings = Settings()
</file>

<file path="rest_service/src/database.py">
"""Database initialization and session management"""
from config import settings
from sqlalchemy import text
from sqlalchemy.ext.asyncio import create_async_engine
from sqlalchemy.orm import sessionmaker
from sqlmodel import SQLModel
from sqlmodel.ext.asyncio.session import AsyncSession
# Create async engine
async_engine = create_async_engine(
    settings.ASYNC_DATABASE_URL,
    echo=settings.DB_ECHO,
    pool_pre_ping=True,
    pool_size=settings.DB_POOL_SIZE,
    max_overflow=settings.DB_MAX_OVERFLOW,
)
# Create session factory
AsyncSessionLocal = sessionmaker(
    async_engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autocommit=False,
    autoflush=False,
)
async def drop_all_tables() -> None:
    """Drop and recreate public schema"""
    async with async_engine.begin() as conn:
        await conn.execute(text("DROP SCHEMA public CASCADE"))
        await conn.execute(text("CREATE SCHEMA public"))
async def init_db(drop_tables: bool = False, create_tables: bool = False) -> None:
    """Initialize database with tables and test data
    Args:
        drop_tables: If True, drops and recreates public schema
        create_tables: If True, creates all tables from models
    """
    if drop_tables:
        await drop_all_tables()
    if create_tables:
        async with async_engine.begin() as conn:
            await conn.run_sync(SQLModel.metadata.create_all)
    # Create test data
    # async with AsyncSessionLocal() as session:
    #    await create_test_data(session)
async def get_session() -> AsyncSession:
    """Get database session"""
    session = AsyncSessionLocal()
    try:
        yield session
    finally:
        await session.close()
</file>

<file path="rest_service/src/main.py">
import logging
from contextlib import asynccontextmanager
from database import init_db
from fastapi import FastAPI, Request
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from routers import (
    assistant_tools,
    assistants,
    calendar,
    cron_jobs,
    secretaries,
    tools,
    users,
)
# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await init_db()
    yield
    # Shutdown
app = FastAPI(lifespan=lifespan, title="Assistant Service API")
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫ 422 —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º."""
    errors = exc.errors()
    logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {errors}")
    return JSONResponse(
        status_code=422,
        content={"detail": errors},
    )
# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ä–æ—É—Ç–µ—Ä–æ–≤
app.include_router(users.router, prefix="/api", tags=["Users"])
app.include_router(cron_jobs.router, prefix="/api", tags=["Cron Jobs"])
app.include_router(calendar.router, prefix="/api", tags=["Calendar"])
app.include_router(assistants.router, prefix="/api", tags=["Assistants"])
app.include_router(tools.router, prefix="/api", tags=["Tools"])
app.include_router(assistant_tools.router, prefix="/api", tags=["Assistant Tools"])
app.include_router(secretaries.router, prefix="/api", tags=["Secretaries"])
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy"}
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
</file>

<file path="rest_service/src/models.py">
from models.base import BaseModel
from models.calendar import CalendarCredentials
from models.cron import (
    CronJob,
    CronJobNotification,
    CronJobRecord,
    CronJobStatus,
    CronJobType,
)
from models.user import TelegramUser
from .models.assistant import Assistant, AssistantInstructions, AssistantType
__all__ = [
    "BaseModel",
    "TelegramUser",
    "CalendarCredentials",
    "CronJob",
    "CronJobType",
    "CronJobStatus",
    "CronJobNotification",
    "CronJobRecord",
    "Assistant",
    "AssistantType",
    "AssistantInstructions",
]
</file>

<file path="rest_service/tests/test_dummy.py">
def test_dummy():
    """–ü—É—Å—Ç–æ–π —Ç–µ—Å—Ç –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–æ–µ–∫—Ç–∞."""
    assert True
</file>

<file path="rest_service/alembic.ini">
[alembic]
# path to migration scripts
script_location = alembic

# template used to generate migration files
file_template = %%(year)d%%(month).2d%%(day).2d_%%(hour).2d%%(minute).2d%%(second).2d_%%(rev)s_%%(slug)s

# timezone to use when rendering the date
# within the migration file as well as the filename.
# string value is passed to dateutil.tz.gettz()
# leave blank for localtime
timezone = UTC

# max length of characters to apply to the
# "slug" field
truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
sourceless = false

# version location specification; this defaults
# to alembic/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path
version_locations = %(here)s/alembic/versions

# the output encoding used when revision files
# are written from script.py.mako
output_encoding = utf-8

# URL –±—É–¥–µ—Ç –ø–æ–¥—Å—Ç–∞–≤–ª—è—Ç—å—Å—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
sqlalchemy.url = 

[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="rest_service/docker-compose.test.yml">
version: '3.8'
services:
  test:
    build:
      context: ..
      dockerfile: rest_service/Dockerfile.test
    environment:
      - TESTING=1
      - PYTHONPATH=/src
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - ASYNC_DATABASE_URL=postgresql+asyncpg://test_user:test_password@test-db:5432/test_db
    env_file:
      - ./tests/.env.test
    depends_on:
      test-db:
        condition: service_healthy
  test-db:
    image: postgres:16
    environment:
      - POSTGRES_USER=test_user
      - POSTGRES_PASSWORD=test_password
      - POSTGRES_DB=test_db
    volumes:
      - test_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U test_user -d test_db"]
      interval: 5s
      timeout: 5s
      retries: 5
volumes:
  test_db_data:
</file>

<file path="rest_service/Dockerfile">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY rest_service/pyproject.toml rest_service/poetry.lock* ./rest_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd rest_service && \
    poetry install --only main --no-interaction --no-ansi --no-root

# Production stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code
COPY rest_service/src/ /src/

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
COPY rest_service/alembic/ ./alembic/
COPY rest_service/manage.py ./manage.py
COPY rest_service/alembic.ini ./alembic.ini

CMD ["python", "src/main.py"]
</file>

<file path="rest_service/Dockerfile.test">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY rest_service/pyproject.toml rest_service/poetry.lock* ./rest_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd rest_service && \
    poetry install --only main,test --no-interaction --no-ansi --no-root

# Test stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    TESTING=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code and tests
COPY rest_service/src/ /src/
COPY rest_service/tests/ /tests/

CMD ["pytest", "-v", "--cov=src", "--cov-report=term-missing", "/tests/"]
</file>

<file path="rest_service/llm_context_rest.md">
# REST Service ‚Äì Detailed Overview

## 1. Overview
- **Purpose:** Provides a RESTful API to manage user data, assistant configurations, and operational data.
- **Functions:**
  - CRUD operations for assistants, tools, and scheduled tasks
  - User management and authentication
  - Calendar integration
  - Task scheduling
  - Assistant-user mapping
- **Tech Stack:** 
  - FastAPI for API endpoints
  - SQLModel/SQLAlchemy for ORM
  - PostgreSQL for data storage
  - Alembic for migrations

## 2. Directory Structure
```
rest_service/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ main.py              # FastAPI application entry point
‚îÇ   ‚îú‚îÄ‚îÄ database.py          # Database connection and ORM setup
‚îÇ   ‚îú‚îÄ‚îÄ config.py            # Configuration settings
‚îÇ   ‚îú‚îÄ‚îÄ models/              # Database models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py          # Base model with timestamps
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assistant.py     # Assistant and tool models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calendar.py      # Calendar integration models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cron.py          # Task scheduling models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py          # User models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user_secretary.py # User-secretary mapping
‚îÇ   ‚îú‚îÄ‚îÄ routers/             # API endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ assistants.py    # Assistant management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ tools.py         # Tool management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ users.py         # User management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ calendar.py      # Calendar integration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cron_jobs.py     # Task scheduling
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ secretaries.py   # Secretary management
‚îÇ   ‚îî‚îÄ‚îÄ schemas/             # Pydantic models
‚îî‚îÄ‚îÄ tests/                   # Test suite
```

## 3. Data Models

### 3.1 Base Model
```python
class BaseModel(SQLModel):
    created_at: datetime = Field(default_factory=get_utc_now)
    updated_at: datetime = Field(default_factory=get_utc_now)
```

### 3.2 Assistant Models
```python
class AssistantType(str, enum.Enum):
    LLM = "llm"
    OPENAI_API = "openai_api"

class Assistant(BaseModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    name: str
    is_secretary: bool
    model: str
    instructions: str
    assistant_type: AssistantType
```

### 3.3 Tool Models
```python
class ToolType(str, enum.Enum):
    CALENDAR = "calendar"
    REMINDER = "reminder"
    TIME = "time"
    SUB_ASSISTANT = "sub_assistant"
    WEATHER = "weather"

class Tool(BaseModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    name: str
    tool_type: ToolType
    description: str
    input_schema: dict
```

### 3.4 User Models
```python
class TelegramUser(BaseModel, table=True):
    id: int = Field(primary_key=True)
    telegram_id: int = Field(unique=True)
    username: Optional[str]
    
    # Relationships
    cronjobs: List[CronJob]
    calendar_credentials: Optional[CalendarCredentials]
    secretary_links: List[UserSecretaryLink]
```

### 3.5 Calendar Models
```python
class CalendarCredentials(SQLModel, table=True):
    id: int = Field(primary_key=True)
    user_id: int = Field(foreign_key="telegramuser.id")
    access_token: str
    refresh_token: str
    token_expiry: datetime
```

### 3.6 Cron Models
```python
class CronJobType(str, enum.Enum):
    NOTIFICATION = "notification"
    SCHEDULE = "schedule"

class CronJob(BaseModel, table=True):
    id: int = Field(primary_key=True)
    name: str
    type: CronJobType
    cron_expression: str
    user_id: int = Field(foreign_key="telegramuser.id")
    is_active: bool
```

### 3.7 User-Secretary Mapping
```python
class UserSecretaryLink(BaseModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    user_id: int = Field(foreign_key="telegramuser.id")
    secretary_id: UUID = Field(foreign_key="assistant.id")
    is_active: bool
```

## 4. API Endpoints

### 4.1 Assistant Management
- **GET /api/assistants**: List all assistants
- **POST /api/assistants**: Create new assistant
- **GET /api/assistants/{id}**: Get assistant details
- **PUT /api/assistants/{id}**: Update assistant
- **DELETE /api/assistants/{id}**: Delete assistant

### 4.2 Tool Management
- **GET /api/tools**: List all tools
- **POST /api/tools**: Create new tool
- **GET /api/tools/{id}**: Get tool details
- **PUT /api/tools/{id}**: Update tool
- **DELETE /api/tools/{id}**: Delete tool

### 4.3 User Management
- **GET /api/users**: List all users
- **POST /api/users**: Create new user
- **GET /api/users/{id}**: Get user details
- **PUT /api/users/{id}**: Update user
- **DELETE /api/users/{id}**: Delete user

### 4.4 Calendar Integration
- **GET /api/calendar/{user_id}**: Get calendar credentials
- **POST /api/calendar/{user_id}**: Store calendar credentials
- **DELETE /api/calendar/{user_id}**: Remove calendar credentials

### 4.5 Task Scheduling
- **GET /api/cron-jobs**: List all scheduled tasks
- **POST /api/cron-jobs**: Create new task
- **GET /api/cron-jobs/{id}**: Get task details
- **PUT /api/cron-jobs/{id}**: Update task
- **DELETE /api/cron-jobs/{id}**: Delete task

## 5. Configuration

### 5.1 Settings
```python
class Settings(BaseSettings):
    ASYNC_DATABASE_URL: str
    DB_ECHO: bool = False
    DB_POOL_SIZE: int = 5
    DB_MAX_OVERFLOW: int = 10
```

### 5.2 Environment Variables
- `ASYNC_DATABASE_URL`: Database connection string
- `DB_ECHO`: SQL query logging
- `DB_POOL_SIZE`: Connection pool size
- `DB_MAX_OVERFLOW`: Maximum overflow connections

## 6. Database Management

### 6.1 Connection Setup
```python
async_engine = create_async_engine(
    settings.ASYNC_DATABASE_URL,
    echo=settings.DB_ECHO,
    pool_pre_ping=True,
    pool_size=settings.DB_POOL_SIZE,
    max_overflow=settings.DB_MAX_OVERFLOW,
)
```

### 6.2 Session Management
```python
AsyncSessionLocal = sessionmaker(
    async_engine,
    class_=AsyncSession,
    expire_on_commit=False,
    autocommit=False,
    autoflush=False,
)
```

## 7. Testing

### 7.1 Test Structure
- **Unit Tests:**
  - Model validation
  - API endpoints
  - Database operations

### 7.2 Test Environment
- Isolated test database
- Mock services
- Environment variables
- Test fixtures

## 8. Best Practices

### 8.1 Development Guidelines
- Use type hints
- Follow naming conventions
- Document changes
- Maintain test coverage

### 8.2 Error Handling
- Structured logging
- Validation errors
- Database errors
- API errors

### 8.3 Performance
- Connection pooling
- Query optimization
- Caching
- Indexing
</file>

<file path="rest_service/manage.py">
#!/usr/bin/env python
import os
import sys
from typing import List
def run_alembic_command(args: List[str]) -> None:
    """Run alembic command with given arguments."""
    os.environ.setdefault("PYTHONPATH", "/app")
    from alembic.config import Config
    alembic_cfg = Config("alembic.ini")
    command = args[0]
    if command == "revision":
        from alembic.command import revision
        message = args[2]
        revision(alembic_cfg, message=message, autogenerate=True)
    elif command == "upgrade":
        from alembic.command import upgrade
        upgrade(alembic_cfg, "head")
    elif command == "downgrade":
        from alembic.command import downgrade
        revision = args[1] if len(args) > 1 else "-1"
        downgrade(alembic_cfg, revision)
    elif command == "current":
        from alembic.command import current
        current(alembic_cfg)
    elif command == "history":
        from alembic.command import history
        history(alembic_cfg)
    else:
        print(f"Unknown command: {command}")
        sys.exit(1)
if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python manage.py <command> [args...]")
        print("Available commands:")
        print("  migrate - Create a new migration")
        print("  upgrade - Apply migrations")
        print("  downgrade - Rollback migrations")
        print("  current - Show current migration")
        print("  history - Show migration history")
        sys.exit(1)
    command = sys.argv[1]
    args = sys.argv[2:]
    if command == "migrate":
        if not args:
            print("Usage: python manage.py migrate <message>")
            sys.exit(1)
        run_alembic_command(["revision", "--autogenerate", args[0]])
    elif command == "upgrade":
        run_alembic_command(["upgrade"])
    elif command == "downgrade":
        if not args:
            print("Usage: python manage.py downgrade <revision>")
            sys.exit(1)
        run_alembic_command(["downgrade", args[0]])
    elif command == "current":
        run_alembic_command(["current"])
    elif command == "history":
        run_alembic_command(["history"])
    else:
        print(f"Unknown command: {command}")
        sys.exit(1)
</file>

<file path="rest_service/pyproject.toml">
[tool.poetry]
name = "rest-service"
version = "0.1.0"
description = "REST API service for Smart Assistant"
authors = ["Your Name <your.email@example.com>"]
packages = [{ include = "rest_service/src" }]

[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.109.2"
uvicorn = "^0.27.1"
sqlmodel = "^0.0.23"
python-jose = {extras = ["cryptography"], version = "^3.3.0"}
passlib = {extras = ["bcrypt"], version = "^1.7.4"}
python-multipart = "^0.0.9"
pydantic-settings = "^2.1.0"
psycopg2-binary = "^2.9.9"
asyncpg = "^0.29.0"
python-dateutil = "^2.8.2"
croniter = "^2.0.1"
openai = "^1.12.0"
python-dotenv = "^1.0.1"
httpx = "^0.26.0"
starlette = ">=0.36.3,<0.37.0"
structlog = ">=24.1.0"
pydantic = "^2.6.1"
trio = "^0.24.0"
alembic = "^1.13.1"
shared-models = {path = "../shared_models"}

[tool.poetry.group.dev.dependencies]
black = "^23.10.1"
isort = "^5.12.0"
flake8 = "^6.1.0"
flake8-pyproject = "^1.2.3"
mypy = "^1.6.1"
pylint = "^3.0.2"
autoflake = "^2.3.1"

[tool.poetry.group.test.dependencies]
pytest = "^8.0.0"
pytest-asyncio = "^0.23.5"
pytest-cov = "^4.1.0"
pytest-env = "^1.1.3"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --cov=src --cov-report=term-missing"
asyncio_mode = "auto"
</file>

<file path="rest_service/pytest.ini">
[pytest]
asyncio_mode = strict
asyncio_default_fixture_loop_scope = function
</file>

<file path="rest_service/requirements.txt">
fastapi==0.109.2
uvicorn==0.27.1
sqlmodel==0.0.23
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-multipart==0.0.9
pydantic-settings==2.1.0
psycopg2-binary==2.9.9
asyncpg==0.29.0
python-dateutil==2.8.2
croniter==2.0.1
openai==1.12.0
python-dotenv==1.0.1
pytest==8.0.0
httpx==0.26.0
pytest-asyncio==0.23.5
pytest-cov==4.1.0
pytest-env==1.1.3
starlette>=0.36.3,<0.37.0
structlog>=24.1.0
pydantic==2.6.1
trio==0.24.0
alembic==1.13.1
</file>

<file path="rest_service/run_tests.sh">
#!/bin/bash
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å–æ–æ–±—â–µ–Ω–∏–π —Å —Ü–≤–µ—Ç–æ–º
print_message() {
    GREEN='\033[0;32m'
    RED='\033[0;31m'
    NC='\033[0m'
    if [ "$2" = "success" ]; then
        echo -e "${GREEN}$1${NC}"
    else
        echo -e "${RED}$1${NC}"
    fi
}
cd "$(dirname "$0")"
# –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã —á–µ—Ä–µ–∑ docker compose
print_message "=== Running tests ===" "success"
docker compose -f docker-compose.test.yml up --build --abort-on-container-exit
# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–æ–≤
TEST_RESULT=$?
# –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∏ —É–¥–∞–ª—è–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
print_message "=== Cleaning up ===" "success"
docker compose -f docker-compose.test.yml down
# –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Ç–µ—Å—Ç–æ–≤
if [ $TEST_RESULT -eq 0 ]; then
    print_message "=== Tests passed successfully! ===" "success"
else
    print_message "=== Tests failed! ===" "error"
fi
exit $TEST_RESULT
</file>

<file path="scripts/format.py">
#!/usr/bin/env python3
"""Script for running code formatters locally."""
import subprocess
from pathlib import Path
def get_services() -> list[str]:
    """Get list of all services."""
    return [
        "assistant_service",
        "rest_service",
        "google_calendar_service",
        "cron_service",
        "telegram_bot_service",
    ]
def run_formatters(service: str | None = None) -> int:
    """Run formatters on Python files locally.
    Args:
        service: Optional service name to format. If None, formats all services.
    Returns:
        Exit code (0 for success, non-zero for failure)
    """
    services = [service] if service else get_services()
    for svc in services:
        service_path = Path(svc)
        if not service_path.exists():
            print(f"Service {svc} not found, skipping...")
            continue
        print(f"\nRunning formatters on {svc} locally...")
        # Run black
        black_result = subprocess.run(
            ["poetry", "run", "black", "--check", str(service_path)],
            capture_output=True,
            text=True,
        )
        if black_result.returncode != 0:
            print("Black found formatting issues:")
            print(black_result.stdout)
            return black_result.returncode
        # Run isort
        isort_result = subprocess.run(
            ["poetry", "run", "isort", "--check-only", str(service_path)],
            capture_output=True,
            text=True,
        )
        if isort_result.returncode != 0:
            print("isort found import sorting issues:")
            print(isort_result.stdout)
            return isort_result.returncode
    return 0
def main():
    """Format all Python files in the project."""
    import subprocess
    import sys
    def run_tool(cmd):
        try:
            subprocess.run(cmd, check=True)
            return True
        except subprocess.CalledProcessError:
            return False
    isort_success = run_tool(["isort", "."])
    black_success = run_tool(["black", "."])
    if not (isort_success and black_success):
        sys.exit(1)
if __name__ == "__main__":
    main()
</file>

<file path="scripts/lint.py">
#!/usr/bin/env python3
"""Script for running code linters locally."""
import subprocess
import sys
from pathlib import Path
def get_services() -> list[str]:
    """Get list of all services."""
    return [
        "assistant_service",
        "rest_service",
        "google_calendar_service",
        "cron_service",
        "telegram_bot_service",
    ]
def run_linters(service: str | None = None) -> int:
    """Run linters on Python files locally.
    Args:
        service: Optional service name to lint. If None, lints all services.
    Returns:
        Exit code (0 for success, non-zero for failure)
    """
    services = [service] if service else get_services()
    for svc in services:
        service_path = Path(svc)
        if not service_path.exists():
            print(f"Service {svc} not found, skipping...")
            continue
        print(f"\nRunning linters on {svc} locally...")
        # Run flake8
        flake8_result = subprocess.run(
            ["poetry", "run", "flake8", str(service_path)],
            capture_output=True,
            text=True,
        )
        if flake8_result.returncode != 0:
            print("flake8 found issues:")
            print(flake8_result.stdout)
            return flake8_result.returncode
        # Run mypy
        mypy_result = subprocess.run(
            ["poetry", "run", "mypy", str(service_path)],
            capture_output=True,
            text=True,
        )
        if mypy_result.returncode != 0:
            print("mypy found type issues:")
            print(mypy_result.stdout)
            return mypy_result.returncode
        # Run pylint
        pylint_result = subprocess.run(
            ["poetry", "run", "pylint", str(service_path)],
            capture_output=True,
            text=True,
        )
        if pylint_result.returncode != 0:
            print("pylint found issues:")
            print(pylint_result.stdout)
            return pylint_result.returncode
    return 0
def main() -> int:
    """Main entry point."""
    service = sys.argv[1] if len(sys.argv) > 1 else None
    return run_linters(service)
if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="shared_models/src/shared_models/__init__.py">
from shared_models.queue import (
    HumanQueueMessage,
    QueueMessage,
    QueueMessageContent,
    QueueMessageSource,
    QueueMessageType,
    ToolQueueMessage,
)
__all__ = [
    "QueueMessage",
    "QueueMessageContent",
    "QueueMessageSource",
    "QueueMessageType",
    "ToolQueueMessage",
    "HumanQueueMessage",
]
</file>

<file path="shared_models/src/shared_models/queue.py">
from datetime import datetime
from enum import Enum
from typing import Any, Dict, Optional
from pydantic import BaseModel, Field
class QueueMessageType(str, Enum):
    """Types of messages in queue"""
    TOOL = "tool_message"
    HUMAN = "human_message"
class QueueMessageSource(str, Enum):
    """Sources of messages in queue"""
    CRON = "cron"
    CALENDAR = "calendar"
    USER = "user"
class QueueMessageContent(BaseModel):
    """Content of queue message"""
    message: str = Field(..., description="Message text")
    metadata: Dict[str, Any] = Field(
        default_factory=dict, description="Additional data"
    )
class QueueMessage(BaseModel):
    """Base class for all queue messages"""
    type: QueueMessageType = Field(..., description="Message type")
    user_id: int = Field(..., description="User ID in database")
    source: QueueMessageSource = Field(..., description="Message source")
    content: QueueMessageContent = Field(..., description="Message content")
    timestamp: datetime = Field(
        default_factory=lambda: datetime.now(), description="Message creation time"
    )
    def to_dict(self) -> Dict[str, Any]:
        """Convert message to dictionary"""
        return self.model_dump()
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "QueueMessage":
        """Create message from dictionary"""
        return cls(**data)
class ToolQueueMessage(QueueMessage):
    """Message from tool"""
    type: QueueMessageType = Field(
        default=QueueMessageType.TOOL, description="Message type"
    )
    tool_name: str = Field(..., description="Tool name")
class HumanQueueMessage(QueueMessage):
    """Message from user"""
    type: QueueMessageType = Field(
        default=QueueMessageType.HUMAN, description="Message type"
    )
    chat_id: Optional[int] = Field(None, description="Telegram chat ID")
</file>

<file path="shared_models/tests/conftest.py">
import pytest
from shared_models import (
    QueueMessage,
    QueueMessageContent,
    QueueMessageSource,
    QueueMessageType,
)
@pytest.fixture
def queue_message():
    """Fixture for basic QueueMessage"""
    content = QueueMessageContent(message="test message", metadata={"key": "value"})
    return QueueMessage(
        type=QueueMessageType.TOOL,
        user_id=1,
        source=QueueMessageSource.USER,
        content=content,
    )
</file>

<file path="shared_models/tests/test_queue.py">
from datetime import datetime
import pytest
from shared_models import (
    HumanQueueMessage,
    QueueMessage,
    QueueMessageContent,
    QueueMessageSource,
    QueueMessageType,
    ToolQueueMessage,
)
def test_queue_message_creation():
    """Test basic QueueMessage creation"""
    content = QueueMessageContent(message="test message", metadata={"key": "value"})
    message = QueueMessage(
        type=QueueMessageType.TOOL,
        user_id=1,
        source=QueueMessageSource.USER,
        content=content,
    )
    assert message.type == QueueMessageType.TOOL
    assert message.user_id == 1
    assert message.source == QueueMessageSource.USER
    assert message.content.message == "test message"
    assert message.content.metadata == {"key": "value"}
    assert isinstance(message.timestamp, datetime)
def test_tool_queue_message():
    """Test ToolQueueMessage creation"""
    content = QueueMessageContent(message="tool message")
    message = ToolQueueMessage(
        user_id=1,
        source=QueueMessageSource.USER,
        content=content,
        tool_name="test_tool",
    )
    assert message.type == QueueMessageType.TOOL
    assert message.tool_name == "test_tool"
def test_human_queue_message():
    """Test HumanQueueMessage creation"""
    content = QueueMessageContent(message="human message")
    message = HumanQueueMessage(
        user_id=1,
        source=QueueMessageSource.USER,
        content=content,
        chat_id=123456,
    )
    assert message.type == QueueMessageType.HUMAN
    assert message.chat_id == 123456
def test_queue_message_serialization():
    """Test QueueMessage serialization/deserialization"""
    content = QueueMessageContent(message="test message")
    original = QueueMessage(
        type=QueueMessageType.TOOL,
        user_id=1,
        source=QueueMessageSource.USER,
        content=content,
    )
    data = original.to_dict()
    restored = QueueMessage.from_dict(data)
    assert restored == original
</file>

<file path="shared_models/pyproject.toml">
[tool.poetry]
name = "shared-models"
version = "0.1.0"
description = "Shared Pydantic models for Smart Assistant"
authors = ["Your Name <your.email@example.com>"]
packages = [{ include = "shared_models", from = "src" }]

[tool.poetry.dependencies]
python = "^3.11"
pydantic = "^2.4.2"
pydantic-settings = "^2.0.3"

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.3"
pytest-cov = "^4.1.0"
black = "^23.10.1"
isort = "^5.12.0"
mypy = "^1.6.1"

[tool.poetry.group.test.dependencies]
pytest = "^7.4.3"
pytest-cov = "^4.1.0"

[tool.black]
line-length = 88
target-version = ['py311']
include = '\.pyi?$'

[tool.isort]
profile = "black"
multi_line_output = 3
include_trailing_comma = true
force_grid_wrap = 0
use_parentheses = true
ensure_newline_before_comments = true
line_length = 88

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = false
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
</file>

<file path="shared_models/README.md">
# Shared Models

Shared Pydantic models for Smart Assistant project.

## Installation

```bash
poetry add shared-models
```

## Usage

```python
from shared_models import QueueMessage, QueueMessageContent, QueueMessageSource, QueueMessageType

# Create a message
content = QueueMessageContent(message="Hello", metadata={"key": "value"})
message = QueueMessage(
    type=QueueMessageType.TOOL,
    user_id=1,
    source=QueueMessageSource.USER,
    content=content,
)

# Serialize to dict
data = message.to_dict()

# Deserialize from dict
restored = QueueMessage.from_dict(data)
```

## Development

1. Install dependencies:
   ```bash
   poetry install
   ```

2. Run tests:
   ```bash
   poetry run pytest
   ```

3. Format code:
   ```bash
   poetry run black .
   poetry run isort .
   ```

4. Type checking:
   ```bash
   poetry run mypy .
   ```

## Models

### QueueMessage
Base class for all queue messages.

### QueueMessageContent
Content of queue message with metadata.

### QueueMessageType
Enum for message types:
- TOOL
- HUMAN

### QueueMessageSource
Enum for message sources:
- CRON
- CALENDAR
- USER

### ToolQueueMessage
Message from tool with tool name.

### HumanQueueMessage
Message from user with chat ID.
</file>

<file path="telegram_bot_service/src/client/rest.py">
from typing import Any, Dict, Optional
import aiohttp
import structlog
from config.settings import settings
logger = structlog.get_logger()
class RestClient:
    """Async client for REST API."""
    def __init__(self):
        self.base_url = settings.rest_service_url
        self.api_prefix = "/api"
        self.session: Optional[aiohttp.ClientSession] = None
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    async def _make_request(
        self, method: str, endpoint: str, **kwargs
    ) -> Dict[str, Any]:
        """Make HTTP request to API."""
        if not self.session:
            raise RuntimeError(
                "Session is not initialized. Use 'async with' context manager."
            )
        url = f"{self.base_url}{self.api_prefix}{endpoint}"
        logger.debug("Making request", url=url, method=method)
        try:
            async with self.session.request(method, url, **kwargs) as response:
                response.raise_for_status()
                return await response.json()
        except aiohttp.ClientError as e:
            logger.error("Request error", url=url, method=method, error=str(e))
            raise
    async def get_user(self, telegram_id: int) -> Optional[Dict[str, Any]]:
        """Get user by telegram_id."""
        try:
            response = await self._make_request(
                "GET", "/users/", params={"telegram_id": telegram_id}
            )
            if not response:
                return None
            if isinstance(response, list):
                return response[0] if response else None
            return response
        except aiohttp.ClientError as e:
            logger.error("Error getting user", telegram_id=telegram_id, error=str(e))
            return None
    async def create_user(
        self, telegram_id: int, username: Optional[str] = None
    ) -> Dict[str, Any]:
        """Create new user."""
        return await self._make_request(
            "POST", "/users/", json={"telegram_id": telegram_id, "username": username}
        )
    async def get_or_create_user(
        self, telegram_id: int, username: Optional[str] = None
    ) -> Dict[str, Any]:
        """Get or create user."""
        try:
            # Try to find user
            user = await self.get_user(telegram_id)
            if user:
                logger.info("Found existing user", telegram_id=telegram_id)
                return user
            # Create new user if not found
            logger.info("Creating new user", telegram_id=telegram_id)
            return await self.create_user(telegram_id, username)
        except aiohttp.ClientError as e:
            logger.error(
                "Error in get_or_create_user", telegram_id=telegram_id, error=str(e)
            )
            # Return basic user info in case of error
            return {"id": telegram_id, "telegram_id": telegram_id, "username": username}
    async def get_user_by_id(self, user_id: int) -> Optional[Dict[str, Any]]:
        """Get user by user_id."""
        try:
            return await self._make_request("GET", f"/users/{user_id}")
        except aiohttp.ClientError as e:
            logger.error("Error getting user by id", user_id=user_id, error=str(e))
            return None
</file>

<file path="telegram_bot_service/src/client/telegram.py">
from typing import Any, Dict, List, Optional
import aiohttp
import structlog
from config.settings import settings
logger = structlog.get_logger()
class TelegramClient:
    """Async client for Telegram Bot API."""
    def __init__(self):
        self.base_url = f"https://api.telegram.org/bot{settings.telegram_token}"
        self.session: Optional[aiohttp.ClientSession] = None
        logger.info("TelegramClient initialized", base_url=self.base_url)
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    async def _make_request(self, method: str, **kwargs) -> Dict[str, Any]:
        """Make request to Telegram API."""
        if not self.session:
            raise RuntimeError(
                "Session is not initialized. Use 'async with' context manager."
            )
        url = f"{self.base_url}/{method}"
        logger.debug("Making request to Telegram", url=url, method=method, **kwargs)
        try:
            async with self.session.post(url, **kwargs) as response:
                response.raise_for_status()
                result = await response.json()
                logger.debug("Got response from Telegram", result=result)
                if not result.get("ok"):
                    error = result.get("description", "Unknown error")
                    logger.error("Telegram API error", error=error, method=method)
                    raise ValueError(f"Telegram API error: {error}")
                return result.get("result", {})
        except Exception as e:
            logger.error(
                "Telegram request error", method=method, error=str(e), exc_info=True
            )
            raise
    async def send_message(self, chat_id: int, text: str) -> Dict[str, Any]:
        """Send message to chat."""
        logger.info("Sending message", chat_id=chat_id, text=text)
        return await self._make_request(
            "sendMessage", json={"chat_id": chat_id, "text": text, "parse_mode": "HTML"}
        )
    async def get_updates(
        self, offset: int = 0, timeout: int = 30
    ) -> List[Dict[str, Any]]:
        """Get updates from Telegram."""
        logger.debug("Getting updates", offset=offset)
        result = await self._make_request(
            "getUpdates",
            json={"offset": offset, "timeout": timeout, "allowed_updates": ["message"]},
        )
        return result if isinstance(result, list) else []
</file>

<file path="telegram_bot_service/src/config/settings.py">
import os
from typing import Dict
import structlog
from pydantic_settings import BaseSettings, SettingsConfigDict
logger = structlog.get_logger()
class Settings(BaseSettings):
    # Telegram settings
    telegram_token: str
    telegram_rate_limit: int = 30  # requests per second
    # Redis settings
    redis_host: str = "redis"
    redis_port: int = 6379
    redis_db: int = 0
    input_queue: str = os.getenv("REDIS_QUEUE_TO_SECRETARY", "queue:to_secretary")
    assistant_output_queue: str = os.getenv(
        "REDIS_QUEUE_TO_TELEGRAM", "queue:to_telegram"
    )
    user_messages_prefix: str = "user_messages:"
    # REST service settings
    rest_service_url: str = "http://rest_service:8000"
    # Application settings
    update_interval: float = 1.0  # seconds
    batch_size: int = 100  # number of updates to process at once
    # Redis connection settings
    redis_settings: Dict = {
        "encoding": "utf-8",
        "decode_responses": True,
        "retry_on_timeout": True,
        "socket_keepalive": True,
    }
    @property
    def redis_url(self) -> str:
        """Get Redis connection URL."""
        return f"redis://{self.redis_host}:{self.redis_port}/{self.redis_db}"
    def get_user_queue(self, user_id: str) -> str:
        """Get queue name for specific user."""
        return f"{self.user_messages_prefix}{user_id}"
    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")
try:
    settings = Settings()
    logger.info(
        "Settings loaded",
        redis_host=settings.redis_host,
        redis_port=settings.redis_port,
        rest_service_url=settings.rest_service_url,
        redis_url=settings.redis_url,
        input_queue=settings.input_queue,
        assistant_output_queue=settings.assistant_output_queue,
    )
except Exception as e:
    logger.error("Error loading settings", error=str(e))
    raise
</file>

<file path="telegram_bot_service/src/handlers/start.py">
from typing import Any, Dict
import structlog
from client.rest import RestClient
from client.telegram import TelegramClient
logger = structlog.get_logger()
async def handle_start(
    telegram: TelegramClient, rest: RestClient, chat_id: int, user: Dict[str, Any]
) -> None:
    """Handle /start command."""
    telegram_id = user.get("id")
    username = user.get("username")
    try:
        # Get or create user in REST service
        user_data = await rest.get_or_create_user(telegram_id, username)
        is_new_user = user_data.get("id") == telegram_id
        # Send appropriate greeting
        if is_new_user:
            message = (
                "üëã –ü—Ä–∏–≤–µ—Ç! –Ø –≤–∞—à –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\n\n"
                "–Ø –º–æ–≥—É –ø–æ–º–æ—á—å –≤–∞–º —Å:\n"
                "üìÖ –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ–º –≤—Å—Ç—Ä–µ—á–∞–º–∏ –∏ —Å–æ–±—ã—Ç–∏—è–º–∏\n"
                "üìù –°–æ–∑–¥–∞–Ω–∏–µ–º –∑–∞–º–µ—Ç–æ–∫\n"
                "üîç –ü–æ–∏—Å–∫–æ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏\n\n"
                "–ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"
            )
        else:
            message = "üëã –° –≤–æ–∑–≤—Ä–∞—â–µ–Ω–∏–µ–º!\n\n" "–ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å —Å–µ–≥–æ–¥–Ω—è?"
        await telegram.send_message(chat_id, message)
        logger.info(
            "Start command handled", telegram_id=telegram_id, is_new_user=is_new_user
        )
    except Exception as e:
        error_message = (
            "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∫–æ–º–∞–Ω–¥—ã. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."
        )
        await telegram.send_message(chat_id, error_message)
        logger.error(
            "Error handling start command", telegram_id=telegram_id, error=str(e)
        )
</file>

<file path="telegram_bot_service/src/services/response_handler.py">
import asyncio
import json
import structlog
from client.rest import RestClient
from client.telegram import TelegramClient
from config.settings import settings
from redis import asyncio as aioredis
logger = structlog.get_logger()
async def handle_assistant_responses(
    telegram: TelegramClient, redis: aioredis.Redis
) -> None:
    """
    Handle responses from assistant.
    Args:
        telegram: Telegram client instance
        redis: Redis client instance
    """
    logger.info("Started assistant response handler")
    async with RestClient() as rest:
        while True:
            try:
                # Get response from assistant queue
                response = await redis.brpop(settings.assistant_output_queue, timeout=1)
                if response:
                    _, data = response
                    response_data = json.loads(data)
                    # Get user data from REST service
                    user_id = response_data.get("user_id")
                    if not user_id:
                        logger.error("No user_id in response data")
                        continue
                    user = await rest.get_user_by_id(int(user_id))
                    if not user:
                        logger.error("User not found", user_id=user_id)
                        continue
                    chat_id = user.get(
                        "telegram_id"
                    )  # telegram_id is the same as chat_id
                    if not chat_id:
                        logger.error("No telegram_id in user data", user_id=user_id)
                        continue
                    # Check response status
                    if response_data.get("status") == "error":
                        error_message = response_data.get(
                            "error", "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞"
                        )
                        logger.error(
                            "Error in assistant response",
                            error=error_message,
                            user_id=user_id,
                        )
                        # Send error message to user
                        await telegram.send_message(
                            chat_id=chat_id,
                            text=f"–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {error_message}",
                        )
                    else:
                        # Send successful response to user
                        response_text = response_data.get("response")
                        if response_text:
                            await telegram.send_message(
                                chat_id=chat_id, text=response_text
                            )
                            logger.info(
                                "Sent response to user",
                                user_id=user_id,
                                message_preview=response_text[:100],
                            )
                        else:
                            logger.warning(
                                "Empty response from assistant", user_id=user_id
                            )
            except json.JSONDecodeError as e:
                logger.error("Invalid JSON in response", error=str(e))
            except KeyError as e:
                logger.error("Missing required field in response", error=str(e))
            except Exception as e:
                logger.error("Error handling assistant response", error=str(e))
            # Small delay to prevent tight loop
            await asyncio.sleep(0.1)
</file>

<file path="telegram_bot_service/src/main.py">
import asyncio
import datetime
import json
import structlog
from client.rest import RestClient
from client.telegram import TelegramClient
from config.settings import settings
from handlers.start import handle_start
from redis import asyncio as aioredis
from services.response_handler import handle_assistant_responses
logger = structlog.get_logger()
async def process_message(
    telegram: TelegramClient, rest: RestClient, message_data: dict
) -> None:
    """Process single message."""
    try:
        message = message_data.get("text")
        username = message_data.get("username")
        telegram_id = message_data.get("user_id")
        logger.info("Processing message", message=message, telegram_id=telegram_id)
        # Get user data from REST service
        user = await rest.get_or_create_user(int(telegram_id), username)
        if not user:
            logger.error("Failed to get or create user", telegram_id=telegram_id)
            return
        if message == "/start":
            await handle_start(telegram, rest, message_data.get("chat_id"), user)
        else:
            # Send message to assistant queue
            redis = aioredis.from_url(settings.redis_url, **settings.redis_settings)
            await redis.lpush(
                settings.input_queue,
                json.dumps(
                    {
                        "type": "human_message",
                        "user_id": user["id"],
                        "source": "user",
                        "content": {
                            "message": message,
                            "metadata": {
                                "username": username,
                                "telegram_id": telegram_id,
                            },
                        },
                        "timestamp": datetime.datetime.utcnow().isoformat(),
                    }
                ),
            )
            logger.info("Message sent to assistant queue", message=message)
    except Exception as e:
        logger.error("Error processing message", error=str(e), exc_info=True)
async def handle_telegram_update(
    telegram: TelegramClient, rest: RestClient, update: dict
) -> None:
    """Handle single Telegram update."""
    try:
        message = update.get("message")
        if not message:
            return
        chat = message.get("chat", {})
        from_user = message.get("from", {})
        message_data = {
            "text": message.get("text"),
            "chat_id": chat.get("id"),  # –ù—É–∂–µ–Ω —Ç–æ–ª—å–∫–æ –¥–ª—è handle_start
            "user_id": str(from_user.get("id")),
            "username": from_user.get("username"),
        }
        await process_message(telegram, rest, message_data)
    except Exception as e:
        logger.error("Error handling update", error=str(e), exc_info=True)
async def main():
    """Main function."""
    logger.info("Starting bot")
    # Initialize Redis
    redis = aioredis.from_url(settings.redis_url, **settings.redis_settings)
    logger.info("Connected to Redis", url=settings.redis_url)
    # Initialize clients
    async with TelegramClient() as telegram, RestClient() as rest:
        logger.info("Initialized clients")
        # Start response handler
        response_handler = asyncio.create_task(
            handle_assistant_responses(telegram, redis)
        )
        logger.info("Started response handler task")
        try:
            # Store last update id
            last_update_id = 0
            while True:
                try:
                    # Get updates from Telegram
                    updates = await telegram.get_updates(offset=last_update_id + 1)
                    for update in updates:
                        # Process update
                        await handle_telegram_update(telegram, rest, update)
                        # Update last_update_id
                        last_update_id = max(last_update_id, update.get("update_id", 0))
                    # Small delay to prevent tight loop
                    await asyncio.sleep(settings.update_interval)
                except Exception as e:
                    logger.error("Error in main loop", error=str(e), exc_info=True)
                    await asyncio.sleep(settings.update_interval)
        except asyncio.CancelledError:
            logger.info("Bot shutdown requested")
        except Exception as e:
            logger.error("Critical error in main loop", error=str(e), exc_info=True)
        finally:
            # Cancel response handler
            response_handler.cancel()
            try:
                await response_handler
            except asyncio.CancelledError:
                pass
            logger.info("Response handler stopped")
if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="telegram_bot_service/tests/test_smoke.py">
import pytest
from client.rest import RestClient
from client.telegram import TelegramClient
from config.settings import settings
from redis import asyncio as aioredis
@pytest.mark.asyncio
async def test_smoke():
    """Smoke test to verify basic service functionality."""
    # Test settings loading
    assert settings.telegram_token, "Telegram token should be set"
    assert settings.redis_host, "Redis host should be set"
    assert settings.rest_service_url, "REST service URL should be set"
    # Test Redis connection
    redis = aioredis.from_url(settings.redis_url, **settings.redis_settings)
    try:
        await redis.ping()
    finally:
        await redis.close()
    # Test clients initialization
    async with TelegramClient() as telegram, RestClient() as rest:
        assert telegram.base_url, "Telegram client should be initialized"
        assert rest.base_url, "REST client should be initialized"
</file>

<file path="telegram_bot_service/docker-compose.test.yml">
services:
  test:
    build:
      context: ..
      dockerfile: telegram_bot_service/Dockerfile.test
    environment:
      - TESTING=1
      - PYTHONPATH=/src
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN:-test_token}
      - REST_SERVICE_URL=${REST_SERVICE_URL:-http://mock:8000}
      - REDIS_HOST=redis
      - REDIS_PORT=6379
    env_file:
      - ./tests/.env.test
    depends_on:
      redis:
        condition: service_healthy
  redis:
    image: redis:7.2-alpine
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3
</file>

<file path="telegram_bot_service/Dockerfile">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY telegram_bot_service/pyproject.toml telegram_bot_service/poetry.lock* ./telegram_bot_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd telegram_bot_service && \
    poetry install --only main --no-interaction --no-ansi --no-root

# Production stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code
COPY telegram_bot_service/src/ /src/

CMD ["python", "src/main.py"]
</file>

<file path="telegram_bot_service/Dockerfile.test">
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY telegram_bot_service/pyproject.toml telegram_bot_service/poetry.lock* ./telegram_bot_service/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd telegram_bot_service && \
    poetry install --only main,test --no-interaction --no-ansi --no-root

# Test stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    TESTING=1

# Install system dependencies
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code and tests
COPY telegram_bot_service/src/ /src/
COPY telegram_bot_service/tests/ /tests/

CMD ["pytest", "-v", "--cov=src", "--cov-report=term-missing", "/tests/"]
</file>

<file path="telegram_bot_service/llm_context_tg_bot.md">
# Telegram Bot Service Detailed Overview

## 1. Overview
- **Purpose:** Provides an interactive interface for end users via Telegram
- **Functions:**
  - Receives and processes user messages
  - Identifies users through integration with the REST API
  - Sends formatted responses and notifications
- **Tech Stack:** Python (aiohttp, asyncio), Telegram Bot API, Dockerized microservice architecture

## 2. Directory Structure
```
telegram_bot_service/
‚îú‚îÄ‚îÄ src/                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ client/            # External service clients
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ telegram.py    # Telegram Bot API client
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rest.py        # REST API client
‚îÇ   ‚îú‚îÄ‚îÄ handlers/          # Message and command handlers
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ start.py       # /start command handler
‚îÇ   ‚îú‚îÄ‚îÄ services/          # Service logic
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ response_handler.py  # Assistant response handling
‚îÇ   ‚îú‚îÄ‚îÄ config/            # Configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py    # Service settings
‚îÇ   ‚îú‚îÄ‚îÄ utils/             # Utilities
‚îÇ   ‚îî‚îÄ‚îÄ main.py            # Service entry point
‚îú‚îÄ‚îÄ tests/                 # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ test_smoke.py     # Basic functionality tests
‚îÇ   ‚îî‚îÄ‚îÄ .env.test         # Test environment configuration
‚îî‚îÄ‚îÄ Dockerfile            # Container configuration
```

## 3. Key Components

### 3.1 Telegram Client
```python
class TelegramClient:
    def __init__(self):
        self.base_url = f"https://api.telegram.org/bot{settings.telegram_token}"
        self.session: Optional[aiohttp.ClientSession] = None
```

- Handles communication with Telegram Bot API
- Implements message sending and update retrieval
- Uses aiohttp for async HTTP requests
- Includes error handling and logging

### 3.2 REST Client
```python
class RestClient:
    def __init__(self):
        self.base_url = settings.rest_service_url
        self.api_prefix = "/api"
```

- Manages communication with REST API service
- Handles user management operations
- Implements get_or_create_user functionality
- Includes error handling and logging

### 3.3 Response Handler
```python
async def handle_assistant_responses(telegram: TelegramClient, redis: aioredis.Redis):
    """Handle responses from assistant service."""
```

- Processes responses from assistant service
- Manages Redis queue for responses
- Handles error cases and user notifications
- Implements retry mechanisms

### 3.4 Command Handlers
```python
async def handle_start(telegram: TelegramClient, rest: RestClient, chat_id: int, user: Dict[str, Any]):
    """Handle /start command."""
```

- Processes /start command
- Manages user registration
- Sends appropriate welcome messages
- Handles error cases

## 4. Configuration

### 4.1 Settings
```python
class Settings(BaseSettings):
    # Telegram settings
    telegram_token: str
    telegram_rate_limit: int = 30

    # Redis settings
    redis_host: str = "redis"
    redis_port: int = 6379
    redis_db: int = 0
    input_queue: str = "queue:to_secretary"
    assistant_output_queue: str = "queue:to_telegram"
```

### 4.2 Environment Variables
- `TELEGRAM_TOKEN`: Bot token
- `REDIS_*`: Redis connection settings
- `REST_SERVICE_URL`: REST API endpoint
- `TESTING`: Test mode flag

## 5. Message Processing

### 5.1 Message Flow
1. Receive message from Telegram
2. Identify user via REST API
3. Process command or forward to assistant
4. Handle assistant response
5. Send response back to user

### 5.2 Error Handling
- Structured error logging with structlog
- User-friendly error messages
- Retry mechanisms for external services
- Graceful degradation

## 6. Testing

### 6.1 Current Implementation
- Smoke test for basic functionality
- Environment configuration for testing
- Basic service connectivity checks

### 6.2 Test Coverage
- Settings validation
- Redis connection
- Client initialization
- Basic service functionality

## 7. Future Enhancements

### 7.1 Planned Improvements
- Enhanced message formatting
- Inline keyboard support
- Rich media handling
- Advanced monitoring

### 7.2 Testing Roadmap
- Unit tests for components
- Integration tests
- E2E test scenarios
- Mock implementations

## 8. Best Practices

### 8.1 Development Guidelines
- Use type hints
- Follow naming conventions
- Document changes
- Maintain test coverage

### 8.2 Error Handling
- Log all errors with context
- Provide user-friendly messages
- Implement retry mechanisms
- Monitor error rates

### 8.3 Performance
- Use async/await for I/O operations
- Implement rate limiting
- Monitor response times
- Optimize Redis usage
</file>

<file path="telegram_bot_service/pyproject.toml">
[tool.poetry]
name = "telegram_bot_service"
version = "0.1.0"
description = "Telegram Bot Service for Smart Assistant Project"
authors = ["Your Name <your.email@example.com>"]
packages = [{include = "src"}]

[tool.poetry.dependencies]
python = "^3.11"
aiohttp = "^3.9.3"
redis = "^5.0.1"
pydantic = "^2.6.1"
pydantic-settings = "^2.1.0"
structlog = "^24.1.0"
shared_models = {path = "../shared_models"}

[tool.poetry.group.test.dependencies]
shared_models = {path = "../shared_models"}
pytest = "^8.0.0"
pytest-asyncio = "^0.23.5"
pytest-cov = "^4.1.0"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --cov=src --cov-report=term-missing"
asyncio_mode = "auto"

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_optional = true
mypy_path = "src"
</file>

<file path=".gitignore">
# Virtual environment
.venv/
.env

# Python
*.pyc
*.pyo
*.pyd
__pycache__/
*.sqlite3
.coverage
*/.coverage

# Docker
*.log
*.pid
*.seed
*.key
*.crt
.dockerignore

# IDE and editor files
.idea/
.vscode/
*.swp
*.swo

# OS-specific files
.DS_Store
Thumbs.db

# Git
*.orig
*.rej

# Database
db.sqlite3
db/*.db
db/*.sql
db/*.sqlite3
db/migrations/

# Node (if used for front-end parts)
node_modules/
npm-debug.log
yarn-debug.log
yarn-error.log

# Build
build/
dist/
*.egg-info/
</file>

<file path="docker_templates.md">
# Docker Templates and Checklist

## –®–∞–±–ª–æ–Ω—ã —Ñ–∞–π–ª–æ–≤

### 1. Dockerfile (–ø—Ä–æ–¥–∞–∫—à–Ω)
```dockerfile
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY service_name/pyproject.toml service_name/poetry.lock* ./service_name/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd service_name && \
    poetry install --only main --no-interaction --no-ansi --no-root

# Production stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1

# Install system dependencies (if needed)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code
COPY service_name/src/ /src/

# –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ (–µ—Å–ª–∏ –µ—Å—Ç—å)
# COPY service_name/alembic/ ./alembic/  # –µ—Å–ª–∏ –µ—Å—Ç—å –º–∏–≥—Ä–∞—Ü–∏–∏
# COPY service_name/manage.py ./manage.py  # –µ—Å–ª–∏ –µ—Å—Ç—å manage.py
# COPY service_name/alembic.ini ./alembic.ini  # –µ—Å–ª–∏ –µ—Å—Ç—å alembic.ini

CMD ["python", "src/main.py"]
```

### 2. Dockerfile.test
```dockerfile
# Build stage
FROM python:3.11-slim as builder

WORKDIR /

# Install poetry
RUN pip install poetry

# Copy dependency files
COPY service_name/pyproject.toml service_name/poetry.lock* ./service_name/
COPY shared_models ./shared_models

# Configure poetry and install dependencies
RUN poetry config virtualenvs.create false && \
    cd service_name && \
    poetry install --only main,test --no-interaction --no-ansi --no-root

# Test stage
FROM python:3.11-slim

WORKDIR /

ENV PYTHONPATH=/src \
    PYTHONUNBUFFERED=1 \
    PYTHONDONTWRITEBYTECODE=1 \
    TESTING=1

# Install system dependencies (if needed)
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy installed packages from builder
COPY --from=builder /usr/local/lib/python3.11/site-packages/ /usr/local/lib/python3.11/site-packages/
COPY --from=builder /usr/local/bin/ /usr/local/bin/

# Copy application code and tests
COPY service_name/src/ /src/
COPY service_name/tests/ /tests/

CMD ["pytest", "-v", "--cov=src", "--cov-report=term-missing", "/tests/"]
```

### 3. docker-compose.yml (—Å–µ—Ä–≤–∏—Å –≤ –æ–±—â–µ–º –∫–æ–º–ø–æ—É–∑–µ)
```yaml
service_name:
  build:
    context: .
    dockerfile: service_name/Dockerfile
  container_name: service-name
  environment:
    - SERVICE_VAR1=${SERVICE_VAR1}
    - SERVICE_VAR2=${SERVICE_VAR2}
  env_file:
    - .env
  ports:
    - "8000:8000"  # –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
  volumes:
    - ./service_name/src:/src  # –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
  depends_on:
    - redis:
      condition: service_healthy
    - db:
      condition: service_healthy
  networks:
    - app_network
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 10s
    retries: 3
```

### 4. docker-compose.test.yml
```yaml
services:
  test:
    build:
      context: ..
      dockerfile: service_name/Dockerfile.test
    environment:
      - TESTING=1
      - PYTHONPATH=/src
      - PYTHONUNBUFFERED=1
      - PYTHONDONTWRITEBYTECODE=1
      # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
      - SERVICE_VAR1=test_value1
      - SERVICE_VAR2=test_value2
    env_file:
      - ./tests/.env.test
    depends_on:
      - test-db:
        condition: service_healthy
      - redis:
        condition: service_healthy

  test-db:
    image: postgres:16
    environment:
      - POSTGRES_USER=test_user
      - POSTGRES_PASSWORD=test_password
      - POSTGRES_DB=test_db
    volumes:
      - test_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U test_user -d test_db"]
      interval: 5s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7.2-alpine
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 3

volumes:
  test_db_data:
```

## –ß–µ–∫-–ª–∏—Å—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–µ—Ä–≤–∏—Å–æ–≤

### assistant_service
- [x] Dockerfile —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] Dockerfile.test —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] –ë–ª–æ–∫ –≤ –æ–±—â–µ–º docker-compose.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] docker-compose.test.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É

### rest_service
- [x] Dockerfile —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] Dockerfile.test —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] –ë–ª–æ–∫ –≤ –æ–±—â–µ–º docker-compose.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] docker-compose.test.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É

### google_calendar_service
- [x] Dockerfile —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] Dockerfile.test —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] –ë–ª–æ–∫ –≤ –æ–±—â–µ–º docker-compose.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] docker-compose.test.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É

### cron_service
- [x] Dockerfile —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] Dockerfile.test —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] –ë–ª–æ–∫ –≤ –æ–±—â–µ–º docker-compose.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] docker-compose.test.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É

### telegram_bot_service
- [x] Dockerfile —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] Dockerfile.test —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] –ë–ª–æ–∫ –≤ –æ–±—â–µ–º docker-compose.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- [x] docker-compose.test.yml —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É

## –ü—Ä–∏–º–µ—á–∞–Ω–∏—è
1. –ü—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –∫–∞–∂–¥–æ–≥–æ —Ñ–∞–π–ª–∞ –Ω—É–∂–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å —Å–ø–µ—Ü–∏—Ñ–∏–∫—É —Å–µ—Ä–≤–∏—Å–∞
2. –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Å–µ—Ä–≤–∏—Å—ã –º–æ–≥—É—Ç –Ω–µ —Ç—Ä–µ–±–æ–≤–∞—Ç—å –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –Ω–µ –≤—Å–µ –Ω—É–∂–¥–∞—é—Ç—Å—è –≤ –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö)
3. –ü–æ—Ä—Ç—ã –∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å–µ—Ä–≤–∏—Å
4. Healthcheck –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å API —Å–µ—Ä–≤–∏—Å–∞
5. –ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ Poetry –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É–±–µ–¥–∏—Ç—å—Å—è, —á—Ç–æ poetry.lock –∞–∫—Ç—É–∞–ª–µ–Ω
6. –î–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π (–Ω–∞–ø—Ä–∏–º–µ—Ä, shared_models) –Ω–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ñ–ª–∞–≥ develop=true –≤ pyproject.toml
</file>

<file path="docker-compose.yml">
services:
  assistant_service:
    build:
      context: .
      dockerfile: assistant_service/Dockerfile
    container_name: assistant-service
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPEN_API_SECRETAR_ID=${OPEN_API_SECRETAR_ID}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - REDIS_DB=${REDIS_DB}
      - REDIS_QUEUE_TO_TELEGRAM=${REDIS_QUEUE_TO_TELEGRAM}
      - REDIS_QUEUE_TO_SECRETARY=${REDIS_QUEUE_TO_SECRETARY}
      - TAVILY_API_KEY=${TAVILY_API_KEY}
    env_file:
      - .env
    volumes:
      - ./assistant_service/src:/src
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - app_network
  telegram_bot_service:
    build:
      context: .
      dockerfile: telegram_bot_service/Dockerfile
    container_name: telegram-bot-service
    environment:
      - ASYNC_DATABASE_URL=postgresql+asyncpg://user:password@db/rest_db
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET}
      - GOOGLE_REDIRECT_URI=${GOOGLE_REDIRECT_URI}
      - OPEN_API_SECRETAR_ID=${OPEN_API_SECRETAR_ID}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_USER=${POSTGRES_USER}
      - REDIS_DB=${REDIS_DB}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - REDIS_QUEUE_TO_SECRETARY=${REDIS_QUEUE_TO_SECRETARY}
      - REDIS_QUEUE_TO_TELEGRAM=${REDIS_QUEUE_TO_TELEGRAM}
      - REST_SERVICE_URL=http://rest_service:8000
      - TELEGRAM_BOT_USERNAME=${TELEGRAM_BOT_USERNAME}
      - TELEGRAM_ID=${TELEGRAM_ID}
      - TELEGRAM_TOKEN=${TELEGRAM_TOKEN}
    volumes:
      - ./telegram_bot_service/src:/src  # –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    depends_on:
      redis:
        condition: service_healthy
      rest_service:
        condition: service_healthy
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "pidof", "python"]
      interval: 10s
      timeout: 10s
      retries: 3
  db:
      image: postgres:15
      container_name: postgres_db
      environment:
        POSTGRES_USER: ${POSTGRES_USER}
        POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
        POSTGRES_DB: ${POSTGRES_DB}
      volumes:
        - postgres_data:/var/lib/postgresql/data
      ports:
        - "5432:5432"
      networks:
        - app_network
      healthcheck:
        test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
        interval: 3s
        timeout: 5s
        retries: 6
  rest_service:
      build:
        context: .
        dockerfile: rest_service/Dockerfile
      container_name: rest-service
      environment:
        - ASYNC_DATABASE_URL=${ASYNC_DATABASE_URL}
      env_file:
        - .env
      ports:
        - "8000:8000"
      volumes:
        - ./rest_service/src:/src
      depends_on:
        db:
          condition: service_healthy
      networks:
        - app_network
      healthcheck:
        test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
        interval: 5s
        timeout: 5s
        retries: 8
  cron_service:
    build:
      context: .
      dockerfile: cron_service/Dockerfile
    container_name: cron-service
    environment:
      - TELEGRAM_ID=${TELEGRAM_ID}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - REDIS_DB=${REDIS_DB}
      - REDIS_QUEUE_TO_TELEGRAM=${REDIS_QUEUE_TO_TELEGRAM}
      - REDIS_QUEUE_TO_SECRETARY=${REDIS_QUEUE_TO_SECRETARY}
    volumes:
      - ./cron_service/src:/src  # –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
    depends_on:
      redis:
        condition: service_healthy
      rest_service:
        condition: service_healthy
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "ps", "aux", "|", "grep", "python"]
      interval: 30s
      timeout: 10s
      retries: 3
  google_calendar_service:
    build:
      context: .
      dockerfile: google_calendar_service/Dockerfile
    container_name: google-calendar-service
    ports:
      - "8001:8000"
    environment:
      - GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID}
      - GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET}
      - GOOGLE_REDIRECT_URI=${GOOGLE_REDIRECT_URI}
      - REST_SERVICE_URL=http://rest_service:8000
      - TELEGRAM_BOT_USERNAME=${TELEGRAM_BOT_USERNAME}
      - REDIS_HOST=${REDIS_HOST}
      - REDIS_PORT=${REDIS_PORT}
      - REDIS_DB=${REDIS_DB}
      - REDIS_QUEUE_TO_TELEGRAM=${REDIS_QUEUE_TO_TELEGRAM}
      - REDIS_QUEUE_TO_SECRETARY=${REDIS_QUEUE_TO_SECRETARY}
    env_file:
      - .env
    volumes:
      - ./google_calendar_service/src:/src
      - ./shared_models:/shared_models
    depends_on:
      rest_service:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
  admin_service:
    build:
      context: .
      dockerfile: admin_service/Dockerfile
    container_name: admin-service
    environment:
      - REST_SERVICE_URL=http://rest_service:8000
    env_file:
      - .env
    ports:
      - "8501:8501"
    volumes:
      - ./admin_service/src:/src
    depends_on:
      rest_service:
        condition: service_healthy
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8501"]
      interval: 30s
      timeout: 10s
      retries: 3
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 2s
      timeout: 3s
      retries: 4
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    restart: always
    ports:
      - "6333:6333" # REST API
      - "6334:6334" # gRPC API
    volumes:
      - ./qdrant_data:/qdrant/storage # Persistent storage for Qdrant data
    environment:
      QDRANT__SERVICE__GRPC_PORT: 6334 # Optional: Specify gRPC port
    networks:
      - app_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 10s
      timeout: 5s
      retries: 3
networks:
  app_network:
    driver: bridge
volumes:
  postgres_data:
  redis_data:
</file>

<file path="llm_context.md">
# High-Level Overview

## Project Structure

The project is organized as a monorepo with the following structure:

```
smart-assistant/
‚îú‚îÄ‚îÄ assistant_service/          # Core assistant service
‚îú‚îÄ‚îÄ rest_service/              # REST API service
‚îú‚îÄ‚îÄ google_calendar_service/   # Google Calendar integration
‚îú‚îÄ‚îÄ cron_service/             # Task scheduler service
‚îú‚îÄ‚îÄ telegram_bot_service/     # Telegram bot interface
‚îú‚îÄ‚îÄ scripts/                  # Utility scripts
‚îú‚îÄ‚îÄ manage.py                # Project management script
‚îú‚îÄ‚îÄ run_tests.sh            # Test execution script
‚îú‚îÄ‚îÄ run_formatters.sh       # Code formatting script
‚îú‚îÄ‚îÄ docker-compose.yml      # Main Docker configuration
‚îú‚îÄ‚îÄ pyproject.toml          # Poetry configuration
‚îî‚îÄ‚îÄ poetry.lock            # Fixed dependency versions
```

Each service follows a consistent structure:
```
service_name/
‚îú‚îÄ‚îÄ src/                    # Source code
‚îú‚îÄ‚îÄ tests/                  # Test suite
‚îú‚îÄ‚îÄ Dockerfile             # Main Dockerfile
‚îú‚îÄ‚îÄ Dockerfile.test        # Test environment Dockerfile
‚îú‚îÄ‚îÄ docker-compose.test.yml # Test environment configuration
‚îú‚îÄ‚îÄ requirements.txt       # Service dependencies
‚îú‚îÄ‚îÄ llm_context_*.md      # Service documentation
‚îî‚îÄ‚îÄ __init__.py           # Package initialization
```

## Services

The project is divided into several independent microservices:

- **assistant_service**  
  - Core engine for handling user messages and coordinating various LLM-based functionalities.
  - Manages context, threads, and asynchronous message processing via Redis.
  - Supports multiple secretary instances with user-specific configurations.
  - Implements secretary selection and caching through AssistantFactory.
  - Handles user-secretary mapping and context isolation.
  
- **rest_service**  
  - Provides a REST API for managing user data, assistant configurations, and related models.
  - Handles CRUD operations for assistants, tools, and scheduled tasks.
  - Manages user-secretary mapping and secretary configurations.
  - Uses PostgreSQL for data storage and Alembic for database migrations.
  
- **google_calendar_service**  
  - Integrates with Google Calendar for event management.
  - Implements OAuth 2.0 authorization, token management, and calendar event retrieval/creation.
  
- **cron_service**  
  - A scheduler service using APScheduler to manage and execute scheduled tasks.
  - Periodically pulls job configurations from the REST API and sends notifications via Redis.
  
- **telegram_bot_service**  
  - A Telegram Bot interface for end-user interaction.
  - Receives user messages, identifies users via REST API, and sends formatted responses.

## Project Management

The project includes several management scripts:

- **manage.py** - Main management script for:
  - Database migrations
  - Service testing
  - Container management
  - Service lifecycle control

- **run_tests.sh** - Script for running tests in Docker containers
- **run_formatters.sh** - Script for code formatting and linting

Detailed information about each service is available in their respective `llm_context_*.md` files.

## Deployment

### Docker & Docker Compose

The project uses Docker Compose for container orchestration. Each service runs in its own container with the following configuration:

- **Base Services:**
  - `redis`: Redis server for message queues and caching
  - `db`: PostgreSQL database for persistent storage
  - `rest_service`: REST API service (port 8000)
  - `google_calendar_service`: Calendar integration (port 8001)
  - `assistant_service`: Core assistant service
  - `telegram_bot_service`: Telegram bot interface
  - `cron_service`: Task scheduler service

- **Container Management:**
  ```bash
  # Build and start all containers
  docker compose up --build -d

  # Check container status
  docker compose ps

  # View logs for a specific service
  docker compose logs -f assistant_service

  # Restart a specific service
  docker compose restart assistant_service
  ```

- **Health Checks:**
  - Each service implements health checks
  - Services wait for dependencies to be healthy before starting
  - Automatic retries and timeouts are configured

### Environment Configuration

The project uses environment variables for configuration:

- **Core Services:**
  - `POSTGRES_*`: Database configuration
  - `REDIS_*`: Redis connection settings
  - `ASYNC_DATABASE_URL`: Async database connection string

- **API Keys & Secrets:**
  - `OPENAI_API_KEY`: OpenAI API key
  - `OPEN_API_SECRETAR_ID`: OpenAI Assistant ID
  - `TELEGRAM_TOKEN`: Telegram Bot token
  - `GOOGLE_*`: Google Calendar API credentials

- **Service Communication:**
  - `REDIS_QUEUE_TO_TELEGRAM`: Queue for messages to Telegram
  - `REDIS_QUEUE_TO_SECRETARY`: Queue for messages to Assistant
  - `REST_SERVICE_URL`: REST API endpoint

### Development Setup

For development:
- Source code is mounted as volumes for live updates
- Each service has its own test environment (`docker-compose.test.yml`)
- Health checks ensure proper service initialization
- Network isolation using Docker networks

### Testing Environment

- Separate Docker Compose configuration for tests
- Isolated test databases and Redis instances
- Automated test execution in containers
- Health checks for test services

## Testing

### Test Types and Structure

The project implements a comprehensive testing strategy with the following components:

- **Unit Tests:**
  - Validate individual components and business logic
  - Use mocks for external dependencies
  - Focus on isolated functionality testing

- **Integration Tests:**
  - Verify inter-service communication
  - Test API endpoints and database operations
  - Validate asynchronous operations

- **End-to-End Tests:**
  - Test complete user workflows
  - Run in isolated Docker environments
  - Validate service interactions

### Test Execution

Tests are executed using Docker containers for isolation and consistency:

```bash
# Run all tests
./run_tests.sh

# Run tests for specific services
./run_tests.sh rest_service assistant_service

# Available services:
# - rest_service
# - cron_service
# - telegram_bot_service
# - assistant_service
# - google_calendar_service
```

### Test Environment

Each service has its own test environment:

- **Docker Configuration:**
  - `Dockerfile.test` - Test-specific Dockerfile
  - `docker-compose.test.yml` - Test environment configuration
  - Isolated databases and Redis instances

- **Environment Variables:**
  - `TESTING=1` - Test mode flag
  - `PYTHONPATH=/src` - Source code path
  - `ASYNC_DATABASE_URL` - Test database connection
  - Service-specific test configurations

### Test Management

- **Test Results:**
  - Color-coded output for better visibility
  - Detailed error reporting
  - Service-specific test status

- **Test Isolation:**
  - Each service runs in its own container
  - Clean environment for each test run
  - Automatic cleanup after tests

- **Continuous Integration:**
  - Automated test execution
  - Service-specific test suites
  - Integration with CI/CD pipeline

### Best Practices

- **Test Organization:**
  - Clear separation of test types
  - Consistent naming conventions
  - Proper test isolation

- **Test Data:**
  - Use of fixtures and factories
  - Clean test data management
  - Proper cleanup after tests

- **Test Coverage:**
  - Coverage reports for each service
  - Integration with coverage tools
  - Regular coverage monitoring

## Scripts & Tools

### Management Scripts

The project provides several management scripts for development and deployment:

#### manage.py

Main management script with the following commands:

```bash
# Database Management
./manage.py migrate "Migration message"  # Create new migration
./manage.py upgrade                      # Apply pending migrations

# Service Management
./manage.py start [--service SERVICE]    # Start service(s)
./manage.py stop [--service SERVICE]     # Stop service(s)
./manage.py restart [--service SERVICE]  # Restart service(s)
./manage.py rebuild [--service SERVICE]  # Rebuild service(s)

# Testing
./manage.py test [--service SERVICE]     # Run tests

# Code Formatting
./manage.py black [--service SERVICE]    # Run black formatter
./manage.py isort [--service SERVICE]    # Run isort formatter
```

#### run_tests.sh

Test execution script:
```bash
# Run all tests
./run_tests.sh

# Run specific service tests
./run_tests.sh rest_service assistant_service
```

#### run_formatters.sh

Code formatting script:
- Runs black and isort on changed files
- Checks formatting before commit
- Provides instructions for fixing formatting issues

### Development Tools

#### Code Formatters
- **Black**: Python code formatter
- **isort**: Import statement sorter
- **flake8**: Code style checker
- **mypy**: Static type checker

#### Testing Tools
- **pytest**: Test framework
- **pytest-asyncio**: Async test support
- **pytest-cov**: Coverage reporting
- **pytest-mock**: Mocking utilities

#### Database Tools
- **Alembic**: Database migrations
- **SQLAlchemy**: ORM and database toolkit
- **psycopg2**: PostgreSQL adapter

#### Container Management
- **Docker**: Containerization
- **Docker Compose**: Container orchestration
- **Health Checks**: Service monitoring

### Best Practices

- **Code Formatting:**
  - Run formatters before committing
  - Use consistent formatting rules
  - Check formatting in CI/CD

- **Testing:**
  - Write tests for new features
  - Maintain test coverage
  - Run tests in containers

- **Database:**
  - Use migrations for schema changes
  - Test migrations before deployment
  - Maintain migration history

- **Container Management:**
  - Use health checks
  - Monitor container status
  - Follow container best practices

---

This high-level summary encapsulates the primary components, deployment strategy, testing approach, and management scripts/tools of the Smart Assistant project.

# General Recommendations and Future Plans

## Best Practices & Internal Guidelines

- **Code Quality:**
  - Use consistent naming conventions (snake_case for services, kebab-case for containers)
  - Follow standardized service structure
  - Maintain high test coverage
  - Use type hints and static type checking

- **Development Workflow:**
  - Run formatters before committing
  - Use pre-commit hooks for code quality
  - Test changes in Docker containers
  - Document all significant changes

- **Security:**
  - Validate configuration at startup
  - Use environment-specific configuration
  - Implement proper secret management
  - Regular dependency updates

- **Container Management:**
  - Use health checks for all services
  - Monitor container status
  - Follow Docker best practices
  - Maintain clean container images

### Current Status

‚úÖ Completed:
- Standardized naming conventions
- Service structure standardization
- Linter implementation
- Basic dependency management
- Initial documentation updates

üîÑ In Progress:
- Dependency updates
- Configuration system improvements
- Service-specific updates
- Testing infrastructure

‚è≥ Planned:
- Enhanced monitoring and logging
- CI/CD pipeline improvements
- Advanced testing capabilities
- Documentation updates

### Future Enhancements

#### Configuration & Security
- Implement centralized configuration management
- Add configuration validation
- Improve secret management
- Environment-specific configurations

#### Testing & Quality
- Increase test coverage (>80%)
- Add integration tests
- Implement performance testing
- Enhance CI/CD pipeline

#### Monitoring & Observability
- Centralized logging with structlog
- Prometheus metrics integration
- Grafana dashboards
- Request tracing

#### Documentation
- Update service documentation
- Add development guidelines
- Create API documentation
- Maintain change logs

### Roadmap

1. **Short-term (1-2 months):**
   - Complete dependency updates
   - Implement configuration system
   - Update service documentation
   - Add basic monitoring

2. **Medium-term (3-6 months):**
   - Enhance testing infrastructure
   - Improve CI/CD pipeline
   - Add advanced monitoring
   - Update all services

3. **Long-term (6+ months):**
   - Implement advanced features
   - Optimize performance
   - Scale infrastructure
   - Enhance security

### Success Criteria

- All services follow standardized structure
- High test coverage (>80%)
- Secure configuration management
- Efficient monitoring and logging
- Comprehensive documentation
- Successful CI/CD pipeline
- Regular dependency updates
- Clean and maintainable code

detailed information on each service in the "llm_context_**" files
</file>

<file path="manage.py">
#!/usr/bin/env python3
import argparse
import subprocess
import sys
from typing import Optional
def run_command(command: str) -> int:
    """Run a command and return its result."""
    try:
        result = subprocess.run(command, shell=True, check=True)
        return result.returncode
    except subprocess.CalledProcessError as e:
        print(f"Error executing command: {e}")
        return e.returncode
def get_service_container(service_name: str) -> str:
    """Get container ID for a service"""
    result = subprocess.run(
        "docker compose ps -q rest_service", shell=True, capture_output=True, text=True
    )
    if result.returncode != 0:
        raise Exception(
            f"Failed to get container ID for {service_name}: {result.stderr}"
        )
    return result.stdout.strip()
def create_migration(message: str):
    """Create a new migration"""
    # Get the container ID
    container_id = get_service_container("rest_service")
    # Create migration in the container
    result = subprocess.run(
        f'docker exec {container_id} python /app/manage.py migrate "{message}"',
        shell=True,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        print(f"Failed to create migration: {result.stderr}")
        return result.returncode
    # Find the newly created migration file in the container
    result = subprocess.run(
        (
            f"docker exec {container_id} bash -c "
            '"ls -t /app/alembic/versions/*.py | head -1"'
        ),
        shell=True,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        print(f"Failed to find migration file: {result.stderr}")
        return result.returncode
    container_file = result.stdout.strip()
    # Copy the migration file from container to host
    result = subprocess.run(
        f"docker cp {container_id}:{container_file} rest_service/alembic/versions/",
        shell=True,
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        print(f"Failed to copy migration file: {result.stderr}")
        return result.returncode
    return 0
def apply_migrations() -> int:
    """Apply all pending migrations."""
    return run_command("docker compose exec rest_service python manage.py upgrade")
def run_tests(service: Optional[str] = None) -> int:
    """Run tests for all services or a specific service."""
    if service:
        return run_command(f"docker compose exec {service} python -m pytest")
    return run_command("docker compose exec rest_service python -m pytest")
def rebuild_containers(service: Optional[str] = None) -> int:
    """Rebuild all containers or a specific service."""
    if service:
        return run_command(f"docker compose build {service} && docker compose up -d")
    return run_command("docker compose build && docker compose up -d")
def restart_service(service: Optional[str] = None) -> int:
    """Stop a service, rebuild all containers,
    and start the service in detached mode."""
    if service:
        return run_command(
            f"docker compose stop {service} && "
            "docker compose build && docker compose up -d"
        )
    return run_command(
        "docker compose down && " "docker compose build && docker compose up -d"
    )
def start_service(service: Optional[str] = None) -> int:
    """Start a service in detached mode."""
    if service:
        return run_command(f"docker compose up -d {service}")
    return run_command("docker compose up -d")
def stop_service(service: Optional[str] = None) -> int:
    """Stop a service."""
    if service:
        return run_command(f"docker compose stop {service}")
    return run_command("docker compose down")
def run_black(service: Optional[str] = None) -> int:
    """Run black formatter for all services or a specific service."""
    if service:
        return run_command(f"black {service}/src")
    return run_command("black */src")
def run_isort(service: Optional[str] = None) -> int:
    """Run isort formatter for all services or a specific service."""
    if service:
        return run_command(f"isort {service}/src")
    return run_command("isort */src")
def main():
    parser = argparse.ArgumentParser(description="Manage the project")
    subparsers = parser.add_subparsers(dest="command", help="Available commands")
    # Create migration command
    migrate_parser = subparsers.add_parser("migrate", help="Create a new migration")
    migrate_parser.add_argument("message", help="Migration message")
    # Apply migrations command
    subparsers.add_parser("upgrade", help="Apply all pending migrations")
    # Run tests command
    test_parser = subparsers.add_parser("test", help="Run tests")
    test_parser.add_argument("--service", help="Service to test")
    # Rebuild containers command
    rebuild_parser = subparsers.add_parser("rebuild", help="Rebuild containers")
    rebuild_parser.add_argument("--service", help="Service to rebuild")
    # Restart service command
    restart_parser = subparsers.add_parser("restart", help="Restart a service")
    restart_parser.add_argument("--service", help="Service to restart")
    # Start service command
    start_parser = subparsers.add_parser("start", help="Start a service")
    start_parser.add_argument("--service", help="Service to start")
    # Stop service command
    stop_parser = subparsers.add_parser("stop", help="Stop a service")
    stop_parser.add_argument("--service", help="Service to stop")
    # Run black command
    black_parser = subparsers.add_parser("black", help="Run black formatter")
    black_parser.add_argument("--service", help="Service to format")
    # Run isort command
    isort_parser = subparsers.add_parser("isort", help="Run isort formatter")
    isort_parser.add_argument("--service", help="Service to format")
    args = parser.parse_args()
    if args.command == "migrate":
        return create_migration(args.message)
    elif args.command == "upgrade":
        return apply_migrations()
    elif args.command == "test":
        return run_tests(args.service)
    elif args.command == "rebuild":
        return rebuild_containers(args.service)
    elif args.command == "restart":
        return restart_service(args.service)
    elif args.command == "start":
        return start_service(args.service)
    elif args.command == "stop":
        return stop_service(args.service)
    elif args.command == "black":
        return run_black(args.service)
    elif args.command == "isort":
        return run_isort(args.service)
    else:
        parser.print_help()
        return 1
if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="naming_conventions.md">
# Naming Conventions in Smart Assistant Project

## 1. General Principles

### 1.1. Naming Styles
- **snake_case**: for Python files, directories, variables, and functions
- **kebab-case**: for Docker container names
- **PascalCase**: for Python classes
- **UPPER_SNAKE_CASE**: for constants and environment variables

### 1.2. Prefixes
- All services and their components must have the `name_service` format
- All containers must have the `name-service` format
- All Docker images must have the `name_service` format

## 2. Service Naming

### 2.1. Service Directories
```
assistant_service/      # Assistant service
rest_service/          # REST API service
calendar_service/      # Calendar service
cron_service/          # Scheduler service
bot_service/           # Telegram bot
```

### 2.2. Containers in docker-compose.yml
```yaml
services:
  assistant_service:
    container_name: assistant-service
  rest_service:
    container_name: rest-service
  calendar_service:
    container_name: calendar-service
  cron_service:
    container_name: cron-service
  bot_service:
    container_name: bot-service
```

### 2.3. Docker Images
```yaml
services:
  assistant_service:
    image: assistant_service:latest
  rest_service:
    image: rest_service:latest
  calendar_service:
    image: calendar_service:latest
  cron_service:
    image: cron_service:latest
  bot_service:
    image: bot_service:latest
```

## 3. Code Naming

### 3.1. Python Files and Directories
- All files and directories in snake_case
- Test files must end with `_test.py`
- Configuration files must end with `_config.py`
- Examples:
  ```
  src/
  ‚îú‚îÄ‚îÄ api/
  ‚îÇ   ‚îú‚îÄ‚îÄ endpoints/
  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_endpoints.py
  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user_endpoints_test.py
  ‚îÇ   ‚îî‚îÄ‚îÄ router.py
  ‚îú‚îÄ‚îÄ core/
  ‚îÇ   ‚îú‚îÄ‚îÄ config.py
  ‚îÇ   ‚îî‚îÄ‚îÄ config_test.py
  ‚îî‚îÄ‚îÄ utils/
      ‚îú‚îÄ‚îÄ helpers.py
      ‚îî‚îÄ‚îÄ helpers_test.py
  ```

### 3.2. Classes
- Use PascalCase
- Names should be nouns
- Examples:
  ```python
  class UserService:
  class DatabaseConfig:
  class TelegramBot:
  ```

### 3.3. Functions and Variables
- Use snake_case
- Function names should be verbs
- Variable names should be nouns
- Examples:
  ```python
  def get_user_by_id():
  def process_message():
  user_count = 0
  message_text = ""
  ```

### 3.4. Constants
- Use UPPER_SNAKE_CASE
- Examples:
  ```python
  MAX_RETRY_COUNT = 3
  DEFAULT_TIMEOUT = 30
  API_VERSION = "v1"
  ```

## 4. Database Naming

### 4.1. Tables
- Use snake_case
- `sa_` prefix for all tables
- Examples:
  ```
  sa_users
  sa_messages
  sa_configurations
  ```

### 4.2. Columns
- Use snake_case
- Examples:
  ```sql
  user_id
  created_at
  updated_at
  is_active
  ```

## 5. Environment Variables Naming

### 5.1. General Rules
- Use UPPER_SNAKE_CASE
- `SA_` prefix for all variables
- Group by services
- Examples:
  ```
  SA_REST_HOST=localhost
  SA_REST_PORT=8000
  SA_BOT_TOKEN=123456
  SA_CALENDAR_API_KEY=abcdef
  ```

### 5.2. Service Grouping
```
# REST Service
SA_REST_HOST=
SA_REST_PORT=
SA_REST_DB_URL=

# Bot Service
SA_BOT_TOKEN=
SA_BOT_WEBHOOK_URL=

# Calendar Service
SA_CALENDAR_API_KEY=
SA_CALENDAR_CLIENT_ID=

# Assistant Service
SA_ASSISTANT_API_KEY=
SA_ASSISTANT_MODEL=
```

## 6. Documentation Naming

### 6.1. Documentation Files
- Use snake_case
- `docs_` prefix for technical documentation
- Examples:
  ```
  docs_architecture.md
  docs_api.md
  docs_deployment.md
  ```

### 6.2. Documentation Sections
- Use PascalCase for headers
- Use snake_case for links
- Examples:
  ```markdown
  # System Architecture
  # API Endpoints
  # Deployment Process
  ```

## 7. Exceptions

### 7.1. Allowed Exceptions
- PyPI package names (must be unique)
- Third-party configuration file names
- Database migration names (must be unique)

### 7.2. Exception Requirements
- Document all exceptions
- Explain the reason for the exception
- Provide alternatives if available
</file>

<file path="package.json">
{
  "dependencies": {
    "playwright": "^1.51.1"
  }
}
</file>

<file path="poetry_requirements.md">
# Poetry Requirements and Architecture

## 1. Overview

This document describes the requirements and architecture for using Poetry in our microservices project. The goal is to standardize dependency management and improve build efficiency across all services.

## 2. Project Structure

```
smart-assistant/
‚îú‚îÄ‚îÄ pyproject.toml           # Root project configuration
‚îú‚îÄ‚îÄ shared_models/          # Shared Pydantic models
‚îÇ   ‚îî‚îÄ‚îÄ pyproject.toml      # Shared models configuration
‚îú‚îÄ‚îÄ assistant_service/
‚îÇ   ‚îî‚îÄ‚îÄ pyproject.toml      # Assistant service configuration
‚îú‚îÄ‚îÄ rest_service/
‚îÇ   ‚îî‚îÄ‚îÄ pyproject.toml      # REST service configuration
‚îú‚îÄ‚îÄ google_calendar_service/
‚îÇ   ‚îî‚îÄ‚îÄ pyproject.toml      # Google Calendar service configuration
‚îú‚îÄ‚îÄ cron_service/
‚îÇ   ‚îî‚îÄ‚îÄ pyproject.toml      # Cron service configuration
‚îî‚îÄ‚îÄ telegram_bot_service/
    ‚îî‚îÄ‚îÄ pyproject.toml      # Telegram bot service configuration
```

## 3. Requirements

### 3.1 Root Project Configuration
- Base Python version and dependencies
- Common development tools (black, isort, flake8, etc.)
- Shared linting and formatting rules
- Common test dependencies

### 3.2 Service-Specific Configuration
Each service should have its own `pyproject.toml` that:
- Inherits from the root configuration
- Defines service-specific dependencies
- Sets up service-specific linting rules
- Configures service-specific test dependencies

### 3.3 Shared Models
- Separate package for shared Pydantic models
- Versioned independently
- Used by all services for data validation
- Published as a separate package

### 3.4 Docker Integration
- Multi-stage builds using Poetry
- Separate test and production dependencies
- Minimal final image size
- Poetry.lock files for deterministic builds

## 4. Implementation Guidelines

### 4.1 Root pyproject.toml
```toml
[tool.poetry]
name = "smart-assistant"
version = "0.1.0"
description = "Smart Assistant Project"
authors = ["Your Name <your.email@example.com>"]
packages = [
    { include = "shared_models" },
    { include = "assistant_service" },
    { include = "rest_service" },
    { include = "google_calendar_service" },
    { include = "cron_service" },
    { include = "telegram_bot_service" },
]

[tool.poetry.dependencies]
python = "^3.11"
pydantic = "^2.4.2"
# Common production dependencies

[tool.poetry.group.dev.dependencies]
black = "^23.10.1"
isort = "^5.12.0"
flake8 = "^6.1.0"
# Common development dependencies

[tool.poetry.group.test.dependencies]
pytest = "^7.4.3"
pytest-asyncio = "^0.21.1"
# Common test dependencies
```

### 4.2 Service-Specific pyproject.toml
```toml
[tool.poetry]
name = "assistant-service"
version = "0.1.0"
description = "Assistant Service"
authors = ["Your Name <your.email@example.com>"]

[tool.poetry.dependencies]
python = "^3.11"
smart-assistant = { path = "..", develop = true }
# Service-specific production dependencies

[tool.poetry.group.dev.dependencies]
smart-assistant = { path = "..", develop = true }
# Service-specific development dependencies

[tool.poetry.group.test.dependencies]
smart-assistant = { path = "..", develop = true }
# Service-specific test dependencies
```

### 4.3 Dockerfile Example
```dockerfile
# Build stage
FROM python:3.11-slim as builder

WORKDIR /app
COPY pyproject.toml poetry.lock ./
COPY shared_models ./shared_models
COPY assistant_service ./assistant_service

RUN pip install poetry && \
    poetry config virtualenvs.create false && \
    poetry install --no-dev --no-interaction --no-ansi

# Production stage
FROM python:3.11-slim

WORKDIR /app
COPY --from=builder /usr/local/lib/python3.11/site-packages /usr/local/lib/python3.11/site-packages
COPY assistant_service/src ./src

CMD ["python", "-m", "src.main"]
```

## 5. Benefits

### 5.1 Development
- Consistent dependency management
- Standardized linting and formatting
- Shared code quality tools
- Simplified local development

### 5.2 Build Process
- Smaller production images
- Deterministic builds
- Faster build times
- Better dependency resolution

### 5.3 Maintenance
- Centralized dependency updates
- Easier version management
- Better dependency tracking
- Simplified testing setup

## 6. Migration Plan

1. Create root pyproject.toml
2. Set up shared_models package
3. Create service-specific pyproject.toml files
4. Update Dockerfiles
5. Migrate dependencies from requirements.txt
6. Update CI/CD pipeline
7. Test and validate

## 7. Best Practices

### 7.1 Dependency Management
- Use exact versions in poetry.lock
- Regular dependency updates
- Clear separation of dev/test/prod dependencies
- Minimal production dependencies

### 7.2 Docker Optimization
- Multi-stage builds
- Minimal base images
- Layer caching
- Security scanning

### 7.3 Development Workflow
- Local Poetry environments
- Pre-commit hooks
- Automated testing
- Documentation updates
</file>

<file path="pyproject.toml">
[tool.black]
line-length = 88
target-version = ['py311']
include = '\.pyi?$'
extend-exclude = '''
# A regex preceded with ^/ will apply only to files and directories
# in the root of the project.
^/docs
'''

[tool.isort]
profile = "black"
multi_line_output = 3
include_trailing_comma = true
force_grid_wrap = 0
use_parentheses = true
ensure_newline_before_comments = true
line_length = 88
sections = ["FUTURE", "STDLIB", "THIRDPARTY", "FIRSTPARTY", "LOCALFOLDER"]
known_first_party = ["src"]
known_third_party = ["fastapi", "pydantic", "sqlalchemy", "redis", "httpx"]

[tool.poetry]
name = "smart-assistant"
version = "0.1.0"
description = "Smart Assistant Project"
authors = ["Your Name <your.email@example.com>"]
packages = []

[tool.poetry.scripts]
format = "scripts.format:main"
lint = "scripts.lint:main"

[tool.poetry.dependencies]
python = "^3.11"

[tool.poetry.group.dev.dependencies]
black = "^23.10.1"
isort = "^5.12.0"
flake8 = "^6.1.0"
flake8-pyproject = "^1.2.3"
mypy = "^1.6.1"
pylint = "^3.0.2"
autoflake = "^2.3.1"
structlog = {extras = ["types"], version = "^25.2.0"}

[tool.poetry.group.test.dependencies]
pytest = "^7.4.3"
pytest-asyncio = "^0.21.1"
pytest-cov = "^4.1.0"
pytest-mock = "^3.12.0"
factory-boy = "^3.3.0"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"

[tool.mypy]
python_version = "3.11"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = false
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

[tool.flake8]
max-line-length = 88
extend-ignore = "E203, W503"
exclude = [
    ".git",
    "__pycache__",
    "build",
    "dist",
    "*.egg-info",
    "docs",
    "migrations",
    "alembic",
    "**/alembic/versions/*.py",
    ".venv",
    ".idea"
]
per-file-ignores = [
    "__init__.py: F401",
]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "-v --cov=src --cov-report=term-missing"
asyncio_mode = "auto"
</file>

<file path="rag_service_implementation_plan.md">
# –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ RAG —Å–µ—Ä–≤–∏—Å–∞

## 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø—Ä–æ–µ–∫—Ç–∞ –¥–ª—è `rag_service`

*   **–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π:**
    ```bash
    mkdir rag_service
    cd rag_service
    mkdir src tests alembic src/api src/config src/models src/services src/scripts src/alembic
    touch src/__init__.py tests/__init__.py src/api/__init__.py src/config/__init__.py src/models/__init__.py src/services/__init__.py src/scripts/__init__.py src/alembic/__init__.py
    touch src/main.py src/config/settings.py src/api/routes.py src/services/vector_db_service.py src/models/rag_models.py
    touch Dockerfile Dockerfile.test docker-compose.test.yml pyproject.toml llm_context_rag.md
    ```

*   **`pyproject.toml` - –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π:**

    ```toml
    [tool.poetry]
    name = "rag-service"
    version = "0.1.0"
    description = "RAG Service for Smart Assistant"
    authors = ["Your Name <your.email@example.com>"]
    packages = [{ include = "src" }]

    [tool.poetry.dependencies]
    python = "^3.11"
    fastapi = "^0.109.2"
    uvicorn = "^0.27.1"
    pydantic = "^2.6.1"
    pydantic-settings = "^2.1.0"
    python-dotenv = "^1.0.1"
    structlog = "^24.1.0"
    chromadb = "^0.4.24" # –ü—Ä–∏–º–µ—Ä –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –ë–î, –º–æ–∂–Ω–æ –∑–∞–º–µ–Ω–∏—Ç—å
    httpx = "^0.26.0"
    shared-models = {path = "../shared_models"} # –î–æ–±–∞–≤–∏—Ç—å –æ–±—â–∏–µ –º–æ–¥–µ–ª–∏


    [tool.poetry.group.dev.dependencies]
    flake8 = "^6.1.0"
    flake8-pyproject = "^1.2.3"
    mypy = "^1.6.1"
    pylint = "^3.0.2"
    autoflake = "^2.3.1"

    [tool.poetry.group.test.dependencies]
    pytest = "^7.4.3"
    pytest-asyncio = "^0.21.1"
    pytest-cov = "^4.1.0"
    pytest-mock = "^3.12.0"

    [tool.pytest.ini_options]
    testpaths = ["tests"]
    python_files = ["test_*.py"]
    addopts = "-v --cov=src --cov-report=term-missing"
    asyncio_mode = "auto"

    [tool.mypy]
    python_version = "3.11"
    warn_return_any = true
    warn_unused_configs = true
    disallow_untyped_defs = true
    disallow_incomplete_defs = true
    check_untyped_defs = true
    disallow_untyped_decorators = true
    no_implicit_optional = true
    warn_redundant_casts = true
    warn_unused_ignores = true
    warn_no_return = true
    warn_unreachable = true
    strict_optional = true
    mypy_path = "src"
    ```

*   **`Dockerfile` –∏ `Dockerfile.test`**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —à–∞–±–ª–æ–Ω—ã –∏–∑ `docker_templates.md`, –∑–∞–º–µ–Ω–∏–≤ `service_name` –Ω–∞ `rag_service`.
*   **`docker-compose.test.yml`**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —à–∞–±–ª–æ–Ω –∏–∑ `docker_templates.md`, –Ω–∞—Å—Ç—Ä–æ–∏–≤ –∏–º—è —Å–µ—Ä–≤–∏—Å–∞ –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (–≤–µ—Ä–æ—è—Ç–Ω–æ Redis –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ —Ç–µ—Å—Ç–æ–≤—É—é –ë–î, –µ—Å–ª–∏ –≤—ã —Ä–µ—à–∏—Ç–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–æ–≤).
*   **`llm_context_rag.md`**: –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–π —Ñ–∞–π–ª –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ `service_template.md` –∏ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –æ–±–∑–æ—Ä, —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π –∏ —Ç.–¥.

## 2. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (`rag_service/src/config/settings.py`)

```python
from pydantic_settings import BaseSettings, SettingsConfigDict

class Settings(BaseSettings):
    """Rag Service Settings."""

    ENVIRONMENT: str = "development"
    LOG_LEVEL: str = "INFO"
    API_PORT: int = 8002 # –í—ã–±—Ä–∞—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–π –ø–æ—Ä—Ç
    VECTOR_DB_PATH: str = "chroma_db" # –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è ChromaDB

    model_config = SettingsConfigDict(env_file=".env", env_file_encoding="utf-8")

settings = Settings()
```

## 3. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö (`rag_service/src/services/vector_db_service.py`)

```python
from typing import List, Optional, UUID
import chromadb
import structlog
from src.config.settings import settings
from src.models.rag_models import RagData, SearchResult

logger = structlog.get_logger()

class VectorDBService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–æ–π –¥–∞–Ω–Ω—ã—Ö."""

    def __init__(self):
        self.client = chromadb.PersistentClient(path=settings.VECTOR_DB_PATH)
        self.collection = self.client.get_or_create_collection(name="rag_data") # –ò–º—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–º

    async def add_data(self, rag_data: RagData) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
        try:
            self.collection.add(
                embeddings=[rag_data.embedding],
                documents=[rag_data.text],
                ids=[str(rag_data.id)], # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å UUID –∫–∞–∫ ID
                metadatas=[rag_data.model_dump(exclude={"embedding", "text", "id"})], # –°–æ—Ö—Ä–∞–Ω—è—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –∏—Å–∫–ª—é—á–∏—Ç—å embedding –∏ text —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è
            )
            logger.info(f"Data added to vector DB with id: {rag_data.id}")
        except Exception as e:
            logger.error(f"Error adding data to vector DB: {e}")
            raise

    async def search_data(self, query_embedding: List[float], data_type: str, user_id: Optional[int], assistant_id: Optional[UUID], top_k: int = 5) -> List[SearchResult]:
        """–ò—â–µ—Ç –¥–∞–Ω–Ω—ã–µ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
        try:
            query_results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where={"data_type": data_type, "user_id": str(user_id) if user_id else None, "assistant_id": str(assistant_id) if assistant_id else None}, # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                where_document={}, # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
            )
            results = []
            for i in range(len(query_results['ids'][0])): # –ò—Ç–µ—Ä–∞—Ü–∏—è –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
                results.append(SearchResult(
                    id=UUID(query_results['ids'][0][i]),
                    text=query_results['documents'][0][i],
                    distance=query_results['distances'][0][i],
                    metadata=query_results['metadatas'][0][i]
                ))
            logger.info(f"Search completed, found {len(results)} results")
            return results
        except Exception as e:
            logger.error(f"Error searching vector DB: {e}")
            raise

    def get_client(self): # –î–ª—è –ø—Ä—è–º–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
        return self.client
```

## 4. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–∞–Ω–Ω—ã—Ö (`rag_service/src/models/rag_models.py`)

```python
from datetime import datetime
from typing import List, Optional, Dict, Any
from uuid import UUID, uuid4
from pydantic import BaseModel, Field

class RagData(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –¥–∞–Ω–Ω—ã—Ö, —Ö—Ä–∞–Ω—è—â–∏—Ö—Å—è –≤ RAG —Å–µ—Ä–≤–∏—Å–µ."""
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    text: str = Field(..., description="–¢–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ")
    embedding: List[float] = Field(..., description="–í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞")
    data_type: str = Field(..., description="–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'shared_rule', 'user_history', 'assistant_note')")
    user_id: Optional[int] = Field(None, description="ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    assistant_id: Optional[UUID] = Field(None, description="ID –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, –µ—Å–ª–∏ –¥–∞–Ω–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã –¥–ª—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
    timestamp: datetime = Field(default_factory=datetime.utcnow)

class SearchQuery(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è –ø–æ–∏—Å–∫–æ–≤—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ RAG —Å–µ—Ä–≤–∏—Å—É."""
    query_embedding: List[float] = Field(..., description="–í–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞")
    data_type: str = Field(..., description="–¢–∏–ø –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∏—Å–∫–∞")
    user_id: Optional[int] = Field(None, description="–§–∏–ª—å—Ç—Ä –ø–æ ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è")
    assistant_id: Optional[UUID] = Field(None, description="–§–∏–ª—å—Ç—Ä –ø–æ ID –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞")
    top_k: int = Field(default=5, description="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞")

class SearchResult(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –∏–∑ RAG —Å–µ—Ä–≤–∏—Å–∞."""
    id: UUID
    text: str
    distance: float
    metadata: Dict[str, Any]
```

## 5. –†–µ–∞–ª–∏–∑–∞—Ü–∏—è API —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤ (`rag_service/src/api/routes.py`)

```python
from typing import List
from fastapi import APIRouter, Depends, HTTPException
from src.services.vector_db_service import VectorDBService
from src.models.rag_models import RagData, SearchQuery, SearchResult

router = APIRouter()

async def get_vector_db_service() -> VectorDBService:
    """–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è VectorDBService."""
    return VectorDBService()

@router.post("/data/add", response_model=RagData)
async def add_data_endpoint(rag_data: RagData, vector_db_service: VectorDBService = Depends(get_vector_db_service)):
    """–≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö."""
    try:
        await vector_db_service.add_data(rag_data)
        return rag_data
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/data/search", response_model=List[SearchResult])
async def search_data_endpoint(search_query: SearchQuery, vector_db_service: VectorDBService = Depends(get_vector_db_service)):
    """–≠–Ω–¥–ø–æ–∏–Ω—Ç –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö."""
    try:
        results = await vector_db_service.search_data(
            query_embedding=search_query.query_embedding,
            data_type=search_query.data_type,
            user_id=search_query.user_id,
            assistant_id=search_query.assistant_id,
            top_k=search_query.top_k
        )
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

## 6. –û—Å–Ω–æ–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (`rag_service/src/main.py`)

```python
from fastapi import FastAPI
from src.api.routes import router
from src.config.settings import settings
import structlog

logger = structlog.get_logger()

app = FastAPI(
    title="RAG Service",
    description="Service for Retrieval-Augmented Generation",
    version="0.1.0",
)

app.include_router(router, prefix="/api", tags=["RAG Data"])

@app.on_event("startup")
async def startup_event():
    logger.info("Starting RAG service")

@app.get("/health")
async def health_check():
    """–≠–Ω–¥–ø–æ–∏–Ω—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏."""
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=settings.API_PORT)
```

## 7. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ `docker-compose.yml`

–î–æ–±–∞–≤–∏—Ç—å `rag_service` –≤ –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–∞–π–ª `docker-compose.yml`, –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –¥—Ä—É–≥–∏–º —Å–µ—Ä–≤–∏—Å–∞–º, –æ–±–µ—Å–ø–µ—á–∏–≤ –µ–≥–æ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Ç–æ–π –∂–µ —Å–µ—Ç–∏ –∏ –ø–µ—Ä–µ–¥–∞—á—É –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è. –í—ã–±—Ä–∞—Ç—å –ø–æ—Ä—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, 8002) –∏ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.

## 8. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å `assistant_service` (–ö–æ–Ω—Ü–µ–ø—Ç—É–∞–ª—å–Ω–æ)

*   **–î–æ–±–∞–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å**: –í `assistant_service/pyproject.toml` –¥–æ–±–∞–≤–∏—Ç—å `httpx` –∏ –æ–±—ä—è–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –æ—Ç `rag_service`.
*   **–°–æ–∑–¥–∞—Ç—å `RAGTool`**: –í `assistant_service/src/tools/rag_tool.py` —Å–æ–∑–¥–∞—Ç—å –∫–ª–∞—Å—Å `RAGTool`.
    *   `__init__`: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å `httpx.AsyncClient` –¥–ª—è —Å–≤—è–∑–∏ —Å `rag_service`.
    *   `execute(query: str, data_type: str, user_id: int, assistant_id: UUID)`:
        *   –ü–æ–ª—É—á–∏—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥ –∑–∞–ø—Ä–æ—Å–∞ `query` (–∏—Å–ø–æ–ª—å–∑—É—è —Ç—É –∂–µ –º–æ–¥–µ–ª—å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤, —á—Ç–æ –∏ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö, –≤–µ—Ä–æ—è—Ç–Ω–æ –∏–∑ OpenAI API –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤).
        *   –í—ã–∑–≤–∞—Ç—å —ç–Ω–¥–ø–æ–∏–Ω—Ç `/api/data/search` —Å–µ—Ä–≤–∏—Å–∞ `rag_service` —Å —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–º, `data_type`, `user_id` –∏ `assistant_id`.
        *   –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –∏ –≤–µ—Ä–Ω—É—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞.
*   **–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å `RAGTool`**: –í `assistant_service` –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å `RAGTool` –∫–∞–∫ —Ç–∏–ø –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞.
*   **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `RAGTool` –≤ –ª–æ–≥–∏–∫–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞**: –ò–∑–º–µ–Ω–∏—Ç—å –ª–æ–≥–∏–∫—É –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ (–ø—Ä–æ–º–ø—Ç—ã, –≤—ã–∑–æ–≤ —Ñ—É–Ω–∫—Ü–∏–π –∏ —Ç.–¥.), —á—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `RAGTool`, –∫–æ–≥–¥–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

## –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:

1.  **–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–æ–¥**: –°–æ–∑–¥–∞—Ç—å —Ñ–∞–π–ª—ã –∏ –≤—Å—Ç–∞–≤–∏—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã –∫–æ–¥–∞ –≤ –Ω–∏—Ö.
2.  **–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å —Ç–µ—Å—Ç—ã**: –ù–∞–ø–∏—Å–∞—Ç—å –º–æ–¥—É–ª—å–Ω—ã–µ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã –¥–ª—è `rag_service`.
3.  **–î–æ–∫–µ—Ä–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å**: –°–æ–±—Ä–∞—Ç—å –∏ –∑–∞–ø—É—Å—Ç–∏—Ç—å `rag_service` —Å –ø–æ–º–æ—â—å—é Docker Compose.
4.  **–ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å `assistant_service`**: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å `RAGTool` –∏ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –µ–≥–æ –≤ —Ä–∞–±–æ—á–∏–π –ø—Ä–æ—Ü–µ—Å—Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞.
5.  **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é**: –¢—â–∞—Ç–µ–ª—å–Ω–æ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é –º–µ–∂–¥—É `assistant_service` –∏ `rag_service`.
6.  **–î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å**: –û–±–Ω–æ–≤–∏—Ç—å `llm_context_rag.md` –∏ `llm_context.md` —Å –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç—è–º–∏ –æ –Ω–æ–≤–æ–º —Å–µ—Ä–≤–∏—Å–µ.

–ù–µ –∑–∞–±—É–¥—å—Ç–µ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å –ø–æ–º–æ—â—å—é `poetry install` –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ `rag_service` –ø–æ—Å–ª–µ —Å–æ–∑–¥–∞–Ω–∏—è —Ñ–∞–π–ª–∞ `pyproject.toml`. –≠—Ç–æ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π –ø–ª–∞–Ω –¥–æ–ª–∂–µ–Ω –¥–∞—Ç—å –≤–∞–º –ø—Ä–æ—á–Ω—É—é –æ—Ç–ø—Ä–∞–≤–Ω—É—é —Ç–æ—á–∫—É –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∞—à–µ–≥–æ RAG –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞.
</file>

<file path="README.md">
# Smart Assistant

A comprehensive intelligent assistant system built on OpenAI Assistants API, designed to manage various aspects of daily life through natural language processing.

## Architecture

### Services
- **assistant_service** - Core assistant service
  - Uses OpenAI Assistants API
  - Manages context and conversation history
  - Coordinates tool operations
  - Supports multiple secretary instances
  - Handles user-secretary mapping
- **rest_service** - REST API service
  - Manages user data and configurations
  - Handles CRUD operations for assistants and tools
  - Manages user-secretary mapping
  - Uses PostgreSQL for data storage
- **google_calendar_service** - Google Calendar integration
  - Event management
  - OAuth 2.0 authorization
  - Token management
- **cron_service** - Task scheduler service
  - Job scheduling and execution
  - Notification handling
  - Task management
- **telegram_bot_service** - Telegram bot interface
  - User interaction
  - Message processing
  - Response formatting
- **admin_service** - Administrative interface
  - System management
  - Configuration control
  - Monitoring capabilities

### Technology Stack
- Python 3.11+
- FastAPI
- PostgreSQL
- Redis
- Docker & Docker Compose
- OpenAI Assistants API
- Telegram Bot API
- Google Calendar API
- Poetry for dependency management
- Black & isort for code formatting
- Pytest for testing
- MyPy for type checking

## Installation

1. Clone the repository:
```bash
git clone <repository-url>
cd smart-assistant
```

2. Create a `.env` file with required environment variables:
```bash
OPENAI_API_KEY=your_openai_api_key
TELEGRAM_TOKEN=your_telegram_bot_token
POSTGRES_USER=your_db_user
POSTGRES_PASSWORD=your_db_password
POSTGRES_DB=your_db_name
ASYNC_DATABASE_URL=postgresql+asyncpg://user:password@db:5432/dbname
GOOGLE_CLIENT_ID=your_google_client_id
GOOGLE_CLIENT_SECRET=your_google_client_secret
GOOGLE_REDIRECT_URI=your_google_redirect_uri
```

3. Start services using Docker Compose:
```bash
docker compose up -d
```

## Development

### Development Environment
All development is done in Docker containers to ensure consistency across environments. The project uses Docker Compose for service orchestration and development.

### Code Quality
Run formatters:
```bash
./run_formatters.sh
```

### Testing
Run all tests:
```bash
./run_tests.sh
```

Run tests for specific services:
```bash
./run_tests.sh rest_service
./run_tests.sh google_calendar_service
./run_tests.sh cron_service
./run_tests.sh assistant_service
./run_tests.sh telegram_bot_service
```

### Project Structure
```
.
‚îú‚îÄ‚îÄ assistant_service/     # Core assistant service
‚îú‚îÄ‚îÄ rest_service/         # REST API service
‚îú‚îÄ‚îÄ google_calendar_service/ # Calendar integration
‚îú‚îÄ‚îÄ cron_service/        # Task scheduler
‚îú‚îÄ‚îÄ telegram_bot_service/ # Telegram bot
‚îú‚îÄ‚îÄ admin_service/       # Admin interface
‚îú‚îÄ‚îÄ shared_models/       # Shared data models
‚îú‚îÄ‚îÄ scripts/            # Utility scripts
‚îú‚îÄ‚îÄ manage.py          # Project management
‚îú‚îÄ‚îÄ run_tests.sh       # Test execution
‚îú‚îÄ‚îÄ run_formatters.sh  # Code formatting
‚îú‚îÄ‚îÄ docker-compose.yml # Docker configuration
‚îú‚îÄ‚îÄ pyproject.toml    # Poetry configuration
‚îî‚îÄ‚îÄ poetry.lock      # Fixed dependencies
```

## Service Communication

### Request Flow
1. User sends message via Telegram bot
2. Message processed by assistant service
3. Assistant determines required tools
4. Tools interact with respective services via REST API
5. Results returned to user via Telegram

### Message Queues
- Redis used for:
  - Conversation history
  - Tool result caching
  - Inter-service message queues
  - Health checks

### Database
- PostgreSQL used for:
  - User data
  - Assistant configurations
  - Interaction history
  - Task scheduling

## Monitoring

### Logging
- Structured logging with structlog
- Docker logs access
- Centralized log collection

### Health Checks
- Service health monitoring
- Dependency health verification
- Automatic retry mechanisms

## Documentation

Detailed implementation and architecture documentation is available in:
- [llm_context.md](llm_context.md) - High-level overview
- [service_template.md](service_template.md) - Service structure
- [naming_conventions.md](naming_conventions.md) - Coding standards
- [docker_templates.md](docker_templates.md) - Docker configuration
- [poetry_requirements.md](poetry_requirements.md) - Dependency management
</file>

<file path="refactoring_plan.md">
# –ü–ª–∞–Ω —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–µ–∫—Ç–∞

## 1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è ‚úÖ

### 1.1. –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
- ‚úÖ –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ –∏–º–µ–Ω–æ–≤–∞–Ω–∏–∏ —Å–µ—Ä–≤–∏—Å–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, `rest_service` vs `tg_bot`)
- ‚úÖ –†–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –≤ docker-compose.yml –∏ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
- ‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –µ–¥–∏–Ω–æ–≥–æ —Å—Ç–∏–ª—è –¥–ª—è –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –∏ –æ–±—Ä–∞–∑–æ–≤

### 1.2. –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
1. ‚úÖ –°–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏ –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è:
   - –°–µ—Ä–≤–∏—Å—ã: snake_case, —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º `smart_assistant_` (–Ω–∞–ø—Ä–∏–º–µ—Ä, `smart_assistant_rest`)
   - –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã: kebab-case (–Ω–∞–ø—Ä–∏–º–µ—Ä, `smart-assistant-rest`)
   - –û–±—Ä–∞–∑—ã: snake_case —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º `smart_assistant_` (–Ω–∞–ø—Ä–∏–º–µ—Ä, `smart_assistant_rest`)
   - –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: snake_case (–Ω–∞–ø—Ä–∏–º–µ—Ä, `smart_assistant_rest`)

2. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å –≤—Å–µ –∏–º–µ–Ω–∞ –≤:
   - docker-compose.yml
   - –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
   - –°–∫—Ä–∏–ø—Ç–∞—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
   - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏

## 2. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–µ—Ä–≤–∏—Å–æ–≤ ‚úÖ

### 2.1. –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
- ‚úÖ –†–∞–∑–Ω—ã–µ –∫–æ—Ä–Ω–µ–≤—ã–µ –ø–∞–ø–∫–∏ –≤ —Å–µ—Ä–≤–∏—Å–∞—Ö (–≥–¥–µ-—Ç–æ `src/`, –≥–¥–µ-—Ç–æ `app/`)
- ‚úÖ –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –∏–º–ø–æ—Ä—Ç–æ–≤
- ‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –µ–¥–∏–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∫–æ–¥–∞
- ‚úÖ –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –≤ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π

### 2.2. –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
1. ‚úÖ –°–æ–∑–¥–∞—Ç—å —à–∞–±–ª–æ–Ω —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å–µ—Ä–≤–∏—Å–∞
2. ‚úÖ –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
3. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å Dockerfile'—ã –∏ docker-compose —Ñ–∞–π–ª—ã –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
4. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å –∫–∞–∂–¥—ã–π —Å–µ—Ä–≤–∏—Å:
   - –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
   - –†–µ–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É
   - –û–±–Ω–æ–≤–∏—Ç—å –∏–º–ø–æ—Ä—Ç—ã
   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ—Å—Ç–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ

## 3. –í–Ω–µ–¥—Ä–µ–Ω–∏–µ –ª–∏–Ω—Ç–µ—Ä–æ–≤ ‚úÖ

### 3.1. –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
- ‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ª–∏–Ω—Ç–µ—Ä–æ–≤
- ‚úÖ –ù–µ–æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –∫–æ–¥
- ‚úÖ –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å —Å—Ç–∏–ª—è

### 3.2. –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
1. ‚úÖ –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è:
   - ‚úÖ black (—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)
   - ‚úÖ isort (—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –∏–º–ø–æ—Ä—Ç–æ–≤)
   - ‚úÖ flake8 (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∏–ª—è)
   - ‚úÖ autoflake (—É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–∏—Å–ø–æ–ª—å–∑—É–µ–º—ã—Ö –∏–º–ø–æ—Ä—Ç–æ–≤)
   - ‚úÖ mypy (–ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–∏–ø–æ–≤)

2. ‚úÖ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å pre-commit —Ö—É–∫–∏:
   - ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π pre-commit –∫–æ–Ω—Ñ–∏–≥
   - ‚úÖ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
   - ‚úÖ –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É —Ç–∏–ø–æ–≤

3. ‚úÖ –°–æ–∑–¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è:
   - –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π –¥–ª–∏–Ω—ã —Å—Ç—Ä–æ–∫–∏ (88 —Å–∏–º–≤–æ–ª–æ–≤, –∫–∞–∫ –≤ black)
   - –°—Ç–∏–ª—è –∏–º–ø–æ—Ä—Ç–æ–≤
   - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
   - –¢–∏–ø–∏–∑–∞—Ü–∏–∏

## 4. –£–ª—É—á—à–µ–Ω–∏–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏ üîÑ

### 4.1. –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
- ‚úÖ –†–∞–∑–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π –≤ —Ä–∞–∑–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–∞—Ö
- ‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –≤–µ—Ä—Å–∏—è–º–∏
- ‚úÖ –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤–µ—Ä—Å–∏–π
- üîÑ –£—è–∑–≤–∏–º–æ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è—Ö (–æ—Å—Ç–∞–ª–æ—Å—å –æ–±–Ω–æ–≤–∏—Ç—å –Ω–µ–∫—Ä–∏—Ç–∏—á–Ω—ã–µ)

### 4.2. –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
1. ‚úÖ –°–æ–∑–¥–∞—Ç—å —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª —Å –≤–µ—Ä—Å–∏—è–º–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
2. ‚úÖ –í–Ω–µ–¥—Ä–∏—Ç—å Poetry –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏
3. ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å Dockerfile'—ã
4. üîÑ –û–±–Ω–æ–≤–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
   - ‚úÖ –û–±–Ω–æ–≤–∏—Ç—å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
   - ‚è≥ –û–±–Ω–æ–≤–∏—Ç—å –æ—Å—Ç–∞–ª—å–Ω—ã–µ —É—è–∑–≤–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏
   - ‚è≥ –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å –≤–µ—Ä—Å–∏–π

## 5. –£–ª—É—á—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ ‚è≥

### 5.1. –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
- –†–∞–∑–±—Ä–æ—Å–∞–Ω–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
- –°–ª–æ–∂–Ω–æ—Å—Ç—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–Ω—ã–º–∏ –æ–∫—Ä—É–∂–µ–Ω–∏—è–º–∏
- –ü–µ—Ä–µ–¥–∞—á–∞ –≤—Å–µ–≥–æ .env —Ñ–∞–π–ª–∞ –≤ —Å–µ—Ä–≤–∏—Å—ã (–ø—Ä–æ–±–ª–µ–º–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –Ω–∞–≥–ª—è–¥–Ω–æ—Å—Ç–∏)

### 5.2. –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
1. –í–Ω–µ–¥—Ä–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏:
   - –í–Ω–µ–¥—Ä–∏—Ç—å pydantic –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
   - –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –∫–ª–∞—Å—Å—ã –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
   - –†–∞–∑–¥–µ–ª–∏—Ç—å –ø–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è–º (dev, test, prod)

2. –†–µ–æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è:
   - –°–æ–∑–¥–∞—Ç—å `.env.example`
   - –†–∞–∑–¥–µ–ª–∏—Ç—å `.env` –Ω–∞ –ª–æ–≥–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–∏
   - –î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
   - –û—Ç–∫–∞–∑–∞—Ç—å—Å—è –æ—Ç –ø–µ—Ä–µ–¥–∞—á–∏ .env —Ñ–∞–π–ª–∞ —Ü–µ–ª–∏–∫–æ–º –≤ —Å–µ—Ä–≤–∏—Å—ã:
     - –û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
     - –Ø–≤–Ω–æ —É–∫–∞–∑–∞—Ç—å –∏—Ö –≤ docker-compose.yml
     - –£–¥–∞–ª–∏—Ç—å env_file –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–µ—Ä–≤–∏—Å–æ–≤
     - –û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é

3. –£–ª—É—á—à–∏—Ç—å —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–µ–∫—Ä–µ—Ç–∞–º–∏:
   - –í–Ω–µ–¥—Ä–∏—Ç—å vault –∏–ª–∏ –∞–Ω–∞–ª–æ–≥
   - –î–æ–±–∞–≤–∏—Ç—å —Ä–æ—Ç–∞—Ü–∏—é —Å–µ–∫—Ä–µ—Ç–æ–≤
   - –£–ª—É—á—à–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å —Ö—Ä–∞–Ω–µ–Ω–∏—è

## 6. –£–ª—É—á—à–µ–Ω–∏–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è ‚è≥

### 6.1. –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –µ–¥–∏–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –∫ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é
- –ù–µ–ø–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤

### 6.2. –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
1. –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:
   - –°–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ —Ñ–∏–∫—Å—Ç—É—Ä—ã
   - –î–æ–±–∞–≤–∏—Ç—å factory_boy
   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å pytest-cov

2. –£–ª—É—á—à–∏—Ç—å CI/CD:
   - –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –ø–æ–∫—Ä—ã—Ç–∏—è –∫–æ–¥–∞
   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∑–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤
   - –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–æ–≤–µ—Ä–∫—É –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

3. –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ç–µ—Å—Ç—ã:
   - –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ
   - –î–æ–±–∞–≤–∏—Ç—å —Ç–µ—Å—Ç—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤
   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ API

## 7. –£–ª—É—á—à–µ–Ω–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è ‚è≥

### 7.1. –¢–µ–∫—É—â–∏–µ –ø—Ä–æ–±–ª–µ–º—ã
- –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–≥–æ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
- –ù–µ—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- –°–ª–æ–∂–Ω–æ—Å—Ç—å –æ—Ç–ª–∞–¥–∫–∏

### 7.2. –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
1. –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ:
   - –í–Ω–µ–¥—Ä–∏—Ç—å structlog
   - –î–æ–±–∞–≤–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Ä–æ—Ç–∞—Ü–∏—é –ª–æ–≥–æ–≤

2. –î–æ–±–∞–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:
   - –í–Ω–µ–¥—Ä–∏—Ç—å Prometheus –º–µ—Ç—Ä–∏–∫–∏
   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å Grafana –¥–∞—à–±–æ—Ä–¥—ã
   - –î–æ–±–∞–≤–∏—Ç—å –∞–ª–µ—Ä—Ç—ã

3. –£–ª—É—á—à–∏—Ç—å –æ—Ç–ª–∞–¥–∫—É:
   - –î–æ–±–∞–≤–∏—Ç—å —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫—É –∑–∞–ø—Ä–æ—Å–æ–≤
   - –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
   - –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

## 8. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ ‚è≥

### 8.1. –ü–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π
1. –û–±–Ω–æ–≤–∏—Ç—å —Ñ–∞–π–ª—ã –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞:
   - llm_context.md
   - llm_context_assistant.md
   - –î—Ä—É–≥–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ —Ñ–∞–π–ª—ã

2. –û–±–Ω–æ–≤–∏—Ç—å README.md:
   - –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ–≤—ã—Ö –ø—Ä–∞–≤–∏–ª–∞—Ö
   - –û–±–Ω–æ–≤–∏—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ
   - –î–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ª–∏–Ω—Ç–µ—Ä–∞—Ö –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏

## 9. –ü–æ—Ä—è–¥–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –∑–∞–¥–∞—á

1. üîÑ –ó–∞–≤–µ—Ä—à–∏—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
2. ‚è≥ –£–ª—É—á—à–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
3. ‚è≥ –û–±–Ω–æ–≤–∏—Ç—å –∫–∞–∂–¥—ã–π —Å–µ—Ä–≤–∏—Å:
   - assistant
   - rest_service
   - google_calendar_service
   - cron_service
   - tg_bot
4. ‚è≥ –ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ CI/CD
5. ‚è≥ –î–æ–±–∞–≤–∏—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
6. ‚è≥ –û–±–Ω–æ–≤–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—é
7. ‚è≥ –ü—Ä–æ–≤–µ—Å—Ç–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

## 10. –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞

- ‚úÖ –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã —Å–ª–µ–¥—É—é—Ç –µ–¥–∏–Ω–æ–º—É —Å—Ç–∏–ª—é –∏–º–µ–Ω–æ–≤–∞–Ω–∏—è
- ‚úÖ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —à–∞–±–ª–æ–Ω—É
- ‚úÖ –õ–∏–Ω—Ç–µ—Ä—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –∏ —Ä–∞–±–æ—Ç–∞—é—Ç
- üîÑ –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω—ã –∏ –±–µ–∑–æ–ø–∞—Å–Ω—ã
- ‚è≥ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –≤–∞–ª–∏–¥–∏—Ä—É–µ—Ç—Å—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
- ‚è≥ –¢–µ—Å—Ç–æ–≤–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ > 80%
- ‚è≥ –ù–∞—Å—Ç—Ä–æ–µ–Ω –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚è≥ CI/CD –ø–∞–π–ø–ª–∞–π–Ω —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—Ö–æ–¥–∏—Ç
- ‚è≥ –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –∏—Å–ø–æ–ª—å–∑—É—é—Ç –µ–¥–∏–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—é
- ‚è≥ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ–ª–Ω–∞—è –∏ –∞–∫—Ç—É–∞–ª—å–Ω–∞—è
</file>

<file path="requirements.txt">
black==24.2.0
isort==5.13.2
</file>

<file path="run_formatters.sh">
#!/bin/bash
# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ —Ç–µ–∫—É—â–µ–º –∫–æ–º–º–∏—Ç–µ
CHANGED_FILES=$(git diff --cached --name-only --diff-filter=d | grep "\.py$")
if [ -z "$CHANGED_FILES" ]; then
    echo "No Python files changed, skipping formatting checks"
    exit 0
fi
echo "Running formatters on changed files..."
# –ó–∞–ø—É—Å–∫–∞–µ–º black –≤ —Ä–µ–∂–∏–º–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
echo "Running black formatter..."
black --check $CHANGED_FILES
BLACK_RESULT=$?
# –ó–∞–ø—É—Å–∫–∞–µ–º isort –≤ —Ä–µ–∂–∏–º–µ –ø—Ä–æ–≤–µ—Ä–∫–∏
echo "Running isort formatter..."
isort --check-only $CHANGED_FILES
ISORT_RESULT=$?
# –ï—Å–ª–∏ —á—Ç–æ-—Ç–æ –±—ã–ª–æ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–æ
if [ $BLACK_RESULT -eq 1 ] || [ $ISORT_RESULT -eq 1 ]; then
    echo -e "\e[33mSome files need formatting. Please run formatters and commit again.\e[0m"
    echo -e "\e[33mTo format files, run:\e[0m"
    echo -e "\e[33mblack $CHANGED_FILES\e[0m"
    echo -e "\e[33misort $CHANGED_FILES\e[0m"
    echo -e "\e[33m\e[0m"
    echo -e "\e[33mThen commit again:\e[0m"
    echo -e "\e[33mgit add .\e[0m"
    echo -e "\e[33mgit commit -m \"your message\"\e[0m"
    exit 1
fi
echo "‚úÖ All files are properly formatted"
exit 0
</file>

<file path="run_tests.sh">
#!/bin/bash
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ —Ü–≤–µ—Ç–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
print_colored() {
    local color=$1
    local text=$2
    echo -e "\e[${color}m${text}\e[0m"
}
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –≤—ã–≤–æ–¥–∞ —Å–ø—Ä–∞–≤–∫–∏
print_help() {
    echo "Usage: $0 [service1 service2 ...]"
    echo "If no services specified, runs tests for all services"
    echo "Available services: rest_service, cron_service, telegram_bot_service, assistant_service, google_calendar_service, admin_service"
    exit 1
}
# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —Ç–µ—Å—Ç–æ–≤ —Å–µ—Ä–≤–∏—Å–∞
run_service_tests() {
    service=$1
    print_colored "36" "\n=== Running $service tests ===\n"
    if [ -d "$service" ] && [ -f "$service/Dockerfile.test" ]; then
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã –≤ Docker
        cd $service
        # –§–∏–ª—å—Ç—Ä—É–µ–º –≤—ã–≤–æ–¥, –æ—Å—Ç–∞–≤–ª—è—è —Ç–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫–∏ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ —Ç–µ—Å—Ç–æ–≤ –∏ –æ—à–∏–±–∫–∞–º–∏
        docker compose -f docker-compose.test.yml up --build --abort-on-container-exit 2>&1 | grep -E "collected|PASSED|FAILED|ERROR|===.*test session starts|===.*in [0-9]+\.[0-9]+s|ImportError|ModuleNotFoundError|SyntaxError|Exception|Traceback"
        test_exit_code=${PIPESTATUS[0]}
        docker compose -f docker-compose.test.yml down -v > /dev/null 2>&1
        cd ..
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        if [ $test_exit_code -eq 0 ]; then
            print_colored "32" "‚úÖ $service tests passed!\n"
        else
            print_colored "31" "‚ùå $service tests failed!\n"
            failed_services+=($service)
        fi
    else
        print_colored "33" "‚ö†Ô∏è No Dockerfile.test found for $service\n"
    fi
}
# –ú–∞—Å—Å–∏–≤ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è —Å–µ—Ä–≤–∏—Å–æ–≤ —Å —É–ø–∞–≤—à–∏–º–∏ —Ç–µ—Å—Ç–∞–º–∏
failed_services=()
# –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
all_services=("rest_service" "cron_service" "telegram_bot_service" "assistant_service" "google_calendar_service" "admin_service")
# –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç—ã –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
if [ $# -eq 0 ]; then
    # –ï—Å–ª–∏ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç, –∑–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã
    services_to_run=("${all_services[@]}")
else
    # –ï—Å–ª–∏ –µ—Å—Ç—å –∞—Ä–≥—É–º–µ–Ω—Ç—ã, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ö –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å
    services_to_run=()
    for service in "$@"; do
        if [[ " ${all_services[@]} " =~ " ${service} " ]]; then
            services_to_run+=("$service")
        else
            print_colored "31" "‚ùå Unknown service: $service\n"
            print_help
        fi
    done
fi
# –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã –¥–ª—è –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
for service in "${services_to_run[@]}"; do
    run_service_tests "$service"
done
# –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
if [ ${#failed_services[@]} -ne 0 ]; then
    print_colored "31" "\n=== Tests failed for the following services: ${failed_services[*]} ===\n"
    exit 1
fi
</file>

<file path="service_template.md">
# Service Template

## Directory Structure

```
name_service/
‚îú‚îÄ‚îÄ src/                    # Source code
‚îÇ   ‚îú‚îÄ‚îÄ api/               # API endpoints and routers
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ endpoints/     # API endpoint handlers
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ resource_endpoints.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ router.py
‚îÇ   ‚îú‚îÄ‚îÄ core/              # Core business logic
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ service.py
‚îÇ   ‚îú‚îÄ‚îÄ models/            # Data models and schemas
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ resource.py
‚îÇ   ‚îú‚îÄ‚îÄ services/          # External service integrations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ external_service.py
‚îÇ   ‚îú‚îÄ‚îÄ utils/             # Utility functions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ helpers.py
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ main.py            # Application entry point
‚îú‚îÄ‚îÄ tests/                 # Test files
‚îÇ   ‚îú‚îÄ‚îÄ api/              # API tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_endpoints.py
‚îÇ   ‚îú‚îÄ‚îÄ core/             # Core logic tests
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_service.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py       # Pytest fixtures
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ alembic/              # Database migrations
‚îÇ   ‚îú‚îÄ‚îÄ versions/        # Migration files
‚îÇ   ‚îú‚îÄ‚îÄ env.py
‚îÇ   ‚îî‚îÄ‚îÄ alembic.ini
‚îú‚îÄ‚îÄ docs/                # Service documentation
‚îÇ   ‚îú‚îÄ‚îÄ api.md          # API documentation
‚îÇ   ‚îî‚îÄ‚îÄ architecture.md # Architecture documentation
‚îú‚îÄ‚îÄ .context/           # LLM context files
‚îÇ   ‚îú‚îÄ‚îÄ service.context.md    # Service-specific context
‚îÇ   ‚îú‚îÄ‚îÄ api.context.md       # API-specific context
‚îÇ   ‚îú‚îÄ‚îÄ models.context.md    # Models-specific context
‚îÇ   ‚îî‚îÄ‚îÄ services.context.md  # External services context
‚îú‚îÄ‚îÄ .env.example        # Example environment variables
‚îú‚îÄ‚îÄ .gitignore         # Git ignore rules
‚îú‚îÄ‚îÄ Dockerfile         # Production Dockerfile
‚îú‚îÄ‚îÄ Dockerfile.test    # Test environment Dockerfile
‚îú‚îÄ‚îÄ docker-compose.test.yml  # Test environment compose file
‚îú‚îÄ‚îÄ pyproject.toml     # Project configuration and dependencies
‚îú‚îÄ‚îÄ README.md         # Service documentation
‚îî‚îÄ‚îÄ requirements.txt  # Python dependencies (if not using Poetry)
```

## File Templates

### 1. Main Application Entry Point (src/main.py)
```python
from fastapi import FastAPI
from src.api.router import router
from src.core.config import settings

app = FastAPI(
    title=settings.PROJECT_NAME,
    version=settings.VERSION,
    description=settings.DESCRIPTION
)

app.include_router(router, prefix=settings.API_PREFIX)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

### 2. Configuration (src/core/config.py)
```python
from pydantic_settings import BaseSettings
from functools import lru_cache

class Settings(BaseSettings):
    PROJECT_NAME: str
    VERSION: str
    DESCRIPTION: str
    API_PREFIX: str
    # Add other settings

    class Config:
        env_file = ".env"

@lru_cache()
def get_settings() -> Settings:
    return Settings()

settings = get_settings()
```

### 3. API Router (src/api/router.py)
```python
from fastapi import APIRouter
from src.api.endpoints import resource_endpoints

router = APIRouter()

router.include_router(
    resource_endpoints.router,
    prefix="/resources",
    tags=["resources"]
)
```

### 4. Endpoint Handler (src/api/endpoints/resource_endpoints.py)
```python
from fastapi import APIRouter, Depends
from src.core.service import Service
from src.models.resource import ResourceCreate, ResourceResponse

router = APIRouter()

@router.post("/", response_model=ResourceResponse)
async def create_resource(
    resource: ResourceCreate,
    service: Service = Depends()
):
    return await service.create_resource(resource)
```

### 5. Service Layer (src/core/service.py)
```python
from src.models.resource import ResourceCreate, ResourceResponse
from src.services.external_service import ExternalService

class Service:
    def __init__(self, external_service: ExternalService):
        self.external_service = external_service

    async def create_resource(self, resource: ResourceCreate) -> ResourceResponse:
        # Implement business logic
        pass
```

### 6. Models (src/models/resource.py)
```python
from pydantic import BaseModel
from datetime import datetime

class ResourceBase(BaseModel):
    name: str
    description: str | None = None

class ResourceCreate(ResourceBase):
    pass

class ResourceResponse(ResourceBase):
    id: int
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True
```

### 7. External Service (src/services/external_service.py)
```python
from typing import Any
import httpx

class ExternalService:
    def __init__(self, base_url: str):
        self.base_url = base_url
        self.client = httpx.AsyncClient()

    async def make_request(self, method: str, endpoint: str, **kwargs) -> Any:
        # Implement external service communication
        pass
```

### 8. Dockerfile
```dockerfile
FROM python:3.11-slim

WORKDIR /src

# Install Poetry
RUN pip install poetry

# Copy dependency files
COPY pyproject.toml poetry.lock ./

# Install dependencies
RUN poetry config virtualenvs.create false \
    && poetry install --no-dev --no-interaction --no-ansi

# Copy application code
COPY . .

# Run the application
CMD ["poetry", "run", "uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### 9. docker-compose.test.yml
```yaml
version: '3.8'

services:
  test:
    build:
      context: .
      dockerfile: Dockerfile.test
    environment:
      - TESTING=1
      - PYTHONPATH=/src
```

### 10. pyproject.toml
```toml
[tool.poetry]
name = "smart-assistant-service"
version = "0.1.0"
description = "Smart Assistant Service Template"
authors = ["Your Name <your.email@example.com>"]

[tool.poetry.dependencies]
python = "^3.11"
fastapi = "^0.104.0"
uvicorn = "^0.24.0"
pydantic = "^2.4.2"
pydantic-settings = "^2.0.3"
httpx = "^0.25.0"

[tool.poetry.group.dev.dependencies]
pytest = "^7.4.3"
pytest-asyncio = "^0.21.1"
pytest-cov = "^4.1.0"
black = "^23.10.1"
isort = "^5.12.0"
flake8 = "^6.1.0"
mypy = "^1.6.1"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"
```

## Additional Guidelines

### 1. Testing
- Use pytest for testing
- Write unit tests for all business logic
- Write integration tests for API endpoints
- Use pytest-cov for coverage reporting
- Use pytest-asyncio for async tests

### 2. Code Quality
- Use black for code formatting
- Use isort for import sorting
- Use flake8 for linting
- Use mypy for type checking
- Follow PEP 8 guidelines

### 3. Documentation
- Document all public APIs
- Include docstrings for all functions and classes
- Keep README.md up to date
- Document environment variables
- Include API documentation

### 4. Error Handling
- Use custom exception classes
- Implement proper error responses
- Log errors appropriately
- Include error details in responses

### 5. Security
- Validate all input data
- Use proper authentication and authorization
- Keep dependencies updated
- Follow security best practices

## Context Files

### 1. Service Context (.context/service.context.md)
```markdown
# Service Context

## Overview
- Service name and purpose
- Main responsibilities
- Key features
- Dependencies on other services

## Architecture
- High-level architecture
- Key components
- Data flow
- Integration points

## Configuration
- Required environment variables
- Configuration options
- Default values
- Configuration validation

## Development
- Setup instructions
- Development workflow
- Testing strategy
- Deployment process

## Maintenance
- Monitoring
- Logging
- Backup procedures
- Update procedures
```

### 2. API Context (.context/api.context.md)
```markdown
# API Context

## Endpoints
- List of all endpoints
- Request/response formats
- Authentication requirements
- Rate limiting

## Data Models
- Request models
- Response models
- Validation rules
- Example payloads

## Error Handling
- Error codes
- Error responses
- Retry policies
- Error logging

## Security
- Authentication methods
- Authorization rules
- Input validation
- Security headers
```

### 3. Models Context (.context/models.context.md)
```markdown
# Models Context

## Database Models
- Table structures
- Relationships
- Indexes
- Constraints

## Pydantic Models
- Schema definitions
- Validation rules
- Serialization options
- Example usage

## Data Types
- Custom types
- Type conversions
- Default values
- Null handling
```

### 4. Services Context (.context/services.context.md)
```markdown
# External Services Context

## Service Integrations
- List of external services
- Integration methods
- Authentication
- Rate limits

## API Clients
- Client implementations
- Error handling
- Retry logic
- Timeout settings

## Data Flow
- Request flow
- Response handling
- Error propagation
- Logging
```

### Guidelines for Context Files

1. **Content Organization**
   - Use clear hierarchical structure
   - Include code examples where relevant
   - Keep information up to date
   - Link to related documentation

2. **Maintenance**
   - Update when adding new features
   - Review during code reviews
   - Keep examples current
   - Remove outdated information

3. **Format**
   - Use Markdown for formatting
   - Include code blocks with language specification
   - Use tables for structured data
   - Include diagrams when helpful

4. **Best Practices**
   - Keep files focused and concise
   - Use consistent terminology
   - Include troubleshooting guides
   - Document known issues
</file>

<file path="sub_assistant_implementation_plan.md">
# –ü–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤

## 1. –ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è

### 1.1 REST —Å–µ—Ä–≤–∏—Å
- ‚úÖ –ú–æ–¥–µ–ª—å `Tool` —Å –ø–æ–ª–µ–º `description`
- ‚úÖ –ú–æ–¥–µ–ª—å `AssistantToolLink` –¥–ª—è —Å–≤—è–∑–∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
- ‚úÖ –¢–∏–ø –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ `SUB_ASSISTANT` –≤ `ToolType`
- ‚úÖ –ë–∞–∑–æ–≤—ã–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º–∏ –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏

### 1.2 –°–µ—Ä–≤–∏—Å –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
- ‚úÖ –†–µ–∞–ª–∏–∑–∞—Ü–∏—è `SubAssistantTool`
- ‚úÖ –§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
- ‚úÖ –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ —á–µ—Ä–µ–∑ –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤

### 1.3 –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å
- ‚úÖ –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º–∏
- ‚úÖ –ë–∞–∑–æ–≤—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
- ‚ùå –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Ç–∏–ø–∞ `sub_assistant`

## 2. –ù–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è

### 2.1 REST —Å–µ—Ä–≤–∏—Å
1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤:
   - –û–±–Ω–æ–≤–∏—Ç—å `ToolCreate` –∏ `ToolUpdate` –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º
   - –î–æ–±–∞–≤–∏—Ç—å –≤–∞–ª–∏–¥–∞—Ü–∏—é –æ–ø–∏—Å–∞–Ω–∏—è –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏/–æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞

### 2.2 –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å
1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤:
   - –î–æ–±–∞–≤–∏—Ç—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ `sub_assistant`
   - –î–æ–±–∞–≤–∏—Ç—å –≤—ã–±–æ—Ä –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ `sub_assistant`
   - –î–æ–±–∞–≤–∏—Ç—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞

2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤:
   - –î–æ–±–∞–≤–∏—Ç—å –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –≤ —Å–ø–∏—Å–∫–µ
   - –î–æ–±–∞–≤–∏—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞

## 3. –ü–æ—Ä—è–¥–æ–∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

1. REST —Å–µ—Ä–≤–∏—Å:
   - –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —ç–Ω–¥–ø–æ–∏–Ω—Ç–æ–≤
   - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π

2. –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å:
   - –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
   - –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤
   - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π

## 4. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### 4.1 REST —Å–µ—Ä–≤–∏—Å
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –æ–ø–∏—Å–∞–Ω–∏—è

### 4.2 –ê–¥–º–∏–Ω-–ø–∞–Ω–µ–ª—å
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ —Ç–∏–ø–∞ `sub_assistant`
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–±–æ—Ä–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –æ–ø–∏—Å–∞–Ω–∏—è

## 5. –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è

1. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ REST API:
   - –û–ø–∏—Å–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–π
   - –ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –æ—Ç–≤–µ—Ç–æ–≤

2. –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –∞–¥–º–∏–Ω-–ø–∞–Ω–µ–ª–∏:
   - –û–ø–∏—Å–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞
   - –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –ø–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

## 6. –†–∏—Å–∫–∏ –∏ –º–∏—Ç–∏–≥–∞—Ü–∏—è

### 6.1 –†–∏—Å–∫–∏
- –í–æ–∑–º–æ–∂–Ω—ã–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—ã –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
- –°–ª–æ–∂–Ω–æ—Å—Ç–∏ —Å –º–∏–≥—Ä–∞—Ü–∏–µ–π –¥–∞–Ω–Ω—ã—Ö
- –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã —Å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é –ø—Ä–∏ –±–æ–ª—å—à–æ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

### 6.2 –ú–∏—Ç–∏–≥–∞—Ü–∏—è
- –¢—â–∞—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∏–≥—Ä–∞—Ü–∏–π
- –ü–æ—ç—Ç–∞–ø–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
- –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

## 7. –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—Ö–∞

1. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ:
   - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ —Ç–∏–ø–∞ `sub_assistant` —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º
   - –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–±–æ—Ä–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
   - –ö–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è

2. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ:
   - –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ—Ö–æ–¥—è—Ç —É—Å–ø–µ—à–Ω–æ
   - –ù–µ—Ç —Ä–µ–≥—Ä–µ—Å—Å–∏–π –≤ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
   - –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è –∞–∫—Ç—É–∞–ª—å–Ω–∞

3. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ:
   - –£–¥–æ–±–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–æ–¥-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞–º–∏
   - –ü–æ–Ω—è—Ç–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–∞—Ö
   - –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –ø—Ä–∏ –¥–µ–π—Å—Ç–≤–∏—è—Ö
</file>

</files>
