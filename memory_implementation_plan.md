# Поэтапный план реализации памяти для ассистента

## Введение

Цель этой доработки — предоставить ассистентам (секретарям) возможность запоминать информацию из предыдущих взаимодействий с пользователем и использовать её для более контекстных и персонализированных ответов. Реализация будет поэтапной, начиная с простого сохранения полного состояния диалога и постепенно усложняясь до специализированного сервиса памяти с иерархической моделью, RAG и семантическим поиском.

## Этап 1: Персистентность состояния графа через Checkpointer

**Цель:** Обеспечить базовую "память" путем сохранения полного состояния (`State`) графа `BaseLLMChat` для каждого диалога в PostgreSQL, используя встроенные механизмы LangGraph. Это позволит возобновлять диалоги и иметь полную историю выполнения в сыром виде.

**Задачи:**

1.  **Настройка `AsyncPostgresSaver` в `assistant_service`:**
    *   Определить строку подключения к существующей PostgreSQL базе данных (той же, что использует `rest_service`).
    *   Создать экземпляр `AsyncPostgresSaver` из `langgraph.checkpoint.aiopg`, указав строку подключения. Убедиться, что зависимости установлены (`psycopg`).
2.  **Интеграция Checkpointer с `StateGraph`:**
    *   В `BaseLLMChat` (в `assistant_service.src.assistants.llm_chat`), при компиляции графа (`self.graph = self._create_graph().compile(...)`), передать созданный экземпляр `AsyncPostgresSaver` в аргумент `checkpointer`.
3.  **Управление `thread_id`:**
    *   Обеспечить, чтобы при каждом вызове графа (`graph.ainvoke` или `graph.astream`) в `assistant_service` (внутри `process_message`) передавался уникальный и консистентный `thread_id` для каждого диалога пользователя с ассистентом. Это делается через параметр `config`: `{"configurable": {"thread_id": "some_unique_dialog_id"}}`. ID может быть сформирован, например, как `f"{user_id}_{self.assistant_id or self.name}"`.
    *   LangGraph автоматически будет загружать последнее состояние для этого `thread_id` перед выполнением и сохранять новое состояние после выполнения шага.
4.  **Хранение:**
    *   `AsyncPostgresSaver` будет автоматически сохранять сериализованное состояние графа (включая все сообщения в `state['messages']`) в специальной таблице в PostgreSQL (например, `langgraph_checkpoints`). Таблица будет создана автоматически при первом использовании.
    *   **На этом этапе мы не создаем кастомную реляционную схему для сообщений/инструментов.** Мы полагаемся на механизм чекпоинтов для хранения всей истории выполнения в "сыром" виде внутри сериализованного состояния.

**Технический подход:**

*   `assistant_service`: Использование `langgraph.checkpoint.aiopg.AsyncPostgresSaver`, интеграция с `StateGraph.compile()`, передача `thread_id` в `config`. Добавление зависимости `psycopg` (асинхронной версии).
*   PostgreSQL: Используется существующая база данных, `AsyncPostgresSaver` управляет своей таблицей.

**Преимущества этого этапа:**

*   Простейший способ получить персистентность и statefulness для диалогов.
*   Использует встроенные, надежные механизмы LangGraph.
*   Сохраняет *всю* историю выполнения, включая промежуточные шаги.
*   Не требует создания нового микросервиса на данном этапе.
*   Обеспечивает полную трассируемость диалога.

**Недостатки:**

*   Загрузка/сохранение *всего* состояния может быть избыточной для простого получения последних N сообщений.
*   Затруднен прямой SQL-запрос к отдельным сообщениям или данным внутри сериализованного состояния (это будут решать последующие этапы).

## Этап 2: Базовая буферная память (Memory Service - STM)

**Цель:** Создать отдельный `memory_service` и интегрировать его с `assistant_service` для **оптимизированного** хранения и извлечения **недавней** истории диалогов (краткосрочная память - STM), не загружая полный чекпоинт графа каждый раз.

**Задачи:**

1.  **Создать `memory_service`:**
    *   Базовая структура сервиса (FastAPI, Dockerfile, docker-compose.yml).
    *   Зависимость от Redis.
2.  **Реализовать API в `memory_service`:**
    *   `POST /api/memory/messages`:
        *   **Вход:** `{ "user_id": str, "assistant_id": str, "messages": List[Dict] }` (только *новые* сообщения: вопрос пользователя и ответ ассистента)
        *   **Действие:** Сохраняет *только новые* сообщения в Redis (например, в `LIST` с ключом `memory_stm:{user_id}:{assistant_id}`). Применяет ограничение на длину истории (например, последние N сообщений). **Не дублирует полное хранение из Этапа 1.**
        *   **Выход:** `{ "status": "success" }` или ошибка.
    *   `GET /api/memory/context`:
        *   **Вход (query params):** `user_id: str`, `assistant_id: str`, `query: str`, `max_tokens: int`, `include_summary: bool` (пока `False`).
        *   **Действие:** Извлекает **только** последние сообщения из **Redis-буфера STM** (в пределах `max_tokens`).
        *   **Выход:** `{ "messages": List[Dict], "summary": Optional[str] }` (summary пока `None`).
3.  **Реализовать `MemoryServiceClient` в `assistant_service`:**
    *   Асинхронный HTTP-клиент для вызова API `memory_service`.
    *   Методы `retrieve_context` и `save_dialog`.
4.  **Интегрировать с `assistant_service` (уточнение интеграции из предыдущего плана):**
    *   Добавить `memory_client` в экземпляры ассистентов.
    *   Модифицировать логику графа в `BaseLLMChat`:
        *   **Перед** вызовом LLM/Agent: вызвать `memory_client.retrieve_context`, чтобы получить **только недавние сообщения из STM**. 
        *   Отформатировать полученный контекст STM.
        *   Добавить отформатированный контекст STM к списку `messages` в `state['messages']` (чекпоинт LangGraph по-прежнему сохраняет *полную* историю графа, но в LLM передаем только актуальный + STM контекст).
        *   **После** получения ответа от LLM/Agent: вызвать `memory_client.save_dialog`, передав *только* текущее сообщение пользователя и ответ ассистента для сохранения в STM Redis-буфере.
5.  **Добавить `MemoryTool` (Опционально, как и ранее):**
    *   Инструмент для явного взаимодействия с `memory_service` (сохранение/извлечение из STM).

**Технический подход:**

*   `memory_service`: FastAPI, Redis (для STM буфера).
*   `assistant_service`: HTTP-клиент, обновление логики графа для вызова `memory_service` и добавления STM контекста в `state['messages']`. Чекпоинт продолжает работать как на Этапе 1.

## Этап 3: Интеграция с векторным хранилищем (Memory Service - RAG/LTM)

**Цель:** Расширить `memory_service` для обеспечения семантического поиска по **полной** истории диалогов (долгосрочная память - LTM), хранящейся в `rag_service`.

**Задачи:**

1.  **Доработать `memory_service`:**
    *   Добавить зависимость от `rag_service`.
    *   При сохранении сообщений (`POST /api/memory/messages`): помимо записи в STM (Redis), **асинхронно** отправлять эти же сообщения на индексацию в `rag_service`. Использовать `user_id` и `assistant_id` как метаданные.
    *   При извлечении контекста (`GET /api/memory/context`):
        *   **Параллельно** с запросом STM из Redis, выполнить семантический поиск в `rag_service` по `query` (с фильтрацией). 
        *   Объединить результаты LTM (RAG) с недавней историей STM (из Этапа 2). Применить логику дедупликации и ранжирования.
        *   Обрезать итоговый комбинированный контекст до `max_tokens`.
2.  **Обновить `MemoryServiceClient` (если требуется):** Добавить параметры для управления RAG (например, `rag_top_k`).
3.  **Обновить `assistant_service`:** Логика добавления контекста в `state['messages']` теперь получает комбинированный (STM + LTM) контекст от `memory_service`.

**Технический подход:**

*   `memory_service`: Клиент для `rag_service`. Асинхронная отправка данных на индексацию. Логика параллельного запроса STM и LTM, комбинирования результатов.
*   `rag_service`: Используется для хранения и поиска векторов *всех* сообщений (данные поступают из `memory_service`).

## Этап 4: Суммаризация и управление контекстом (Memory Service - MTM)

**Цель:** Добавить в `memory_service` возможность автоматической суммаризации диалогов (среднесрочная память - MTM) для оптимизации контекста.

**Задачи:**

1.  **Доработать `memory_service`:**
    *   Добавить зависимость от LLM (например, настроить вызов LLM напрямую или через API `assistant_service`).
    *   Реализовать логику автоматической суммаризации:
        *   При сохранении (`POST /api/memory/messages`): проверять длину диалога (например, по данным из STM Redis или счетчику). Если порог превышен, запускать **фоновую** задачу для генерации/обновления саммари.
        *   Хранить саммари в **Redis** (управляемом `memory_service`) с ключом `memory_mtm:{user_id}:{assistant_id}`.
    *   При извлечении контекста (`GET /api/memory/context`):
        *   Если `include_summary=True`, извлечь последнее актуальное саммари из Redis MTM.
        *   Вернуть саммари в поле `summary` ответа.
        *   Алгоритм комбинирования теперь включает STM, LTM (RAG) и MTM (summary), приоритезируя их для `max_tokens`.
2.  **Обновить `assistant_service`:** Использовать поле `summary` из ответа `memory_service` при формировании итогового контекста для `state['messages']`.

**Технический подход:**

*   `memory_service`: Логика триггера суммаризации (фоновые задачи, например, Celery или asyncio tasks), вызов LLM, хранение саммари в Redis. Алгоритм комбинирования STM+LTM+MTM.

## Этап 5: Иерархическая модель памяти

**Цель:** Формализовать и оптимизировать взаимодействие между STM, MTM и LTM внутри `memory_service`.

**Задачи:**

1.  **Рефакторинг `memory_service`:**
    *   Четко структурировать внутреннюю логику работы с уровнями STM (Redis-буфер), MTM (Redis-саммари), LTM (`rag_service`).
    *   Оптимизировать алгоритм `retrieve_context`:
        *   Улучшить логику ранжирования и отбора информации с разных уровней (возможно, с использованием метаданных или эвристик релевантности).
        *   Улучшить динамическое формирование контекста в пределах `max_tokens`.
    *   Рассмотреть механизм "затухания" для STM или архивации очень старых данных LTM (если потребуется).

**Технический подход:**

*   `memory_service`: Внутренний рефакторинг и оптимизация алгоритмов извлечения и комбинирования данных. Внешний API остается стабильным.

## Этап 6: Специализированная семантическая память (Knowledge Graph)

**Цель:** Улучшить персонализацию за счет извлечения, структурирования и использования фактов и знаний о пользователе и сущностях.

**Задачи:**

1.  **Доработать `memory_service`:**
    *   Реализовать (или интегрировать) механизм извлечения сущностей и отношений (NER, RE) из сообщений при сохранении (`POST /api/memory/messages`).
    *   Создать хранилище для графа знаний: **Рекомендация:** использовать `rag_service` (Qdrant) с соответствующей структурой метаданных и векторов для представления фактов, чтобы избежать добавления еще одной БД (например, Neo4j).
    *   Обновлять "граф" в `rag_service` новыми фактами.
    *   При извлечении контекста (`GET /api/memory/context`):
        *   Выполнять дополнительный семантический поиск по "фактам" в `rag_service`, связанным с сущностями в `query`.
        *   Добавлять извлеченные факты к контексту, возвращаемому `assistant_service`.
2.  **Обновить `assistant_service`:** Адаптировать формирование промпта для включения структурированных фактов.

**Технический подход:**

*   `memory_service`: Интеграция с моделями NER/RE, разработка схемы хранения фактов в `rag_service`, алгоритмы поиска по фактам.

## Этап 7: Мониторинг, оптимизация и масштабирование

**Цель:** Обеспечить надежную, быструю и масштабируемую работу системы памяти (`memory_service`) и интеграции (`assistant_service`).

**Задачи:**

1.  **Реализовать в `memory_service`:**
    *   Метрики производительности (время ответа API, задержки RAG/суммирования/извлечения фактов).
    *   Детальное логирование.
    *   Кэширование.
    *   Оптимизация запросов.
    *   Масштабируемость сервиса.
2.  **Реализовать в `assistant_service`:**
    *   Мониторинг задержек при взаимодействии с `memory_service`.
    *   Обработка ошибок от `memory_service`.
3.  **Интегрировать с общей системой мониторинга.**

**Технический подход:**

*   `memory_service` и `assistant_service`: Инструментация кода, логирование, кэширование, асинхронная обработка.
*   Инфраструктура: Мониторинг, масштабирование.
